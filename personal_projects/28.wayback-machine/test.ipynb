{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wayback Machine API \n",
    "\n",
    "The purpose of this notebook is to test out using the wayback machines API, with selenium to take screenshots of webpages across many years.\n",
    "\n",
    "\n",
    "\n",
    "### Dependencies:\n",
    "\n",
    "`pip install webdriver_manager selenium requests`\n",
    "\n",
    "- selenium - Selenium is an open-source tool used for automating web browsers\n",
    "- webdriver_manager - library to automatically manage the web driver\n",
    "\n",
    "### Links:\n",
    "\n",
    "- https://selenium-python.readthedocs.io/getting-started.html\n",
    "- https://github.com/SeleniumHQ\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "> *October 2019, users are limited to 15 archival requests and retrievals per minute*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from PIL import Image\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snapshot_status(url, timestamp, max_attempts=3):\n",
    "    \"\"\"\n",
    "    Get the closest snapshot and its status.\n",
    "    \n",
    "    :param url: URL to get the snapshot for\n",
    "    :param timestamp: Timestamp in the format YYYYMMDDHHMMSS\n",
    "    :param max_attempts: Maximum attempts to retry on failure\n",
    "    :return: Tuple containing snapshot URL and status (green/red) or rate limiting information\n",
    "    \"\"\"\n",
    "    api_url = \"http://archive.org/wayback/available\"\n",
    "    params = {\n",
    "        'url': url,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_attempts):\n",
    "        try:\n",
    "            response = requests.get(api_url, params=params)\n",
    "            if response.status_code == 429:\n",
    "                retry_after = response.headers.get('Retry-After', 60)  # Default to 60 seconds if not provided\n",
    "                print(f\"Rate limited. Retry after {retry_after} seconds.\")\n",
    "                time.sleep(int(retry_after))\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()  # Raise an HTTPError for bad responses other than 429\n",
    "            data = response.json()\n",
    "\n",
    "            print(data)\n",
    "            \n",
    "            if 'archived_snapshots' in data and 'closest' in data['archived_snapshots']:\n",
    "                closest_snapshot = data['archived_snapshots']['closest']\n",
    "                snapshot_url = closest_snapshot['url']\n",
    "                snapshot_status = 'green' if closest_snapshot.get('status') == \"200\" else 'red'\n",
    "                return snapshot_url, snapshot_status\n",
    "            else:\n",
    "                return None, 'red'\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            time.sleep(2 ** attempt)  # Exponential backoff\n",
    "\n",
    "    return None, 'red'\n",
    "\n",
    "def get_embedded_links(snapshot_url):\n",
    "    \"\"\"\n",
    "    Extract all embedded links from the snapshot.\n",
    "    \n",
    "    :param snapshot_url: URL of the snapshot to extract links from\n",
    "    :return: List of extracted links\n",
    "    \"\"\"\n",
    "    response = requests.get(snapshot_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    return links\n",
    "\n",
    "def archive_links(links):\n",
    "    \"\"\"\n",
    "    Archive the extracted links using the Wayback Machine save API.\n",
    "    \n",
    "    :param links: List of links to archive\n",
    "    \"\"\"\n",
    "    save_api_url = \"http://web.archive.org/save/\"\n",
    "    for link in links:\n",
    "        response = requests.get(save_api_url + link)\n",
    "        if response.status_code == \"200\":\n",
    "            print(f\"Successfully archived {link}\")\n",
    "        else:\n",
    "            print(f\"Failed to archive {link}\")\n",
    "\n",
    "# def take_screenshot(url, output_path):\n",
    "#     \"\"\"\n",
    "#     Take a screenshot of a web page.\n",
    "    \n",
    "#     :param url: URL of the web page to take a screenshot of\n",
    "#     :param output_path: Path to save the screenshot\n",
    "#     \"\"\"\n",
    "#     os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "#     options = Options()\n",
    "#     options.headless = True\n",
    "#     driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "#     driver.get(url)\n",
    "#     total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "#     driver.set_window_size(1920, total_height)\n",
    "    \n",
    "#     # Ensure the page has loaded completely\n",
    "#     driver.implicitly_wait(10)\n",
    "    \n",
    "#     driver.save_screenshot(output_path)\n",
    "#     driver.quit()\n",
    "\n",
    "\n",
    "def take_full_page_screenshot(url, output_path):\n",
    "    \"\"\"\n",
    "    Take a full-page screenshot of a web page by scrolling and capturing segments.\n",
    "    \n",
    "    :param url: URL of the web page to take a screenshot of\n",
    "    :param output_path: Path to save the final stitched screenshot\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    \n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Allow some time for the page to load completely\n",
    "    \n",
    "    total_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    viewport_height = driver.execute_script(\"return window.innerHeight\")\n",
    "    driver.set_window_size(1920, viewport_height)\n",
    "\n",
    "    screenshots = []\n",
    "    for i in range(0, total_height, viewport_height):\n",
    "        driver.execute_script(f\"window.scrollTo(0, {i})\")\n",
    "        time.sleep(2)  # Allow some time for the page to scroll and render content\n",
    "        screenshot = driver.get_screenshot_as_png()\n",
    "        screenshots.append(Image.open(BytesIO(screenshot)))\n",
    "\n",
    "    # Stitch the screenshots together\n",
    "    stitched_image = Image.new('RGB', (screenshots[0].width, total_height))\n",
    "    y_offset = 0\n",
    "    for screenshot in screenshots:\n",
    "        stitched_image.paste(screenshot, (0, y_offset))\n",
    "        y_offset += screenshot.height\n",
    "\n",
    "    stitched_image.save(output_path)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20140701000000\n",
      "{'url': 'https://youtube.com', 'archived_snapshots': {'closest': {'status': '200', 'available': True, 'url': 'http://web.archive.org/web/20140701000037/http://www.youtube.com/', 'timestamp': '20140701000037'}}, 'timestamp': '20140701000000'}\n",
      "Snapshot URL: http://web.archive.org/web/20140701000037/http://www.youtube.com/\n",
      "Snapshot Status: green\n",
      "_img/youtube.com/20140701000000.png\n",
      "Screenshot saved to _img/youtube.com/20140701000000.png\n"
     ]
    }
   ],
   "source": [
    "url = \"https://youtube.com\"\n",
    "timestamp = \"20140701000000\"  # April 1, 2024, 00:00:00\n",
    "print(timestamp)\n",
    "snapshot_url, snapshot_status = get_snapshot_status(url, timestamp)\n",
    "\n",
    "\n",
    "\n",
    "if snapshot_url:\n",
    "    print(f\"Snapshot URL: {snapshot_url}\")\n",
    "    print(f\"Snapshot Status: {snapshot_status}\")\n",
    "    \n",
    "    # Take a screenshot of the snapshot\n",
    "    output_path = Path(f\"_img/{url.replace('https://', '')}/{timestamp}.png\")\n",
    "    print(output_path)\n",
    "    take_full_page_screenshot(snapshot_url, output_path)\n",
    "    print(f\"Screenshot saved to {output_path}\")\n",
    "    \n",
    "    if snapshot_status == 'green':\n",
    "        # Extract embedded links and archive them\n",
    "        links = get_embedded_links(snapshot_url)\n",
    "        # archive_links(links)\n",
    "    else:\n",
    "        print(\"Snapshot is not fully successful (red status).\")\n",
    "else:\n",
    "    print(\"No snapshot available for the given URL and timestamp.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'url': 'https://yappstore.ai', 'archived_snapshots': {}, 'timestamp': '20240401000000'}\n",
    "# No snapshot available for the given URL and timestamp."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
