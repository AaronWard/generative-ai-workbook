{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLX LLM Finetuning\n",
    "\n",
    "MLX is an array library that is similar to Numpy, It contains an python API for Apply Silicon. The focus of this notebook is to for LLM finetuning, which a capability of MLX. \n",
    "\n",
    "- `pip install mlx`\n",
    "- `pip install mlx-lm`\n",
    "\n",
    "`mlx-lm` is an abstraction library for interacting with language models using mlx.\n",
    "\n",
    "To view compute resource usage:\n",
    "- `pip install asitop` \n",
    "\n",
    "Note: MLX currently doesn't support Apple Neural Engine(ANE) which is a component within Apple devices, specifically designed for efficiently executing deep neural network operations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate, convert\n",
    "from mlx_lm.utils import get_model_path, save_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      " \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmlx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtokenizer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPreTrainedTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtemp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mformatter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Generate text from the model.\n",
      "\n",
      "Args:\n",
      "   model (nn.Module): The language model.\n",
      "   tokenizer (PreTrainedTokenizer): The tokenizer.\n",
      "   prompt (str): The string prompt.\n",
      "   temp (float): The temperature for sampling (default 0).\n",
      "   max_tokens (int): The maximum number of tokens (default 100).\n",
      "   verbose (bool): If ``True``, print tokens and timing information\n",
      "       (default ``False``).\n",
      "   formatter (Optional[Callable]): A function which takes a token and a\n",
      "       probability and displays it.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/masterclass/lib/python3.10/site-packages/mlx_lm/utils.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "? generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name =\"s3nh/Mistral_Sonyichi-7B-slerp\"\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "response = generate(model, tokenizer, prompt=\"Who are you?\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(model, tokenizer, prompt=\"Bill gates is a:\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"<s>[INST] Who are you? [/INST]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of where the model is stored locally\n",
    "get_model_path(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh /Users/award40/.cache/huggingface/hub/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  ls /Users/award40/.cache/huggingface/hub/models--s3nh--Mistral_Sonyichi-7B-slerp/snapshots/a749122da3653ed7133197687f84e25961372f48/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!  ls /Users/award40/.cache/huggingface/hub/models--s3nh--Mistral_Sonyichi-7B-slerp/snapshots/a749122da3653ed7133197687f84e25961372f48/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize and save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlx_path = f\"/Volumes/Extreme\\ Pro/models/mlx/{model_name}\"\n",
    "\n",
    "convert(\n",
    "    hf_path=model_name,\n",
    "    mlx_path = mlx_path,\n",
    "    # quantize = True,\n",
    "    # dtype = \"float16\",\n",
    "    upload_repo = None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize models from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import convert\n",
    "\n",
    "upload_repo = \"mlx-community/My-Mistral-7B-v0.1-4bit\"\n",
    "\n",
    "convert(\"mistralai/Mistral-7B-v0.1\", quantize=True, upload_repo=upload_repo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
