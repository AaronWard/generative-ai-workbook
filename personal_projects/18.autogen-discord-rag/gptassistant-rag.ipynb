{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import autogen\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.retrieve_utils  import query_vector_db, create_vector_db_from_dir\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma DB created successfully.\n"
     ]
    }
   ],
   "source": [
    "# CREATING VECTOR STORE\n",
    "collection_name = \"autogen-discord\"\n",
    "docs_path = Path(os.getcwd(), 'docs')\n",
    "# Check if the docs directory exists\n",
    "if not docs_path.exists():\n",
    "    raise ValueError(f\"The docs directory at {docs_path} does not exist.\")\n",
    "\n",
    "# Path where the Chroma database will be stored\n",
    "db_path = Path(os.getcwd(), 'chromadb')\n",
    "\n",
    "# Creating a Chroma DB client\n",
    "client = chromadb.PersistentClient(path=str(db_path))\n",
    "\n",
    "# Creating the vector database from documents in the docs directory\n",
    "# This processes each document, creating chunks of text, and then adds them to the Chroma database\n",
    "create_vector_db_from_dir(\n",
    "    dir_path=str(docs_path),  # Path to your documents\n",
    "    client=client,  # The Chroma DB client\n",
    "    collection_name=collection_name,  # Name of the collection in the database\n",
    "    max_tokens=3000,  # Max tokens per chunk of text\n",
    "    chunk_mode=\"multi_lines\"  # How the text is chunked\n",
    ")\n",
    "\n",
    "print(\"Chroma DB created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../../.env',\n",
    "    model_api_key_map={\n",
    "        \"gpt-4-1106-preview\": \"OPENAI_API_KEY\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4-1106-preview\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "query_vector_db_tool_config = {\n",
    "    \"name\": \"query_vector_db\",\n",
    "    \"description\": \"Function to query the Chroma vector database.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query_texts\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"n_results\": {\"type\": \"integer\"}\n",
    "        },\n",
    "        \"required\": [\"query_texts\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"assistant_id\": None,\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": query_vector_db_tool_config\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gpt-4-1106-preview\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_ASSISTANT_SYSTEM_MESSAGE = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
    "context provided by the user. You should follow the following steps to answer a question:\n",
    "\n",
    "Step 1: you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
    "a question answering task.\n",
    "Step 2: you reply based on the intent.\n",
    "\n",
    "Use the provided tool `query_vector_db` to get the context from a Chroma vectorstore.\n",
    "If user's intent is code generation you must follow the formats below to write your code:\n",
    "```language\n",
    "# your code\n",
    "```\n",
    "\n",
    "If user's intent is question answering, you must give as short an answer as possible.\n",
    "Use the provided tool `query_vector_db` to get the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:autogen.agentchat.contrib.gpt_assistant_agent:assistant_id was None, creating a new assistant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33muser_proxy\u001b[0m (to Chroma_SQL_Assistant):\n",
      "\n",
      "How can you use Chainlit with Autogen\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_vector_db...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.agentchat.contrib.gpt_assistant_agent:Intermediate executing(query_vector_db, Sucess: False) : Error: 'int' object has no attribute 'get_collection'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m user_proxy \u001b[39m=\u001b[39m autogen\u001b[39m.\u001b[39mUserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muser_proxy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     is_termination_msg\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m msg: \u001b[39m\"\u001b[39m\u001b[39mTERMINATE\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m msg[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     max_consecutive_auto_reply\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m initial_message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHow can you use Chainlit with Autogen\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/18.autogen-discord-rag/gptassistant-rag.ipynb#W2sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m user_proxy\u001b[39m.\u001b[39;49minitiate_chat(sql_assistant, message\u001b[39m=\u001b[39;49minitial_message)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:544\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \n\u001b[1;32m    532\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 544\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:344\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    342\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    343\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 344\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    345\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    346\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    347\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    348\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:475\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    476\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    477\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:887\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    886\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 887\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    888\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    889\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/contrib/gpt_assistant_agent.py:147\u001b[0m, in \u001b[0;36mGPTAssistantAgent._invoke_assistant\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39m# Create a new run to get responses from the assistant\u001b[39;00m\n\u001b[1;32m    140\u001b[0m run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_openai_client\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mruns\u001b[39m.\u001b[39mcreate(\n\u001b[1;32m    141\u001b[0m     thread_id\u001b[39m=\u001b[39massistant_thread\u001b[39m.\u001b[39mid,\n\u001b[1;32m    142\u001b[0m     assistant_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_openai_assistant\u001b[39m.\u001b[39mid,\n\u001b[1;32m    143\u001b[0m     \u001b[39m# pass the latest system message as instructions\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     instructions\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msystem_message,\n\u001b[1;32m    145\u001b[0m )\n\u001b[0;32m--> 147\u001b[0m run_response_messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_run_response(assistant_thread, run)\n\u001b[1;32m    148\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(run_response_messages) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo response from the assistant.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m response \u001b[39m=\u001b[39m {\n\u001b[1;32m    151\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: run_response_messages[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    152\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/contrib/gpt_assistant_agent.py:175\u001b[0m, in \u001b[0;36mGPTAssistantAgent._get_run_response\u001b[0;34m(self, thread, run)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mWaits for and processes the response of a run from the OpenAI assistant.\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39m    Updated run object, status of the run, and response messages.\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     run \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait_for_run(run\u001b[39m.\u001b[39;49mid, thread\u001b[39m.\u001b[39;49mid)\n\u001b[1;32m    176\u001b[0m     \u001b[39mif\u001b[39;00m run\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcompleted\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    177\u001b[0m         response_messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_openai_client\u001b[39m.\u001b[39mbeta\u001b[39m.\u001b[39mthreads\u001b[39m.\u001b[39mmessages\u001b[39m.\u001b[39mlist(thread\u001b[39m.\u001b[39mid, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39masc\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/contrib/gpt_assistant_agent.py:244\u001b[0m, in \u001b[0;36mGPTAssistantAgent._wait_for_run\u001b[0;34m(self, run_id, thread_id)\u001b[0m\n\u001b[1;32m    242\u001b[0m     in_progress \u001b[39m=\u001b[39m run\u001b[39m.\u001b[39mstatus \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39min_progress\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mqueued\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m     \u001b[39mif\u001b[39;00m in_progress:\n\u001b[0;32m--> 244\u001b[0m         time\u001b[39m.\u001b[39;49msleep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_config\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcheck_every_ms\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m1000\u001b[39;49m) \u001b[39m/\u001b[39;49m \u001b[39m1000\u001b[39;49m)\n\u001b[1;32m    245\u001b[0m \u001b[39mreturn\u001b[39;00m run\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# INSTANTIATE AGENTS\n",
    "\n",
    "sql_assistant = GPTAssistantAgent(\n",
    "    name=\"Chroma_SQL_Assistant\",\n",
    "    instructions=RAG_ASSISTANT_SYSTEM_MESSAGE,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "sql_assistant.register_function(\n",
    "    function_map={\n",
    "        \"query_vector_db\": lambda query_texts, n_results=10: query_vector_db(query_texts=query_texts, \n",
    "                                                                             n_results=n_results, \n",
    "                                                                             client=client,\n",
    "                                                                             collection_name=collection_name)\n",
    "    }\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"./autogen_results\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3\n",
    ")\n",
    "\n",
    "initial_message = \"How can you use Chainlit with Autogen\"\n",
    "user_proxy.initiate_chat(sql_assistant, message=initial_message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
