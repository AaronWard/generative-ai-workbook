{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Generation\n",
    "\n",
    "Last time i did this it cost about $10 to create question:answer pairs, this time im gonna use local models to it using ollama.\n",
    "This will iterate over the posts extracted from r/localllama and generate a QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import ollama\n",
    "import pickle\n",
    "import pprint as pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The reason why the sky appears blue during a clear day is due to a particular type of scattering called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with different gases and particles present in the air. Blue light has a shorter wavelength and gets scattered more easily than other colors in the visible spectrum like red or yellow. This scattered blue light then enters our eyes from all directions, giving the sky its characteristic blue hue.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "response = ollama.chat(model='mistral:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "    'temperature': 0.01,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'mistral:latest',\n",
       " 'created_at': '2024-02-18T09:16:28.075788Z',\n",
       " 'message': {'role': 'assistant',\n",
       "  'content': \" The reason why the sky appears blue during a clear day is due to a particular type of scattering called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with different gases and particles present in the air. Blue light has a shorter wavelength and gets scattered more easily than other colors in the visible spectrum like red or yellow. This scattered blue light then enters our eyes from all directions, giving the sky its characteristic blue hue.\"},\n",
       " 'done': True,\n",
       " 'total_duration': 11635267083,\n",
       " 'load_duration': 8825254416,\n",
       " 'prompt_eval_count': 15,\n",
       " 'prompt_eval_duration': 149672000,\n",
       " 'eval_count': 93,\n",
       " 'eval_duration': 2659788000}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Client.generate() got an unexpected keyword argument 'messages'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mollama\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmistral:latest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhy is the sky blue?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: Client.generate() got an unexpected keyword argument 'messages'"
     ]
    }
   ],
   "source": [
    "# test\n",
    "response = ollama.generate(model='mistral:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "    'temperature': 0.01,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama client\n",
    "\n",
    "`OLLAMA_HOST=127.0.0.1:5050 ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 975 questions in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Post ID: 1at0288\\nTitle: Ok, which one of you was this? ü§£ü§£ü§£\\nLink: https://redd.it/1at0288\\nContent: \\nReplies:\\n- No, I don't think OpenAI would ever allow porn to be generated. I rather think that copies of Sora, recreated open source image generators will appear and fullfill this task. Porn is always one of the first use cases in any technologie that appeared and I don't think it'll take long for the industry to hop into this new tech. This is good for us as it further pushes open source AI technology for any use case.\\n\\n\",\n",
       " ' 1aszy6f\\nTitle: What are your favorite resources for evaluating text generation for stuff like readability, engagement (and other \"soft\" metrics)\\nLink: https://redd.it/1aszy6f\\nContent: Hi everyone, i\\'m working on a thesis looking at different prompt engineering methods and trying to evaluate the quality of generated content for stuff like articles, newsletters = human read content. Most research focuses on stuff like factuality, reasoning but I\\'m trying to find more research or these \"soft\" metrics. Any resources welcome, academic papers preferred. Many thanks!\\n\\n',\n",
       " ' 1aszspr\\nTitle: State of Opensource and Closedsource as of right now:\\nLink: https://redd.it/1aszspr\\nContent: What are the best opensource and closedsourced AI models?\\n\\nWhat will be the best opensource and closedsource AI models at the end of this year?\\n\\nList as per the following categories:\\n\\n1. Large Language Models:\\n\\n2. Large Multimodal Models:\\n\\n3. Text to image: \\n\\n4. Text to video: \\n\\n5. Text to 3D:\\n\\n6. Text to audio:\\n\\n7. Audio to text (transcribing):\\n\\n8. General purpose robots powered by Large multimodal models:\\n\\n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./_output/new/localllama-new-17-02-2024.txt\"\n",
    "with open(DATA_PATH, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_chunks = data.split(\"---\\nPost ID:\")\n",
    "\n",
    "print(f\"There are {len(data_chunks)} questions in total\")\n",
    "data_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating QA Pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 975/975 [4:45:06<00:00, 17.55s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://127.0.0.1:5050')\n",
    "\n",
    "response_chunks = []\n",
    "\n",
    "print(\"Generating QA Pairs...\")\n",
    "for chunk in tqdm.tqdm(data_chunks):\n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {chunk}\n",
    "    ```\n",
    "    \\n\n",
    "    Your job is to look at this reddit post and to produce several question/answer pairs based on the content provided. \n",
    "    Look at the replies also and try to extract informative technical information. \n",
    "    Do not produce QA pairs for anything that is not in the provided text. \n",
    "    For longer posts (such as ones with a lot of information in the content or with many comments) produce a lot of QA pairs. \n",
    "    For posts with less content, produce fewer. Only include QA pairs with general useful information. \n",
    "    Do not include anything that could be considered personal information, opinion, or conversational text. \n",
    "    Only provide the QA pairs. Do NOT provide introductions or conclusions. Write your answer in this format:\n",
    "\n",
    "    ```\n",
    "    Q: What is the colour of the sky?\n",
    "    A: The colour of the sky is blue.\n",
    "    ---\n",
    "    Q: How old is OpenAI? \n",
    "    A: OpenAI was founded in 2015, therefore it is 8 years old.\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    response_chunks.append(\n",
    "        client.generate(model='mistral:latest', messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'temperature': 0.2\n",
    "        }])\n",
    "    )\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' 1ar7lfq\\n'\n",
      " 'Title: The Dilemma of AI Accelerators: Bridging the Gap for Affordable '\n",
      " 'Solutions\\n'\n",
      " 'Link: https://redd.it/1ar7lfq\\n'\n",
      " 'Content: Why is there no middle ground with AI accelerators? The current '\n",
      " 'landscape presents us with either small USB AI accelerators lacking memory '\n",
      " 'or massive data center solutions like GH200 Superchip, Gaudi, or Graphcore. '\n",
      " \"It's time we address this issue and advocate for a more balanced approach.\\n\"\n",
      " '\\n'\n",
      " 'GPUs, despite their versatility, fall short in terms of cost-efficiency '\n",
      " 'compared to purpose-built NPUs/TPUs. A simplified solution, like a 24GB '\n",
      " 'A2000 or A4000 without unnecessary features such as NVENC or display outs, '\n",
      " 'could be a game-changer. While not suitable for gaming, it would meet the '\n",
      " 'majority of consumer AI needs at an affordable price point.\\n'\n",
      " '\\n'\n",
      " 'Some might argue, \"Isn\\'t that what workstation/data center GPUs are for?\" '\n",
      " 'Well, not quite. The existing GPUs often have an imbalanced ratio of compute '\n",
      " 'performance to VRAM. Take the Nvidia RTX A4000, for instance, with ample '\n",
      " 'compute power but only 16GB of VRAM. A version with 24/32GB would '\n",
      " 'significantly enhance its utility for AI. The truth is, when it comes to AI '\n",
      " \"inference, we're usually VRAM bound, and the current options can get \"\n",
      " 'prohibitively expensive.\\n'\n",
      " '\\n'\n",
      " 'By expressing this frustration, I hope to draw attention to the need for '\n",
      " 'more affordable AI hardware solutions. Modern GPUs are inefficient cost-wise '\n",
      " 'due to their VRAM limitations, and they could be streamlined for better '\n",
      " 'performance. A 24GB A2000 or A4000, sans unnecessary frills, would be a '\n",
      " \"welcome addition, especially if priced reasonably. I'd gladly pay $600-700 \"\n",
      " 'for such a solution, considering the original MSRP of the A2000 was $450.\\n'\n",
      " '\\n'\n",
      " 'I apologize for the rant, but I want to voice this problem in the hope that, '\n",
      " 'collectively, we can encourage someone to take notice and bring about a '\n",
      " 'positive change in the AI hardware market. If we raise awareness‚Äîmaybe, just '\n",
      " 'maybe, someone will take up the challenge of providing us with the '\n",
      " 'affordable and efficient AI accelerators we need.\\n'\n",
      " 'Replies:\\n'\n",
      " '- >Why is there no middle ground with AI accelerators?\\n'\n",
      " '\\n'\n",
      " \"Because there's almost no demand compared to both ends of the spectrum.\\n\"\n",
      " '\\n'\n",
      " 'This is still a niche hobby in the grand scheme of things.\\n'\n",
      " \"  - And because it's niche it's going to be expensive because they have to \"\n",
      " 'get their money back by providing enterprise solutions with Enterprise '\n",
      " 'prices.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'If you want \"affordable\" then you\\'re going to have to buy a GPU.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'GPUs drive the price down because of the mainstream market.\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"You're only going to have even worse prices by going it asic solutions \"\n",
      " 'because the volume is so low that price has to make up for it.\\n'\n",
      " '- NVIDIA charges what they do because they can. Their competitors will '\n",
      " 'charge as much as they can, too. They may not be able to charge as much as '\n",
      " 'NVIDIA, but they can charge a lot because NVIDIA charges even more.\\n'\n",
      " '\\n'\n",
      " \"They aren't likely to make $700 high-memory, low-frills, cards because \"\n",
      " 'some/many of their existing customers would stop paying many times that for '\n",
      " 'a high end card.\\n'\n",
      " '\\n'\n",
      " 'An upstart might see a growth opportunity in desktop LLM accelerators and '\n",
      " 'target the niche with a dramatically different pricepoint. Tensortorrent has '\n",
      " 'a $700 card, but, well, it only has 8GB of RAM and 100GB/s bandwidth.\\n'\n",
      " \"  - >They aren't likely to make $700 high-memory, low-frills, cards because \"\n",
      " 'some/many of their existing customers would stop paying many times that for '\n",
      " 'a high end card.\\n'\n",
      " '\\n'\n",
      " \"Depends on it's perfomance. No one from actula current customer will move to \"\n",
      " 'a for example tesla p100 but with 48gb VRAM because it will be banal '\n",
      " 'terribly slow (compared to even a desktop 3060) and greedy/hot for their '\n",
      " 'tasks. Even if it costs only 400-500-700$ \\n'\n",
      " '\\n'\n",
      " \"But ML-enthusiasts who don't need to serve 100500 clients or train huge \"\n",
      " 'models (and LORA can be trained at home in a week instead of 2 days) - will '\n",
      " 'be ready to kill for such a card.\\n'\n",
      " '  - What will actually happen is as this persists for longer eventually '\n",
      " 'hobbyists will get their hands on some of the hand me downs.\\xa0\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " \"That being said I've given some thought to what a Raspberry Pi-style \"\n",
      " 'accelerator would look like. Honestly I think you could build an inference '\n",
      " \"accelerator fairly easy, you'd want a high performance ARM chip stacked with \"\n",
      " 'some high bandwidth RAM. All off the shelf stuff. Training, on the other '\n",
      " \"hand, that's more difficult.\\xa0\\n\"\n",
      " '- All the hyperscalers who actually buy most of these things have '\n",
      " 'practically unlimited amounts of money. So it makes sense for HW makers to '\n",
      " 'jack up prices and margins as much as possible.\\n'\n",
      " '\\n'\n",
      " 'I don‚Äôt think it makes sense for hardware manufacturers to court this '\n",
      " '‚Äúmiddle‚Äù market. Also I am not sure if there is much addressable market for '\n",
      " 'it.\\n'\n",
      " '\\n'\n",
      " 'If you are trying to optimize total cost it might make more sense but HW '\n",
      " 'makers don‚Äôt care about that at all. Maybe Google with their TPUs can care '\n",
      " 'and have more efficient chips like TPUv5e.\\n'\n",
      " '- Currently running dual rx7600xt. 32gb vram for under 700USD. Only works on '\n",
      " 'Linux though...\\n'\n",
      " '  - Can you give some benchmark results, for a larger model that uses both '\n",
      " 'cards?\\n'\n",
      " \"    - Not benchmarks per se, but using HF transformers in python I'm getting \"\n",
      " 'approx 8 tok/s with WhiteRabbitNeo-13B-v1 split accross both. \\n'\n",
      " '\\n'\n",
      " 'If there is a particular benchmark / model you would like me to check out '\n",
      " 'let me know!\\n'\n",
      " '- I agree that we consumers, SMBs need better inferencing / ML performance,\\n'\n",
      " 'and that having more VRAM full cost effective NPUs is one way to get there.\\n'\n",
      " '\\n'\n",
      " \"But look at the Mac M series (N.B. I'm not an apple / mac user or fan in \"\n",
      " 'particular), the higher end models have unified memory ( CPU + GPU + NPU all '\n",
      " 'have high bandwidth access ) and like 300-400 GBy/s unified RAM (not VRAM) '\n",
      " \"throughput.  That's, what, vs something like 1TBy/s for a high end consumer \"\n",
      " \"GPU?  It isn't BETTER than a high end consumer GPU, but it's not a lot \"\n",
      " 'worse,\\n'\n",
      " 'AND this is MAIN MEMORY so you can get models with like 128GBy or more with '\n",
      " 'that performance.\\n'\n",
      " '\\n'\n",
      " 'Look at the following article, AI performance on Intel consumer chips is '\n",
      " 'supposed to increase 3x in 2024, and another 2x in 2025 giving 6x the '\n",
      " 'current\\n'\n",
      " \"levels.  How do you suppose that's going to happen?  Will it be for RAM BW \"\n",
      " \"bound AI?  If so then no matter how much you improve the CPU/IGPU you're not \"\n",
      " 'going to scale 3x or 6x performance on large models (more than cache sized '\n",
      " \"etc.) if you don't increase the RAM BW, right?  So are we FINALLY getting 4, \"\n",
      " '6 channel RAM interfaces or its equivalent on PCs?\\n'\n",
      " '\\n'\n",
      " 'https://wccftech.com/intel-panther-lake-cpus-double-ai-performance-over-lunar-lake-clearwater-forest-in-fabs\\n'\n",
      " '\\n'\n",
      " 'Now look at x64 architecture servers:\\n'\n",
      " '\\n'\n",
      " 'https://infohub.delltechnologies.com/en-US/p/ddr5-memory-bandwidth-for-next-generation-poweredge-servers-featuring-4th-gen-amd-epyc-processors/\\n'\n",
      " '\\n'\n",
      " 'DDR5 RAM BW: 256 GBy/s at 4 DIMMs, 512 GBy/s at 8 DIMMs, and over 700 GBy/s '\n",
      " 'at 12 DIMMs.\\n'\n",
      " '\\n'\n",
      " \"Then look at GPUs which between the 1990s and today have followed Moore's \"\n",
      " 'law scaling the SIMD processing resources and VRAM BW to give us many '\n",
      " 'thousands of SIMD SMs, and 1-2 TBy/s in consumer level HW.\\n'\n",
      " '\\n'\n",
      " 'In ~2017 the NV 1080Ti achieved ~450 GBy/s VRAM BW\\n'\n",
      " 'In ~2015 the NV 980Ti achieved ~350 GBy/s VRAM BW\\n'\n",
      " 'In ~2013 the NV 780Ti achieved ~336 GBy/s VRAM BW\\n'\n",
      " 'In ~2012 the NV 680 achieved ~192 GBy/s VRAM BW\\n'\n",
      " 'In ~2010 the NV 580 achieved ~192 GBy/s VRAM BW\\n'\n",
      " '\\n'\n",
      " \"So for way more than 1 decade we've been able to get GPUs with vastly more \"\n",
      " 'RAM BW than current high-mid end consumer desktops.\\n'\n",
      " 'Even today and for years past \"server\" PCs have 2x, 4x, etc. the RAM BW '\n",
      " 'than\\n'\n",
      " 'consumer desktops, they use better (ECC, registered, many more slots '\n",
      " 'available) RAM as well.\\n'\n",
      " '\\n'\n",
      " 'And for the past few years in the high end consumer space the Mac M series '\n",
      " 'has been utterly embarrassing the Intel / AMD PC with its unified memory '\n",
      " 'performance.\\n'\n",
      " '\\n'\n",
      " \"Right now we've got L1, L2, L3 cache, then a pretty large gap of BW/latency \"\n",
      " \"then DRAM which still doesn't reach 256 GBy easily available on high end \"\n",
      " 'consumer non HEDT desktops, and which is vastly slower than our GPUs.\\n'\n",
      " '\\n'\n",
      " 'Maybe ONE THING we need to close the AIML inteferencing problem\\n'\n",
      " 'BUT ALSO improve HPC and performance computing IN GENERAL is for\\n'\n",
      " 'our personal workstations to not be artificially crippled (market '\n",
      " 'segmentation) by\\n'\n",
      " 'RAM channel count / RAM interface width / RAM BW so that we can actually\\n'\n",
      " 'get like 128-256-512 GBy or whatever of ECC RAM with 200-512 GBy/s BW!\\n'\n",
      " '\\n'\n",
      " 'In fact one could even devise a scheme where one has L1/L2/L3 cache, then\\n'\n",
      " 'some large-ish amount of wide fast RAM such as 64-256 GBy at \"level 4\",\\n'\n",
      " 'then if you want to expand to more narrow slower RAM maybe have that able\\n'\n",
      " 'to go out to another 256GB or something at lower performance if that makes\\n'\n",
      " 'any architectural sense vs. just keeping it all wide.\\n'\n",
      " '\\n'\n",
      " 'Right now (still) the consumer desktop capability is being victimized by '\n",
      " 'artificial crippling of things like ECC, RAM slots, and RAM BW.  Also PCIE '\n",
      " 'lane count, slot count / usability.\\n'\n",
      " '\\n'\n",
      " 'Right now (still) the consumer GPU capability is being victimized by '\n",
      " 'artificial crippling of VRAM size.\\n'\n",
      " '\\n'\n",
      " \"In both cases we're the victims of poorly built / poor quality / poor \"\n",
      " 'support of the consumer HW, bad warranties (i.e. only 3y on RTX 4090 '\n",
      " \"straight from nvidia whereas a few years ago we'd get lifetime warranty from \"\n",
      " 'XFX/EVGA).\\n'\n",
      " '\\n'\n",
      " 'And the ability to even install scalable numbers of GPUs, M.2 NVME drives, '\n",
      " 'or other PCIE x16 cards in a consumer desktop is pitiful wrt. PCIE lanes and '\n",
      " 'even more so with usable slot availability vs cooling etc.\\n'\n",
      " '\\n'\n",
      " 'We need an improved PC architecture to better scale HPC / edge AIML.\\n'\n",
      " 'We need less \"GPUs\" and more capability to enjoy the BW through the\\n'\n",
      " 'system as a whole whether the workload is graphics, HPC, ML, etc.\\n'\n",
      " 'Sure we need better NPU options, but this \"1-2 PCIE x8 slots and 4 DIMMS if '\n",
      " 'you\\'re lucky\" bit needs to radically improve in any case.\\n'\n",
      " '\\n'\n",
      " 'Take a look at the grace hopper design from nvidia.\\n'\n",
      " \"AFAIK they're bringing that sort of thing to the desktop in a year or two:\\n\"\n",
      " '\\n'\n",
      " 'https://www.phoronix.com/review/nvidia-gh200-gptshop-benchmark\\n'\n",
      " '\\n'\n",
      " 'AMD and Intel will be getting their lunches eaten by nvidia, apple who \"get '\n",
      " 'it\" moreso about evolving the platform architecture holistically.\\n'\n",
      " '- This problem is created intentionally to milk massive profits out of '\n",
      " 'enterprises that will pay any sticker price Nvidia puts on their latest '\n",
      " '\"enterprise\" GPU.\\n'\n",
      " '\\n'\n",
      " 'Why would they risk cannibalizing this market?  \"Screw hobbyists\" is a '\n",
      " 'rational business decision.\\n'\n",
      " '- $1k 3090, 24gb, decent cuda cores. Very close to what you want, no?\\n'\n",
      " '  - Remove all graphic related stuff (video cores, hdmi/DP, etc) and replace '\n",
      " 'it with double memory and than it will be good for this price. \\n'\n",
      " '\\n'\n",
      " 'What (other than low power consumption) can a single 3090 do compared to 4x '\n",
      " 'p100s for the same money? Both can Exl2, and the difference between 64gb vs '\n",
      " '24gb will give more benefit than the difference in speed.\\n'\n",
      " \"    - most motherboards/cases won't accept two GPUs, never mind 4.\\n\"\n",
      " '- A CPU or even a blackhole is sufficient.\\n'\n",
      " '- A Tesla M10 is available for ‚Ç¨350, 32gb VRAM - (4\\\\*8gb) - is older and '\n",
      " \"slightly slower but for most stuff it's absolutely fine. (llama.cpp, \"\n",
      " 'stablediffusion and LoRA are all fast, the only small gripe is '\n",
      " 'stablediffusion not scaling across multiple GPUs)\\n'\n",
      " \"  - oh, second gripe is the jank cooling I set up for them - you'll need a \"\n",
      " 'fan for it :)\\n'\n",
      " '    - Man.. I got no idea how you are making it on maxwell..\\n'\n",
      " '      - Works fine, bit slower token rate but still order of magnitude '\n",
      " 'faster than CPU and for ‚Ç¨300 works fine for experimenting :)\\n'\n",
      " '- Because consumer electronics market is low margin. You can only make it '\n",
      " 'work if you can ship enormous volume (and there is demand for it). Thus only '\n",
      " 'AMD, Intel, Nvidia is in a position to target this market. All the startups '\n",
      " 'are targeting high margin servers or high volume IOT.\\n'\n",
      " \"- because it's like still day 0 really for all this?\\n\"\n",
      " \"- Why can't FPGA's be given more ram and used for inference?\\n\"\n",
      " '\\n')\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(data_chunks[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Q: What is the current capacity and bandwidth of consumer desktop RAM '\n",
      " 'compared to that of high-end GPUs?\\n'\n",
      " 'A: Current high-end consumer desktops have up to 64GB of RAM with a maximum '\n",
      " 'bandwidth of around 32 GBy/s, while high-end GPUs can have VRAM sizes '\n",
      " 'ranging from 12GB to over 256GB and VRAM bandwidths exceeding 500 GBy/s.\\n'\n",
      " '---\\n'\n",
      " 'Q: What is the significance of GPU architecture evolution for HPC and AIML?\\n'\n",
      " 'A: The lack of improvement in consumer desktop hardware, such as ECC RAM '\n",
      " 'capacity, number of memory channels, and available PCIe lanes, hinders the '\n",
      " 'development of high-performance computing (HPC) and artificial intelligence '\n",
      " 'machine learning (AIML) applications. GPUs have vastly more VRAM bandwidth '\n",
      " 'than current consumer desktops, which is crucial for HPC and AIML '\n",
      " 'workloads.\\n'\n",
      " '---\\n'\n",
      " 'Q: What is the Tesla M10 GPU and what are its advantages?\\n'\n",
      " 'A: The Tesla M10 is a GPU designed for server use, featuring 32GB of VRAM '\n",
      " \"and relatively high compute capabilities. It's an older model with slightly \"\n",
      " 'slower performance but offers a cost-effective solution for experimenting '\n",
      " 'with AIML applications. Its low price point makes it an attractive '\n",
      " 'alternative to more expensive options like the RTX 4090.\\n'\n",
      " '---\\n'\n",
      " 'Q: What are some challenges in scaling consumer desktop systems for '\n",
      " 'high-performance computing and AIML?\\n'\n",
      " 'A: Consumer desktops face limitations in terms of memory capacity, VRAM '\n",
      " 'bandwidth, number of available PCIe lanes, and cooling capabilities which '\n",
      " 'hinder the scalability of HPC and AIML applications. This limits their '\n",
      " 'potential performance compared to dedicated server hardware or specialized '\n",
      " 'GPUs.')\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(response_chunks[99]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle file\n",
    "with open('./_output/new/response1.pkl', \"wb\") as file:\n",
    "    pickle.dump(response_chunks, file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
