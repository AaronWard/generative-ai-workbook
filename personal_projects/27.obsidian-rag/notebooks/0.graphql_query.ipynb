{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL query with Obsidian Notes\n",
    "\n",
    "I am exposing the notes as a local GraphQL server using this [plugin](https://github.com/TwIStOy/obsidian-local-graphql). \n",
    "Access the Apollo client UI by going to this link [http://localhost:28123/](http://localhost:28123/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl --request POST \\\n",
    "#   --header 'content-type: application/json' \\\n",
    "#   --url http://localhost:28123/ \\\n",
    "#   --data '{\"query\":\"query { __typename }\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_NEG_SCORE = -3\n",
    "\n",
    "def plot_graph(question, result, search_type):\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    G.add_node(question, size=20, title=question)\n",
    "\n",
    "    # Parse JSON and add nodes and edges\n",
    "    for item in result[\"vault\"][search_type]:\n",
    "        if item[\"score\"] > MIN_NEG_SCORE:\n",
    "            file_name = item[\"file\"][\"name\"]\n",
    "            score = item[\"score\"]\n",
    "\n",
    "            print(file_name)\n",
    "            print(item['file']['readContent'])\n",
    "            print(\"-----------------------------\")\n",
    "\n",
    "            # Adding each file as a node\n",
    "            G.add_node(file_name, size=10, title=f\"Score: {score}\")\n",
    "            \n",
    "            # Adding an edge from each file to the central query node, weight is the inverse of score for visualization\n",
    "            G.add_edge(question, file_name, weight=score)\n",
    "\n",
    "    # Convert to a PyVis network for visualization\n",
    "    nt = Network(\"500px\", \"1000px\", notebook=True)\n",
    "    nt.from_nx(G)\n",
    "    nt.show(\"fuzzy_search_graph.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter-Efficient Fine-Tuning (PEFT).md\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - generative-ai\n",
      "  - llm\n",
      "---\n",
      "> [[Large Language Model]], [[Generative AI]], [[Fine Tuning]]\n",
      "\n",
      "https://github.com/huggingface/peft\n",
      "\n",
      "**Parameter-Efficient Fine-Tuning (PEFT):**\n",
      "   - PEFT techniques aim to reduce the computational costs of fine-tuning by limiting parameter updates.\n",
      "   - One approach, Low-Rank Adaptation (LoRA), posits that not all parameters need updating for effective fine-tuning. By focusing on a low-dimension matrix representative of the task, LoRA can substantially reduce fine-tuning costs.\n",
      "-----------------------------\n",
      "Quantization.md\n",
      "> [[Large Language Model]], [[QLoRA]],\n",
      "### What is Quantization?\n",
      "\n",
      "![[Screenshot 2024-03-05 at 13.40.43.png]]\n",
      "\n",
      "- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\n",
      "- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions\n",
      "-----------------------------\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "fuzzy_search_graph.html\n"
     ]
    }
   ],
   "source": [
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport\n",
    "\n",
    "# Configure the client\n",
    "transport = RequestsHTTPTransport(\n",
    "    url='http://localhost:28123/',  # Make sure this URL is correct\n",
    "    use_json=True,\n",
    "    headers={'Content-Type': 'application/json'},\n",
    "    verify=True,\n",
    "    retries=3,\n",
    ")\n",
    "\n",
    "question = \"LoRA\"\n",
    "search_type = \"simpleSearch\"  # assuming this is the correct field name you want to replace\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "# Define your query with the variable\n",
    "string = f\"\"\" \\\n",
    "query ExampleQuery($query: String!) {{\n",
    "  vault {{\n",
    "    {search_type}(query: $query) {{\n",
    "      file {{\n",
    "        name\n",
    "        readContent\n",
    "        stat {{\n",
    "          size\n",
    "        }}\n",
    "      }}\n",
    "      score\n",
    "    }}\n",
    "  }}\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "query = gql(string)\n",
    "# Variables dictionary\n",
    "variables = {\n",
    "    \"query\": question\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(query, variable_values=variables)\n",
    "# print(result)\n",
    "\n",
    "# Assuming plot_graph is a function you have defined earlier to create and display the graph\n",
    "plot_graph(question, result, search_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vault': {'simpleSearch': [{'file': {'name': 'QLoRA.md',\n",
       "     'readContent': '---\\ntags:\\n  - fine-tuning\\n  - generative-ai\\n---\\n> [[Fine Tuning]], [[Fine Tuning]], [[Generative AI]], [[Quantization]]\\n### QLoRA\\n\\nThere are 4 components to QLoRA:\\n1. **4-bit NormalFloat**\\n\\t- This is essentially just a way to bucket values\\n\\t- Representing a value within 4 binary values\\n\\t- *ex: `0101`*\\n\\t- 16 Unique unique combinations ![[Screenshot 2024-03-05 at 13.48.24.png]]\\n2. **Double Quantization**\\n\\t- \\n3. **Paged Optimizers**\\n\\t- Used to provide higher utilization of memory across resources\\n\\t- Instead of being restricted to GPU only memory, you can move pages of memory to the CPU memory, and retrieve when required.\\n4. **LoRA**\\n\\t- Full finetuning would involved reconfiguring weights for entire model - LoRA allows you to  add a small number of trainable parameters to the \\n\\n![[Screenshot 2024-03-05 at 14.22.14.png]]',\n",
       "     'stat': {'size': 827}},\n",
       "    'score': -1.5467},\n",
       "   {'file': {'name': 'Quantization.md',\n",
       "     'readContent': '> [[Large Language Model]], [[QLoRA]],\\n### What is Quantization?\\n\\n![[Screenshot 2024-03-05 at 13.40.43.png]]\\n\\n- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\\n- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions',\n",
       "     'stat': {'size': 390}},\n",
       "    'score': -0.079}]}}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variables = {\n",
    "    \"query\": \"QLoRA\"\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(query, variable_values=variables)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
