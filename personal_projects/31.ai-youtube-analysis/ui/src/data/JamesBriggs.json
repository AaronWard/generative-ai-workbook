[
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "AI Agents as Neuro-Symbolic Systems?",
        "description": "Thinking through AI agents and the neuro-symbolic definition from an early LLM agent paper called MRKL. I'm sharing my reasoning behind using the \"neuro-symbolic system\" definition for AI agents.\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#artificialintelligence #ai #python #machinelearning \n\n00:00 AI Agents\n02:04 ReAct Agents\n07:28 Redefining Agents\n12:48 Origins of Connectionism\n17:23 Neuro-symbolic AI\n21:09 Agents without LLMs\n25:21 Broader Definition of Agents",
        "summary": "In this video, the presenter discusses a shift in thinking about AI agents, focusing on a broader definition that goes beyond the typical understanding of agents as just LLMs (Large Language Models) with tool-calling capabilities. The presenter explains that their work is part of a larger project which has kept them away from posting regularly.\n\nThey introduce the concept of a react agent, which uses multiple reasoning steps and tool interactions to answer queries, highlighting the limitations of defining agents in this way. The video emphasizes the need to redefine agents through a neuro-symbolic system approach, which combines neural network-based AI (like LLMs) with symbolic AI (like code and logic), offering a more holistic view of AI agents.\n\nThe presenter references the early LLM agent paper called MRKL, which described systems as neuro-symbolic architectures, integrating neural and symbolic components. They also delve into the history of AI, explaining the origins of connectionism\u2014a neural network-based approach\u2014and its development over the years alongside symbolic AI.\n\nAdditionally, the presenter stresses the importance of using various neural network models beyond LLMs to create more comprehensive and effective AI agents. They share examples of how embedding models can be used in AI systems to enhance efficiency and control, illustrating these concepts with tools like Semantic Router.\n\nOverall, the video is a reflection on redefining AI agents to include a mix of neural and symbolic elements, advocating for a more flexible and inclusive understanding of what constitutes an AI agent. This broader perspective aims to improve the practical application and effectiveness of AI systems in various contexts.",
        "categories": [
            "Agents",
            "Chain of thought reasoning",
            "Philosophical reasoning and ethics",
            "In-context learning",
            "Rewriting",
            "Executing code",
            "Search",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=JaHfCrVTYF4",
        "published_at": "2024-11-19T14:31:29Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Llama Index Workflows | Building Async AI Agents",
        "description": "Llama Index Workflows is an event-driven framework for building AI agents. It aims to provide AI Engineers with a structured conceptual frame around which we can build AI software, similar in some respects to LangChain's LangGraph. In this video, we'll compare the two frameworks (Llama Index Workflows and Langchain's LangGraph), learn how to use Workflows, and build an async research agent with the library.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/llama-index/llama-index-research-agent.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#artificialintelligence #llamaindex #coding #programming \n\n00:00 Llama Index Workflows\n00:53 Llama Index vs. LangGraph\n05:27 Python Prerequisites\n06:40 Building Knowledge Base\n08:20 Defining Agent Tools\n11:02 Defining the LLM\n12:31 Llama Index Workflow Events\n14:00 Llama Index Agent Workflow\n24:25 Debugging our Workflow\n26:47 Using and Tweaking our Agent\n30:05 Testing Llama Index Async",
        "summary": "## Video Title - `Llama Index Workflows for AI Agent Development`\n\n## Video Description - `Llama Index Workflows is an event-driven framework for building AI agents. It aims to provide AI Engineers with a structured conceptual frame around which we can build AI software, similar in some respects to LangChain's LangGraph. In this video, we'll compare the two frameworks (Llama Index Workflows and Langchain's LangGraph), learn how to use Workflows, and build an async research agent with the library.`\n\n### Transcript Summary\n\nIn this video, the presenter introduces the Llama Index Workflows, an event-driven framework designed for developing AI agents, and compares it with LangChain's LangGraph. The key difference highlighted is Llama Index's higher-level abstractions and event-driven nature, which contrasts with LangGraph's node and edge connection approach. The discussion emphasizes how Llama Index allows for defining steps and events that trigger these steps, offering a structured approach to building agentic flows.\n\nThe presenter also notes that Llama Index prioritizes async operations, which, while increasing the learning curve, results in more scalable and performant systems due to efficient handling of tasks like LLM processing. This contrasts with synchronous operations where the system waits for LLM responses.\n\nThe video demonstrates setting up a research agent using Llama Index, highlighting components like rag search, web search, and archive search tools. These are integrated into the workflow to construct a complex AI research agent. The code implementation focuses on using Python prerequisites, defining agent tools and LLMs, and managing workflow events and steps.\n\nBoth frameworks are acknowledged to have their pros and cons, depending on the developer's needs and the specific application. The video concludes by showcasing the benefits of using async in Llama Index Workflows, presenting it as a potentially more scalable solution compared to LangChain's more traditional synchronous systems.",
        "categories": [
            "Agents",
            "Framework or Library",
            "Executing code",
            "Querying Data"
        ],
        "url": "https://www.youtube.com/watch?v=KMZBLBAfE1s",
        "published_at": "2024-09-24T14:00:16Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Local LangGraph Agents with Llama 3.1 + Ollama",
        "description": "LangGraph is one of the most versatile Python libraries for building AI agents. We can combine LangChain's LangGraph with Ollama and Llama 3.1 to build highly custom and fully local LLM agents. In this video, we will do exactly that by building a pizza recommendation agent using the Reddit API.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent\n\n\ud83d\udcbb LangGraph Intro Video:\nhttps://youtu.be/usOmwLZNVuM\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#langchain #ai #artificialintelligence #ollama #meta \n\n00:00 Local Agents with LangGraph and Ollama\n01:00 Setting up Ollama and Python\n05:35 Reddit API Tool\n12:40 Overview of the Graph\n17:11 Final Answer Tool\n18:33 Agent State\n19:09 Ollama Llama 3.1 Setup\n26:21 Organizing Agent Tool Use\n35:21 Creating Agent Nodes\n39:14 Building the Agent Graph\n43:10 Testing the Llama 3.1 Agent\n46:07 Final Notes on Local Agents",
        "summary": "In this video, the presenter demonstrates how to build a local AI agent using LangGraph, Ollama, and Llama 3.1, focusing on creating a pizza recommendation agent with the Reddit API. The video details the setup process, including downloading Ollama, setting up a Python environment, and managing dependencies with pip and poetry. The presenter explains the use of LangGraph, an open-source library from LangChain, to create agents in a graph-like structure, and highlights the benefits of running local LLMs such as Llama 3.1. \n\nThe video provides a comprehensive walkthrough of building the agent, covering the setup of the Reddit API for gathering data, and the creation of a graph structure for the agent. The structure includes components like an Oracle for decision-making, a search tool for gathering data, and a final answer tool for delivering recommendations. The presenter discusses the challenges faced, such as ensuring the agent uses tools effectively, and the limitations of using a smaller model like the 8 billion parameter Llama 3.1.\n\nFurthermore, the video addresses the importance of structured output formats and JSON mode for reliable tool use, and suggests improvements for handling multiple tools and steps within the agent. The video concludes with testing the agent's effectiveness in recommending pizza places in Rome, showcasing the capability of local LLMs to perform complex tasks efficiently, even with limited computational resources.\n\n---\n\n## Video Title - Building Local AI Agents with LangGraph and Ollama\n\n## Video Description - LangGraph is one of the most versatile Python libraries for building AI agents. We can combine LangChain's LangGraph with Ollama and Llama 3.1 to build highly custom and fully local LLM agents. In this video, we will do exactly that by building a pizza recommendation agent using the Reddit API. \n\n\ud83d\udccc Code: https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/langgraph/02-ollama-langgraph-agent\n\n\ud83d\udcbb LangGraph Intro Video: https://youtu.be/usOmwLZNVuM\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos: https://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting: https://aurelio.ai\n\n\ud83d\udc7e Discord: https://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#langchain #ai #artificialintelligence #ollama #meta \n\n---\n\nOverall, the video is a practical guide for developers interested in building local AI agents using modern open-source tools and frameworks, providing a deep dive into the technical aspects and practical applications of such systems.",
        "categories": [
            "Agents",
            "Framework or Library",
            "APIs",
            "Infrastructure",
            "Data, Text and Code generation"
        ],
        "url": "https://www.youtube.com/watch?v=5a-NuqTaC20",
        "published_at": "2024-08-29T13:00:07Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "LangGraph Deep Dive: Build Better Agents",
        "description": "LangGraph is an agent framework from LangChain that allows us to develop agents via graphs. By building agents using graphs we have much more control and flexibility in our AI agent execution path.\n\nIn this video, we will build an AI research agent using LangGraph. Research agents are multi-step LLM agents that can produce in-depth research reports on a topic of our choosing through multiple steps.\n\nWe will see how we can build our own AI research agent using gpt-4o, Pinecone, LangGraph, arXiv, and Google via the SerpAPI.\n\n\ud83d\udccc Code:\nhttps://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/01-gpt-4o-research-agent.ipynb\n\n\ud83d\udcd6 Article:\nhttps://www.pinecone.io/learn/langgraph-research-agent/\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#artificialintelligence #langchain #llm #python #rag \n\n00:00 LangGraph Agents\n02:04 LangGraph Agent Overview\n04:46 Short History of Agents and ReAct\n07:58 Agents as Graphs\n10:18 LangGraph\n12:41 Research Agent Components\n14:30 Building the RAG Pipeline\n17:28 LangGraph Graph State\n18:56 Custom Agent Tools\n19:10 ArXiv Paper Fetch Tool\n21:22 Web Search Tool\n22:42 RAG Tools\n23:57 Final Answer Tool\n25:10 Agent Decision Making\n30:16 LangGraph Router and Nodes\n33:00 Building the LangGraph Graph\n36:52 Building the Research Agent Report\n39:39 Testing the Research Agent\n43:42 Final Notes on Agentic Graphs",
        "summary": "The video introduces LangGraph, a framework from LangChain that allows for the development of AI agents using graph-based structures. This approach offers enhanced control and flexibility in executing complex multi-step tasks. The tutorial demonstrates building a research agent capable of producing in-depth reports by utilizing multiple AI tools such as GPT-4o, Pinecone, arXiv, and the SerpAPI for Google searches.\n\nThe key components of this research agent include:\n1. **Oracle**: Acts as the decision maker, using a large language model (LLM) with access to various tools for processing user queries.\n2. **Tools**: Custom tools including the ArXiv Paper Fetch Tool for retrieving academic papers, a Web Search Tool for general queries, RAG (Retriever Augmented Generation) tools for accessing a knowledge base, and a Final Answer tool for generating structured responses.\n3. **Graph Structure**: The agent\u2019s operations are mapped out as nodes in a graph, enhancing transparency and customizability compared to traditional object-oriented frameworks like ReAct.\n\nLangGraph's graph-based approach is portrayed as advantageous for creating transparent, customized agent workflows, offering a clearer understanding of processes and facilitating modifications tailored to specific use cases. The video emphasizes the potential of such frameworks in constructing sophisticated AI systems, particularly in scenarios requiring detailed, multi-source research capabilities. This development reflects a shift towards more modular and adaptable AI frameworks in the field of LLM and AI agent design. Overall, the video provides a comprehensive guide to deploying LangGraph for building advanced AI research agents.",
        "categories": [
            "In-context learning",
            "Agents",
            "Vector Databases",
            "Search",
            "Framework or Library",
            "Planning and Complex Reasoning"
        ],
        "url": "https://www.youtube.com/watch?v=usOmwLZNVuM",
        "published_at": "2024-08-07T13:03:33Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "RAG with Mistral AI!",
        "description": "We build an RAG pipeline using Mistral AI's mistral-embed and mistral-large, using Pinecone vector DB as our knowledge base.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/integrations/mistralai/mistral-rag.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#artificialintelligence #mistralai #rag #llm\n\n00:00 RAG with Mistral and Pinecone\n01:07 Mistral API in Python\n01:44 Setting up Vector DB\n03:14 Mistral Embeddings\n04:12 Creating Pinecone Index\n08:24 RAG with Mistral\n11:20 Final Thoughts on Mistral",
        "summary": "In this video, the presenter provides a detailed walkthrough of using the Mistral AI's API for Retrieval-Augmented Generation (RAG) with their models, mistral-embed and mistral-large, along with Pinecone as a vector database. Mistral AI is noted for its open-source approach to releasing models and offering API services. The video demonstrates how to set up the necessary environment, including installing prerequisites like Hugging Face datasets and the Mistral AI client, and configuring API keys for both Mistral and Pinecone.\n\nThe process involves downloading a semantically chunked dataset for embedding, restructuring data for Pinecone, and initializing connections to Mistral for embeddings. The presenter explains the importance of including both title and content in embeddings to provide more context, which enhances search capabilities later in the pipeline.\n\nThe video also covers the retrieval process using Mistral's embeddings and Pinecone, showing how to construct queries and retrieve relevant documents. The generation component uses the retrieved documents to generate responses with the mistral-large model, demonstrating how RAG can be applied in practical scenarios.\n\nThe presenter discusses the flexibility Mistral offers with various models, allowing users to choose based on latency and accuracy needs. They highlight the advantages of using Mistral's API alongside Pinecone for efficient data management and retrieval, providing a practical alternative to other AI service providers like Anthropic and OpenAI.\n\nOverall, the video is a technical demonstration of using Mistral AI's tools to build a RAG system, offering insights into embedding strategies, data management, and API integration. The content is relevant for those interested in AI frameworks, vector databases, and LLM applications, providing a comprehensive guide to setting up and utilizing these technologies.",
        "categories": [
            "In-context learning",
            "Vector Databases",
            "Data, Text and Code generation",
            "APIs",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=I0c405L7-9A",
        "published_at": "2024-07-11T13:00:16Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Superfast RAG with Llama 3 and Groq",
        "description": "Groq API provides access to Language Processing Units (LPUs) that enable incredibly fast LLM inference. The service offers several LLMs including Meta's Llama 3. In this video, we'll implement a RAG pipeline using Llama 3 70B via Groq, an open source e5 encoder, and the Pinecone vector database.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/integrations/groq/groq-llama-3-rag.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#artificialintelligence #llama3 #groq \n\n00:00 Groq and Llama 3 for RAG\n00:37 Llama 3 in Python\n04:25 Initializing e5 for Embeddings\n05:56 Using Pinecone for RAG\n07:24 Why We Concatenate Title and Content\n10:15 Testing RAG Retrieval Performance\n11:28 Initialize connection to Groq API\n12:24 Generating RAG Answers with Llama 3 70B\n14:37 Final Points on Why Groq Matters",
        "summary": "In this video, the presenter demonstrates the use of the Groq API in conjunction with Llama 3 for Rapid Application Generation (RAG). The Groq API provides access to Language Processing Units (LPUs), enabling rapid inference of large language models (LLMs). This video highlights the implementation of a RAG pipeline using the Llama 3 70B model via Groq, along with the use of an open-source e5 encoder and Pinecone vector database.\n\nThe video starts by explaining the Groq API and its benefits, particularly its ability to run LLMs with high token throughput, which is useful for applications needing fast processing speeds. The presenter goes on to demonstrate how to set up the environment using Google Colab, install necessary libraries like Hugging Face datasets, Groq API, Semantic Router, and Pinecone, and then downloads a semantic chunk dataset for embeddings.\n\nThe process involves using an E5 model for embedding with a larger context window to accommodate chunks that exceed typical token limits, allowing for better performance on real data. The presenter emphasizes the importance of including both title and content in embeddings to provide more context, which improves the relevance of search results.\n\nA test retrieval is conducted to showcase the speed and accuracy of the RAG process, which is then followed by pairing with Groq's 70 billion parameter Llama 3 model. The video concludes by underlining the speed and efficiency of Groq API, making it suitable for applications involving large models and agent flows. The presenter suggests that Groq's service could significantly enhance the performance of agent systems by reducing response times and easing the use of open-source LLMs.\n\nThis demonstration provides a comprehensive overview of integrating various technologies to achieve rapid LLM inference, making it a valuable resource for developers interested in utilizing large models efficiently.",
        "categories": [
            "In-context learning",
            "Agents",
            "Vector Databases",
            "APIs",
            "Infrastructure",
            "Search",
            "Data, Text and Code generation"
        ],
        "url": "https://www.youtube.com/watch?v=ne-lrm0n0bg",
        "published_at": "2024-07-02T13:00:53Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "NEW Pinecone Assistant",
        "description": "Pinecone Assistant is a new AI assistant service from Pinecone, bringing together the best of LLMs and GenAI with advanced Retrieval Augmented Generation (RAG) methods to reduce hallucination and optimize assistant reliability.\n\n\ud83d\udea9 Get Access:\nhttps://www.pinecone.io/product/pinecone-assistant/\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/pinecone-assistant/assistants-ai-demo.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 AI Assistants\n00:41 Pinecone Assistants in Python\n01:19 Building an AI Research Assistant\n02:11 Assistant Message and Chat\n03:05 Adding Files to the Assistant\n05:30 Chatting with our Assistant\n07:23 Assistant Chat History\n10:47 Asking about Mamba 2\n12:11 Wrapping up with Assistants",
        "summary": "In this video, the presenter introduces Pinecone Assistant, an innovative AI assistant service that enhances AI models using Retrieval Augmented Generation (RAG) methods to minimize hallucinations and improve reliability. The video demonstrates how to build an AI research assistant using Pinecone systems in Python. It guides viewers through installing prerequisites, creating an assistant, and integrating knowledge sources via PDF documents to ensure the assistant provides accurate, up-to-date information. The video showcases the assistant's ability to interact with users by answering questions grounded in real knowledge, supported by citations from the provided documents. It emphasizes the assistant\u2019s capability to handle complex queries with detailed responses, citing relevant pages from the uploaded documents. Additionally, the video discusses practical applications of the assistant, such as querying AI models like Mixture of Experts and Mamba 2, and highlights Pinecone\u2019s user-friendly interface for managing assistant data storage. Overall, the video is a tutorial on leveraging Pinecone Assistant to create AI models with enhanced knowledge grounding and reliability. This video is particularly relevant to topics such as AI, LLMs, Vector Databases, and Prompting, as it demonstrates a technological tool designed to improve AI capabilities.",
        "categories": [
            "In-context learning",
            "Agents",
            "Vector Databases",
            "Prompting",
            "Data, Text and Code generation",
            "Summarization",
            "Querying Data",
            "Framework or Library",
            "APIs",
            "Infrastructure"
        ],
        "url": "https://www.youtube.com/watch?v=MPCiNA8BqO8",
        "published_at": "2024-06-25T14:02:51Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Semantic Chunking - 3 Methods for Better RAG",
        "description": "Semantic chunking allows us to build more context-aware chunks of information. We can use this for RAG, splitting video and audio, and much more.\n\nIn this video, we will use a simple RAG-focused example. We will learn about three different types of chunkers: StatisticalChunker, ConsecutiveChunker, and CumulativeChunker.\n\nAt the end, we also discuss semantic chunking for video, such as for the new gpt-4o and other multi-modal use cases.\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-chunkers/blob/main/docs/00-chunkers-intro.ipynb\n\n\u2b50\ufe0f Article:\nhttps://www.aurelio.ai/learn/semantic-chunkers-intro\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#ai #artificialintelligence #chatbot #nlp \n\n00:00 3 Types of Semantic Chunking\n00:42 Python Prerequisites\n02:44 Statistical Semantic Chunking\n04:38 Consecutive Semantic Chunking\n06:45 Cumulative Semantic Chunking\n08:58 Multi-modal Chunking",
        "summary": "In this video, the presenter discusses three types of semantic chunkers used to intelligently segment text data for applications like retrieval-augmented generation (RAG). The focus is on text modality, though the methods can also be applied to video and audio. The three chunkers introduced are the StatisticalChunker, ConsecutiveChunker, and CumulativeChunker, all of which are part of the semantic chunkers library. \n\nThe StatisticalChunker is recommended for general use due to its efficiency and cost-effectiveness. It calculates similarity thresholds dynamically based on document variations, allowing for adaptable chunking. The ConsecutiveChunker, while also cost-effective and quick, requires manual input for the similarity threshold and may need adjustments for optimal performance. It works by splitting text into sentences and merging them based on similarity drops. The CumulativeChunker, the most computationally intensive, builds embeddings cumulatively and compares them to determine chunking points, offering noise resistance but at a higher cost and time.\n\nThe video also touches on the application of these chunkers for multi-modal data processing, particularly highlighting the suitability of the ConsecutiveChunker for video data. The presenter explains that while the StatisticalChunker is limited to text, the ConsecutiveChunker can handle various modalities, and the CumulativeChunker remains more text-focused.\n\nOverall, the video serves as a practical guide to utilizing semantic chunkers for improved data processing in AI applications, particularly focused on RAG. It also provides insights into the considerations for choosing the right chunking method based on specific use cases and data modalities.",
        "categories": [
            "In-context learning",
            "Multimodal models",
            "Agents",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=7JS0pqXvha8",
        "published_at": "2024-06-01T14:23:02Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Processing Videos for GPT-4o and Search",
        "description": "Recent multi-modal models like OpenAI's gpt-4o and Google's Gemini 1.5 models can comprehend video. When feeding video into these new models, we can push frames at a set frequency (for example, one frame every second) \u2014 but this method can be wildly inefficient and expensive.\n\nFortunately, there is a better method called \"semantic chunking.\" Semantic chunking is a common method used in text-based Retrieval-Augmented Generation (RAG), but we can apply the same logic to video using image embedding models. Using the similarity between these frames, we can effectively split videos based on the semantic meaning of the constituent frames.\n\nIn this video, we'll explore how to use two test videos and chunk them into semantic blocks.\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-chunkers/blob/main/docs/01-video-chunking.ipynb\n\n\ud83d\udcd6 Article:\nhttps://www.aurelio.ai/learn/video-chunking\n\n\u2b50 Repo:\nhttps://github.com/aurelio-labs/semantic-chunkers\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#ai #artificialintelligence #openai \n\n00:00 Semantic Chunking\n00:24 Video Chunking and gpt-4o\n01:59 Video Chunking Code\n03:28 Setting up the Vision Transformer\n05:56 ViT vs. CLIP and other models\n06:40 Video Chunking Results\n08:37 Using CLIP for Vision Chunking\n11:29 Final Conclusion on Video Processing",
        "summary": "The video discusses the concept of semantic chunking as a more efficient method for processing video content for AI models, particularly those that include vision capabilities. The presenter explains that traditional methods of feeding video frames at a fixed frequency into AI models can be inefficient and costly, especially with fast or slow-moving videos. Instead, semantic chunking identifies changes in video content, allowing models to process only meaningful frames and optimize resources.\n\nThe video demonstrates this technique using the Semantic Chunkers library, which applies semantic chunking to videos through a vision transformer model and the OpenCV library for image processing. The presenter showcases how different models, including the Vision Transformer and CLIP models, can be used to semantically divide videos into chunks based on content changes, using test videos as examples. By adjusting the sensitivity threshold of these models, the granularity of chunking can be controlled, offering flexibility in how detailed the content separation is.\n\nThe video highlights that while models like the Vision Transformer are good for broader classification, more recent models like CLIP, which focus on semantic similarity, may offer better performance in identifying nuanced changes in video content. This makes them potentially more effective for applications that require more detailed understanding of video frames.\n\nThe main takeaway is that semantic chunking provides a smarter way to manage video data for AI systems, reducing processing time and costs by focusing only on significant frames. This method is particularly beneficial for use cases where video frames are fed into expensive and time-consuming AI models, offering a more efficient and cost-effective solution.\n\nThe presenter also points out that while the primary focus is on optimizing video processing for AI models, semantic chunking could have other applications as well. The video provides practical insights and demonstrations on how different models and settings can impact the results of semantic chunking, making it a valuable resource for anyone looking to improve video processing efficiency in AI applications.\n\nIn summary, the video is a demonstration of the semantic chunking technique for video processing, showcasing its benefits and practical implementation using different models and settings to optimize AI resources.\n\n### Video Title - Semantic Chunking for Video Processing\n\n### Video Description - Recent multi-modal models like OpenAI's gpt-4o and Google's Gemini 1.5 models can comprehend video. When feeding video into these new models, we can push frames at a set frequency (for example, one frame every second) \u2014 but this method can be wildly inefficient and expensive.\n\n### Transcript - Provided in task context.",
        "categories": [
            "Multimodal models",
            "Image classification and generation (If multi-modal)",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=gxqdNl1nTYw",
        "published_at": "2024-05-21T15:59:01Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "NVIDIA's NEW AI Workbench for AI Engineers",
        "description": "NVIDIA AI Workbench (NVWB) is a software toolkit designed to help AI engineers and data scientists build in GPU-enabled environments.\n\nUsing NVWB, we can set up a local AI project with a prebuilt template with a few clicks. Then, after building out our project locally, we can quickly deploy it to a more powerful remote GPU instance, switch to a different remote, or go back to local.\n\nBy abstracting away many repetitive and tedious boilerplate actions, NVWB aims to help AI engineers focus on the core of AI development. It helps us reduce time spent on managing our dev environment, deployments, or maintaining remote compute instances.\n\nIn this tutorial, we'll learn about NVWB's features, where to use it, and how to use it.\n\nI'm using NVIDIA RTX 5000 Ada Generation Laptop GPU here from Dell Precision AI-ready workstations \u2014 enabling substantial performance across AI projects and streamlining both training and development phases while still keeping things lightweight for travel. More info here https://dell.com/precisionai\n\n\ud83d\udccc Download NVWB:\nhttps://nvda.ws/4acZNRZ\n\n\ud83d\udcd6 Read the Article:\nhttps://www.aurelio.ai/learn/ai-workbench-intro\n\n\ud83d\udcbb Deploying remote EC2 instances for AI Workbench:\nhttps://www.aurelio.ai/learn/ai-workbench-remote\n\n\ud83d\udccc Code:\nhttps://github.com/NVIDIA/workbench-example-rapids-cudf/blob/main/code/cudf-pandas-demo.ipynb\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 NVIDIA AI Workbench\n01:18 Installing AI Workbench\n04:05 Sponsor Segment\n05:46 AI Workbench Locations\n06:54 Creating and Loading Projects\n09:21 AI Workbench Projects\n14:18 Jupyterlab in AI Workbench\n17:46 Using CuDF and Pandas\n19:51 Finishing up with AI Workbench\n\n#ai #artificialintelligence #chatbot #nlp  #ad",
        "summary": "In this video, the presenter discusses NVIDIA's new AI Workbench, a software toolkit designed to assist AI engineers and data scientists in building projects within GPU-enabled environments. Nvidia's AI Workbench abstracts many complex aspects of AI development, allowing users to focus more on building and less on managing environments. It facilitates easy switching between local and remote GPU instances, which is particularly useful for handling compute-intensive AI tasks.\n\nThe video provides a step-by-step guide to setting up NVIDIA AI Workbench, highlighting necessary installations such as Docker Desktop for Windows users or Podman as an alternative. It also covers the installation of GPU drivers, tailored to different GPU models like GeForce and RTX. The presenter uses an NVIDIA RTX 5000 Ada Generation Laptop GPU for demonstration, emphasizing the convenience of running AI tasks on a CUDA-enabled GPU locally.\n\nThe video explains the process of creating and managing projects within AI Workbench, where each project is built within a container instance, either Podman or Docker. The presenter demonstrates using JupyterLab to run a demo project from NVIDIA, showcasing GPU-accelerated data processing with CuDF and Pandas. The demonstration illustrates significant speed improvements using CuDF compared to traditional Pandas, highlighting the efficiency of GPU acceleration.\n\nOverall, the presenter positions NVIDIA AI Workbench as a practical solution for data scientists and AI engineers, particularly those less familiar with Docker and CUDA, to streamline AI development and reduce setup complexities. The video provides links to download NVWB, additional resources, and example code for further exploration. This content is categorized under topics such as Infrastructure, Framework or Library, and Data, Text, and Code generation. The video serves as an introduction and tutorial rather than sharing news or providing advice. It focuses on demonstrating a tool and discussing its features and setup process.",
        "categories": [
            "Infrastructure",
            "Framework or Library",
            "Data, Text and Code generation"
        ],
        "url": "https://www.youtube.com/watch?v=DcrFL_zNRKM",
        "published_at": "2024-05-16T18:52:33Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Semantic Chunking for RAG",
        "description": "Semantic chunking for RAG allows us to build more concise chunks for our RAG pipelines, chatbots, and AI agents. We can pair this with various LLMs and embedding models from OpenAI, Cohere, Anthropic, etc, and libraries like LangChain or CrewAI to build potentially improved Retrieval Augmented Generation (RAG) pipelines.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/02b-semantic-chunking.ipynb\n\n\ud83d\udea9 Intro to Semantic Chunking:\nhttps://www.aurelio.ai/learn/semantic-chunkers-intro\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Semantic Chunking for RAG\n00:45 What is Semantic Chunking\n03:31 Semantic Chunking in Python\n12:17 Adding Context to Chunks\n13:41 Providing LLMs with More Context\n18:11 Indexing our Chunks\n20:27 Creating Chunks for the LLM\n27:18 Querying for Chunks\n\n#artificialintelligence #ai #nlp #chatbot #openai",
        "summary": "In this video, the presenter explores the concept of semantic chunking as a method to enhance retrieval augmented generation (RAG) performance in AI systems. Semantic chunking involves embedding documents in a way that optimally captures single meanings within each chunk, rather than compressing multiple meanings into one. This approach contrasts with traditional chunking methods that rely on token count or delimiters, often leading to diluted meanings. \n\nThe video provides a detailed walkthrough of implementing semantic chunking in a Python-based RAG pipeline. It involves creating concise chunks optimized for specific embedding models, illustrated with the use of the OpenAI text embedding model. The process includes initializing datasets, embedding chunks, storing them in Pinecone, and querying the data.\n\nAdditionally, the presenter discusses adding context to chunks by integrating titles or hierarchical elements, thus providing richer information for the embedding model. There are also methods to enhance the chunks for large language models (LLMs) by incorporating surrounding context. Tools and libraries such as Pinecone, Hugging Face datasets, and OpenAI's embedding models are demonstrated. \n\nThe presenter emphasizes the importance of aligning chunking strategies with the embedding models used, and provides insights on adjusting parameters like chunk size and similarity thresholds to optimize RAG pipelines. The video also highlights the potential of semantic chunking to improve performance in AI-driven chatbots and agents when paired with various LLMs and embedding models from providers like OpenAI and Cohere.\n\n## Video Title - \"Semantic Chunking for RAG\"\n\n## Video Description - \"Semantic chunking for RAG allows us to build more concise chunks for our RAG pipelines, chatbots, and AI agents. We can pair this with various LLMs and embedding models from OpenAI, Cohere, Anthropic, etc., and libraries like LangChain or CrewAI to build potentially improved Retrieval Augmented Generation (RAG) pipelines.\"",
        "categories": [
            "In-context learning",
            "Vector Databases",
            "Prompting",
            "Classification",
            "Data, Text and Code generation",
            "Querying Data",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=TcRRfcbsApw",
        "published_at": "2024-05-04T14:30:16Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "LangGraph 101: it's better than LangChain",
        "description": "LangGraph is a special LangChain-built library that builds intelligent AI Agents using graphs. Ie, agentic state machines. It allows us to build more powerful and flexible AI agents than what we can build using just the core library, LangChain.\n\nIn this video, we'll see how to build agents with LangGraph and OpenAI.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/00-langgraph-intro.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#ai #langchain #artificialintelligence #nlp #chatbot #openai \n\n00:00 Intro to LangGraph\n00:52 Graphs in LangGraph\n03:00 More Complex LangGraph Agent\n08:12 LangGraph Graph State\n14:00 LangGraph Agent Node\n17:08 Forcing a Specific LLM Output\n20:00 Building the Graph\n23:23 Using our Agent Graph\n28:32 LangGraph vs LangChain",
        "summary": "## Video Title - Not Provided\n\n## Video Description - LangGraph is a special LangChain-built library that builds intelligent AI Agents using graphs. Ie, agentic state machines. It allows us to build more powerful and flexible AI agents than what we can build using just the core library, LangChain.\n\nIn this video, we'll see how to build agents with LangGraph and OpenAI.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/langgraph/00-langgraph-intro.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n#ai #langchain #artificialintelligence #nlp #chatbot #openai\n\n### Summary\nIn this video, the presenter introduces LangGraph, a new library within the LangChain ecosystem, designed to create more powerful and flexible AI agents by using graphs rather than the traditional object-oriented approach of LangChain. LangGraph emphasizes the use of nodes and edges in graph structures to define the flow and decision-making processes of agents, enhancing their functionality and potential in AI applications.\n\nThe video demonstrates how LangGraph can be used to build agents with a focus on flexibility and control, showcasing its ability to force specific outputs and manage errors more effectively than LangChain. The presenter also details the process of setting up a graph, defining nodes and edges, and integrating it with OpenAI's tools to improve the agent's decision paths and outputs.\n\nThe video serves as an introductory guide, comparing LangGraph with LangChain, highlighting the advantages of using graphs for agent development, such as reduced abstraction and improved transparency of processes. The presenter concludes by suggesting LangGraph as a preferred method for developing AI agents due to its extensibility and simplicity compared to LangChain's more convoluted approach. This video is particularly relevant for topics related to AI agents, frameworks, and libraries, especially in the context of using OpenAI and LangChain technologies.",
        "categories": [
            "Agents",
            "Framework or Library",
            "Planning and Complex Reasoning",
            "Querying Data"
        ],
        "url": "https://www.youtube.com/watch?v=qaWOwbFw3cs",
        "published_at": "2024-04-23T13:30:31Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "AI Agent Evaluation with RAGAS",
        "description": "RAGAS (RAG ASsessment) is an evaluation framework for RAG pipelines. Here, we see how to use RAGAS for evaluating an AI agent built using LangChain and using Anthropic's Claude 3, Cohere's embedding models, and the Pinecone vector database.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/better-rag/03-ragas-evaluation.ipynb\n\n\ud83d\udcd5 Article:\nhttps://www.pinecone.io/learn/series/rag/ragas/\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 RAG Evaluation\n00:39 Overview of LangChain RAG Agent\n03:04 RAGAS Code Prerequisites\n03:40 Agent Output for RAGAS\n05:14 RAGAS Evaluation Format\n08:04 RAGAS Metrics\n08:56 Understanding RAGAS Metrics\n09:16 Retrieval Metrics\n11:55 RAGAS Context Recall\n14:43 RAGAS Context Precision\n15:52 Generation Metrics\n16:05 RAGAS Faithfulness\n17:16 RAGAS Answer Relevancy\n18:40 Metrics Driven Development\n\n#ai #artificialintelligence #nlp #chatbot #langchain",
        "summary": "In this video, the presenter discusses Metric Driven Agent Development focusing on the use of RAGAS (RAG ASsessment) for evaluating AI agents. The video highlights the integration of RAGAS with LangChain, utilizing Anthropic's Claude 3, Cohere's embedding models, and the Pinecone vector database to assess both retrieval and generation aspects of RAG pipelines. The presenter explains the process of setting up the agent, including the retrieval pipeline and the metrics used to evaluate performance. Key metrics discussed include context recall, context precision, faithfulness, and answer relevancy. The video emphasizes the importance of these metrics in improving agent performance by evaluating the retrieval and generation components. It also offers a walkthrough of the evaluation format and how to interpret results, underscoring the benefit of a metrics-driven approach to agent development. The video is relevant to topics such as AI, NLP, vector databases, and evaluation frameworks for AI models.",
        "categories": [
            "Agents",
            "Vector Databases",
            "Data, Text and Code generation",
            "Summarization",
            "Querying Data",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=-_52DIIOsCE",
        "published_at": "2024-04-04T18:08:54Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Claude 3 Opus RAG Chatbot (Full Walkthrough)",
        "description": "Claude 3 Opus is a state-of-the-art (SOTA) LLM from Anthropic. In this walkthrough, we'll see how to use Claude 3 Opus as a conversational AI agent with LangChain v1, using a Retrieval Augmented Generation (RAG) tool powered by Voyage AI embeddings and the Pinecone vector database.\n\nPutting all of these together, we have an extremely accurate AI RAG conversational agent.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/v1/claude-3-agent.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Claude 3 AI Agent in LangChain\n00:33 Finding Claude 3 RAG Code\n01:35 Using Voyage AI Embeddings\n02:25 Using Pinecone Knowledge Base for RAG\n03:55 Claude 3 AI Agent Setup\n09:19 Using Claude 3 Agent\n10:17 Adding Conversational Memory\n12:32 Testing Claude 3 Agent with Memory\n14:40 Final Thoughts on AI Agents and Anthropic\n\n#ai #claude3 #artificialintelligence #anthropic #nlp #chatbot #langchain",
        "summary": "### Summary: \nIn this video, the presenter demonstrates how to build a fully conversational AI agent using the Claude 3 Opus model from Anthropic, integrated with Voyage AI embeddings and the Pinecone vector database, through LangChain version 0.1.1. The video details the installation of prerequisites, acquisition of API keys from Voyage AI and Pinecone, and setting up the embedding model. The process involves creating an archive search tool using LangChain agents and utilizing the XML agent format with Anthropic models for improved data retrieval and response generation.\n\nThe presenter explains how to initialize the Claude 3 Opus model and integrate it with external memory using a conversational buffer window memory object from LangChain, allowing the agent to recall past interactions. A demonstration is provided where the agent successfully retrieves information about \"Llama 2\" and its red teaming exercises, showcasing the model's ability to maintain context over multiple interactions.\n\nThis walkthrough serves as a guide to setting up a conversational AI agent that leverages Retrieval Augmented Generation (RAG) capabilities, emphasizing the use of LangChain and Anthropic's Claude 3 Opus model. The video is particularly valuable for those interested in AI, LLMs, and conversational agents, as it combines frameworks and tools to achieve an effective knowledge retrieval and conversational system.\n\n## Video Title: \"Building a Conversational AI Agent with Claude 3 Opus\"\n\n## Video Description: \nClaude 3 Opus is a state-of-the-art (SOTA) LLM from Anthropic. In this walkthrough, we'll see how to use Claude 3 Opus as a conversational AI agent with LangChain v1, using a Retrieval Augmented Generation (RAG) tool powered by Voyage AI embeddings and the Pinecone vector database.\n\nPutting all of these together, we have an extremely accurate AI RAG conversational agent.",
        "categories": [
            "In-context learning",
            "Agents",
            "Vector Databases",
            "Prompting",
            "Framework or Library",
            "APIs"
        ],
        "url": "https://www.youtube.com/watch?v=rbzYZLfQbAM",
        "published_at": "2024-03-15T15:30:14Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Multi-Modal NSFW Detection with AI",
        "description": "Using multi-modal models like OpenAI's CLIP we can use the Semantic Router library for detection of specific images or videos, for example the detection of Not Shrek For Work (NSFW) and Shrek For Work (SFW) images. In this video, we'll see how.\n\n\u2b50 GitHub Repo:\nhttps://github.com/aurelio-labs/semantic-router/\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/07-multi-modal.ipynb\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 AI Image Classification\n00:23 How to use Multi-Modal AI\n01:47 Finding Image Detection Notebook\n02:18 Shrek Dataset\n04:55 Creating Multi-Modal Routes\n06:36 Testing NSFW Image Detection\n07:53 Final Notes on Multi-Modal AI\n\n#ai #artificialintelligence #nlp #openai",
        "summary": "In this video, the presenter explores the new vision and image features of the Semantic Router, which now incorporates Vision Transformers and a multimodal model called CLIP. These advancements enable the use of image and multimodal routes, facilitating various applications such as data preprocessing, automatic video splitting based on imagery, and image detection. A specific example discussed is the classification of images as \"Shrek For Work\" (SFW) or \"Not Shrek For Work\" (NSFW) using a dataset split into training and test sets. The process involves setting up routes to detect Shrek images using a multimodal clip encoder and testing the system with both text and image inputs. The integration of such models allows for both text and image classification, showcasing the flexibility of the Semantic Router in handling diverse data types. The presenter also touches on topics like route optimization and hints at future applications of these technologies in more intelligent data processing and video content analysis. The video serves as a demonstration of the Semantic Router's capabilities in using multimodal AI for image classification and routing, inviting viewers to explore further developments and applications. The presenter encourages feedback and the sharing of innovative uses of this technology.",
        "categories": [
            "Multimodal models",
            "Image classification and generation",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=EqKjaLrpeI4",
        "published_at": "2024-03-07T13:31:24Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "AI Decision Making \u2014 Optimizing Routes",
        "description": "AI decision-making can now be easily trained using the optimization methods available in semantic router.\n\nRoute score thresholds define whether a route should be chosen. If the score we identify for any given route is higher than the Route.score_threshold, it passes; otherwise, it does not, and either another route is chosen or we return no route.\n\nGiven that this one score_threshold parameter can define the choice of a route, it's important to get it right \u2014 but it's incredibly inefficient to do so manually. Instead, we can use the fit and evaluate methods of our RouteLayer. All we must do is pass a smaller number of (utterance, target route) examples to our methods, and with the fit, we will often see dramatically improved performance within seconds \u2014 we will see how to measure that performance gain with evaluation.\n\n\u2b50 GitHub Repo:\nhttps://github.com/aurelio-labs/semantic-router/\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/06-threshold-optimization.ipynb\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/",
        "summary": "In this video, the presenter explores the enhancements made to the semantic router library, focusing on improving route accuracy through threshold customization on a per-route basis. Previously, threshold adjustments were applied universally, but now they can be tailored individually, providing greater flexibility and precision. The video demonstrates the use of training methods to optimize route values effectively, emphasizing the efficiency of utilizing a GPU for speed improvements. The presenter walks through a practical example, using a threshold optimization notebook in Google Colab, which involves defining route layers and testing queries to evaluate performance. The video highlights the importance of adding utterances to routes and refining training datasets for improved accuracy. The presenter also discusses the use of different models, like the E5 base V2 model, over the default mini LM model for better performance. The video serves as a demonstration of the optimization function, showing how various models can achieve different accuracy levels and emphasizing the need for continuous iteration and data augmentation for enhanced performance. This content is relevant to AI and LLMs, as it discusses frameworks and tools for optimizing AI decision-making processes, particularly in the context of route selection and model performance evaluation. The presenter provides a practical guide to using semantic router tools and methodologies, making it a useful resource for developers looking to enhance AI applications. The video is part of a broader educational effort, linked to a GitHub repository and a course on semantic routers, indicating a focus on both demonstration and learning in the field of AI frameworks and optimization techniques.",
        "categories": [
            "In-context learning",
            "Framework or Library",
            "Fine tuning",
            "Executing code"
        ],
        "url": "https://www.youtube.com/watch?v=Qi2_r4AopLM",
        "published_at": "2024-02-27T14:00:18Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Steerable AI with Pinecone + Semantic Router",
        "description": "We can make AI steerable and predictable using Semantic Router. How much fine-grained control we need will adjust the scale required by our routes. At very large scales, it can be useful to use a vector database to store and search through your route vector space. In this walkthrough, we will see how to use the new Pinecone integration in Semantic Router.\n\n\u2b50 GitHub Repo:\nhttps://github.com/aurelio-labs/semantic-router/\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/examples/pinecone-and-scaling.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Pinecone and Semantic Router\n01:53 Finding Code for Pinecone\n04:12 Getting Routes from Hugging Face\n07:36 Loading Route Layers from Pinecone\n\n#ai #artificialintelligence #nlp #chatbot",
        "summary": "In this video, the presenter explores the new integration between Semantic Router and Pinecone, emphasizing the benefits of scalability and ease of use. The integration is designed to handle high-scale operations by utilizing Pinecone's vector database capabilities, which allows for storing and searching through extensive route vector spaces. This is particularly useful for applications requiring fine-grained control and large scale, such as creating numerous routes and utterances within Semantic Router.\n\nThe video demonstrates how to implement this integration using a Colab notebook, showcasing the process of initializing route layers with Pinecone and the advantages of persistent indexing. By storing route layers in Pinecone, users can easily load them across different environments, enhancing flexibility and efficiency. The presenter also hints at potential use cases for this integration, including semantic splitting for document and conversation chunking, video frame chunking, and content moderation for images.\n\nOverall, the integration simplifies the process of managing semantic routes at scale, making it more versatile and applicable to various AI applications. The video provides a practical walkthrough of the integration, demonstrating its simplicity and potential for unlocking new AI use cases.",
        "categories": [
            "In-context learning",
            "Vector Databases",
            "Framework or Library",
            "Search",
            "Classification",
            "Data, Text and Code generation"
        ],
        "url": "https://www.youtube.com/watch?v=qjRrMxT20T0",
        "published_at": "2024-02-21T15:00:56Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "OpenAI's Sora: Incredible AI Generated Video",
        "description": "Taking a look at the new text-to-video diffusion model, Sora, from OpenAI \u2014 it is truly incredible.\n\nOpenAI Blog Post:\nhttps://openai.com/sora\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/",
        "summary": "In this video, the presenter explores the groundbreaking capabilities of \"Sora,\" a new text-to-video diffusion model developed by OpenAI. The model can create realistic and imaginative video scenes based on text instructions. The presenter shares their astonishment at the level of detail and realism Sora can achieve, noting how the AI-generated videos maintain high-quality backgrounds and realistic movements, which is a significant advancement over previous AI models.\n\nThe video highlights Sora's strengths, such as its ability to generate photorealistic images and videos that would be difficult to distinguish from real footage. However, it also acknowledges the model's current limitations, including challenges with simulating physics accurately and potential errors in spatial details, such as mixing up left and right or struggling with complex scenes and cause-and-effect scenarios.\n\nSora is still in an early phase, being tested by OpenAI's Red team and a select group of visual artists, designers, and filmmakers to gather feedback for improvement. The presenter reflects on the implications of this technology, expressing concerns about distinguishing real from AI-generated content in the future.\n\nAdditionally, the video touches upon the technical aspects of Sora, mentioning that it uses a diffusion model starting with static noise and removing it over several steps, similar to Video St diffusion. It also employs a Transformer architecture and a recapturing technique from DALL-E 3 for generating detailed captions, which aids in faithfully following user instructions.\n\nOverall, the video conveys excitement about the potential uses of Sora in creative industries, while also pondering the ethical and practical challenges posed by such advanced AI-generated content. The presenter concludes with curiosity about how open source video generation models will evolve in response to OpenAI's developments.",
        "categories": [
            "Multimodal models",
            "Image classification and generation (If multi-modal)",
            "AI Ethics"
        ],
        "url": "https://www.youtube.com/watch?v=F-BuJId6cK4",
        "published_at": "2024-02-15T23:30:11Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "New LangChain XML Agents",
        "description": "In this video, we learn how to use LangChain v1 XML Agents by building a conversational agent using Anthropic's Claude 2.1, Cohere's Embed v3, and Pinecone Serverless!\n\nWe'll be adding conversational memory to our XML agent, and defining the agent itself using LangChain Expression Language (LCEL).\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/v1/xml-agents.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 LangChain v1 XML Agents\n00:36 LangChain Agent Types\n02:13 LangChain Python Setup\n02:43 AI ArXiv 2 Dataset\n04:34 Building Index with Cohere and Pinecone\n08:37 Building a LangChain XML Agent\n15:19 Giving our Agent Conversation Memory\n19:21 XML Agents Conclusion\n\n#langchain #ai #artificialintelligence #nlp #chatbot",
        "summary": "In this video, the presenter guides viewers through the setup and utilization of LangChain v1 XML Agents, showcasing how to build a conversational agent by integrating several AI technologies, including Anthropic's Claude 2.1, Cohere's Embed v3, and Pinecone Serverless. The video highlights the use of XML agents as an alternative agent type suitable for use with Anthropics language models, emphasizing the efficiency and reliability it offers over traditional methods. The presenter also discusses the creation of a retrieval-augmented generation (RAG) pipeline using Pinecone and the embedding capabilities of Cohere, which are described as relatively new models and services. \n\nThe video includes a walkthrough of the LangChain Python setup, the use of the AI ArXiv 2 dataset, and the process of building an index with Cohere and Pinecone. A significant portion of the presentation is dedicated to demonstrating how to give the XML agent conversational memory, allowing it to maintain context over interactions. The video concludes by summarizing the benefits of using these integrated technologies to create a functional and effective conversational agent.\n\nOverall, the video serves as both a tutorial and a demonstration of AI tools and frameworks, with a focus on agents, vector databases, and conversational memory. The presenter provides practical advice and insights into the applications and advantages of using these specific technologies for AI development.",
        "categories": [
            "Agents",
            "Vector Databases",
            "Framework or Library",
            "In-context learning",
            "Search",
            "Data, Text and Code generation",
            "Summarization"
        ],
        "url": "https://www.youtube.com/watch?v=a4R4vdqSdkI",
        "published_at": "2024-02-06T14:00:59Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "OpenAI's NEW 256-d Embeddings vs. Ada 002",
        "description": "In this video, I'm testing OpenAI's new text-embedding-3-large 256-d embeddings against text-embedding-ada-002 \u2014 the results are impressive, to say the least.\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/encoders/openai-embed-3.ipynb\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 OpenAI Embed 3 256 dimensions\n01:05 Code setup\n04:34 Optimizing text-embedding-3-large\n08:45 Embed 3 vs Ada 002\n11:00 Embed 3 with 256-d beats Ada 002\n\n#openai #ai #artificialintelligence #nlp",
        "summary": "In this video, the presenter explores the concept of semantic routing for AI agents using OpenAI's third-generation embedding models. The focus is on testing the efficacy of the new text-embedding-3-large model with a reduced embedding size of 256 dimensions, compared to the default 3072 dimensions. Despite the smaller size, the model reportedly performs better than the previous ada-002 model, which is a significant finding.\n\nThe video demonstrates the use of semantic router libraries, which aim to make decisions based on semantic similarity rather than relying solely on language models (LM) for decision-making. The presenter sets up a protective route to prevent AI agents from discussing certain topics, utilizing the semantic routing capabilities to identify and restrict certain user interactions.\n\nThe setup involves using OpenAI's embedding models in Colab, requiring specific versions of libraries to ensure compatibility. The presenter initializes a route layer with defined encoders and routes, testing it with example questions to verify its functionality. The results show that the 256-dimensional vectors work effectively in routing different questions correctly.\n\nFurther optimization is carried out using threshold optimization techniques, comparing the performance of the new model against the older ada-002 model. The tests reveal that the new model achieves an accuracy of 88.57%, outperforming the ada-002 model. This is particularly impressive given the reduced dimensionality of the embeddings.\n\nThe presenter concludes that despite the smaller embedding size, the new model performs well, suggesting potential for further improvements with optimization. The video serves as a demonstration of the capabilities of OpenAI's new embedding models and their application in semantic routing for AI agents. The presenter shares insights into AI news, demonstrates the tool, and discusses the framework and its implications for AI development.\n\nThe video provides valuable information for those interested in AI technologies, particularly in the context of enhancing AI decision-making processes through semantic similarity and optimization techniques.",
        "categories": [
            "In-context learning",
            "Agents",
            "Prompting",
            "Topic Modelling",
            "Framework or Library",
            "APIs",
            "Infrastructure"
        ],
        "url": "https://www.youtube.com/watch?v=bW931qHLV0M",
        "published_at": "2024-01-31T15:46:32Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "OpenAI's NEW Embedding Models",
        "description": "OpenAI's new embedding models are text-embedding-3-small and text-embedding-3-large. These models are better than Ada 002 (text-embedding-ada-002), and we have the option of latency and storage-optimized text-embedding-3-smallor the higher accuracy text-embedding-3-large.\n\nKey takeaways here are the pretty huge performance gains for multilingual embeddings \u2014 measured by the leap from 31.4% to 54.9% on the MIRACL benchmark. For English-language performance, we look at MTEB and see a smaller but still significant increase from 61% to 64.6%.\n\nIt's worth noting that the max tokens and knowledge cutoff have not changed. That lack of new knowledge represents a minor drawback for use cases performing retrieval in domains requiring up-to-date knowledge.\n\nWe also have a different embedding dimensionality for the new v3 large model, resulting in higher storage costs and paired with higher embedding costs than what we get with Ada 002.\n\nNow, there is some nuance to the dimensionality of these models. By default, these models use the dimensionality noted above. However, it turns out that they still perform even if we cut down those vectors. For v3 small, we can keep just the first 512 dimensions. For v3 large, we can trim the vectors down to a tiny 256-dimensions or a more midsized 1024-dimensions.\n\n\ud83d\udcd5 Article:\nhttps://www.pinecone.io/learn/openai-embeddings-v3/\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/search/semantic-search/openai-embed-v3/openai-embed-v3.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 OpenAI Ada 002\n01:25 New OpenAI Embedding Models\n03:50 OpenAI Embedding Dimension Parameter\n05:04 Using OpenAI Embedding 3\n10:08 Comparing Ada 002 to Embed 3\n\n#openai #ai #artificialintelligence #nlp",
        "summary": "In this video, the presenter discusses the advancements in OpenAI's embedding models, particularly the newly released text-embedding-3-small and text-embedding-3-large models, which show significant improvements over the previous text-embedding-ada-002 model. The video highlights the substantial performance gains in multilingual embeddings, with an increase in the MIRACL benchmark score from 31.4% to 54.9%. There is also a modest improvement in English-language performance, measured by the MTEB benchmark, from 61% to 64.6%.\n\nThe presenter notes that the maximum token limit and the knowledge cutoff date remain unchanged, which could be a limitation for use cases requiring up-to-date information retrieval. The new models also feature different embedding dimensionalities, which impact storage and embedding costs. The video explains that while these models use the noted dimensionalities by default, they can still perform effectively when the vectors are reduced. For example, the v3 small model can be reduced to 512 dimensions, and the v3 large model can be trimmed to as low as 256 dimensions, which is particularly noteworthy.\n\nThe presenter goes through a practical demonstration of using these models in a notebook, comparing their performance in various scenarios, such as indexing and retrieval speed. The video indicates that the large model, despite its higher performance, has slower embedding latency compared to the others. The presenter expresses interest in further testing, especially with the reduced dimensionality to confirm the claimed performance benefits.\n\nOverall, the video serves as an update on OpenAI's new embedding models, providing insights into their applications in natural language processing tasks and their potential impacts on AI technologies.",
        "categories": [
            "In-context learning",
            "Vector Databases",
            "Search",
            "Topic Modelling",
            "Data, Text and Code generation",
            "APIs",
            "Infrastructure"
        ],
        "url": "https://www.youtube.com/watch?v=cUyw5eG-VtM",
        "published_at": "2024-01-26T01:33:55Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Llama.cpp for FULL LOCAL Semantic Router",
        "description": "Using fully local semantic router for agentic AI with llama.cpp LLM and HuggingFace embedding models.\n\nThere are many reasons we might decide to use local LLMs rather than use a third-party service like OpenAI. It could be cost, privacy, compliance, or fear of the OpenAI apocalypse. To help you out, we made Semantic Router fully local with local LLMs available via llama.cpp like Mistral 7B.\n\nUsing llama.cpp also enables the use of quantized GGUF models, reducing the memory footprint of deployed models and allowing even 13-billion parameter models to run with hardware acceleration on an Apple M1 Pro chip. We also use LLM grammars to enable high output reliability even from the smallest of models.\n\nIn this video, we'll use HuggingFace's MiniLM encoder, and llama.cpp's Mistral-7B-instruct GGUF quantized.\n\n\u2b50 GitHub Repo:\nhttps://github.com/aurelio-labs/semantic-router/\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/05-local-execution.ipynb\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/",
        "summary": "The video focuses on demonstrating the use of a fully local Semantic Router for agentic AI, leveraging llama.cpp LLM and HuggingFace embedding models. The presenter discusses the advantages of using local LLMs over third-party services like OpenAI, citing reasons such as cost, privacy, compliance, and independence from external services.\n\nKey highlights include the use of llama.cpp to enable quantized GGUF models, which reduce memory usage and allow even large models to run efficiently on devices like the Apple M1 Pro chip. The video showcases how LLM grammars contribute to high output reliability, enabling small models like Mistral 7B to perform effectively. The presenter compares this setup favorably against GPT 3.5, noting better performance in certain tasks.\n\nThe video also details the process of setting up and running the Semantic Router on a local machine, specifically an M1 MacBook Pro. It begins with downloading and installing the Semantic Router library, followed by initializing dynamic routes using the Mistral 7B model and HuggingFace's MiniLM encoder. The presenter illustrates how to configure static and dynamic routes and how to utilize these configurations to perform tasks like fetching the current time in various locations.\n\nA significant part of the demonstration is the use of LLM grammars to enforce structured outputs, which enhances the performance of small models. The video concludes by emphasizing the flexibility of the Semantic Router in supporting various embedding models from HuggingFace and the ability to run these models locally, opening up more options for developers.\n\nOverall, the video is a practical demonstration of a tool or technology relevant to multimodal models, agentic AI, and large language models, providing insights into local model execution and optimization techniques.",
        "categories": [
            "In-context learning",
            "Agents",
            "Infrastructure",
            "Framework or Library",
            "Model security and privacy"
        ],
        "url": "https://www.youtube.com/watch?v=NGCtBFjzndc",
        "published_at": "2024-01-19T15:00:14Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "FIRST Look at Pinecone Serverless!",
        "description": "Here we take a first look at the new Pinecone serverless, a complete rebuild of the Pinecone vector database optimized for real-world requirements, focusing on cost, latency, and recall performance.\n\nHere, we take a look at serverless and how to use it for RAG, semantic search, or other AI applications via the Pinecone Python client.\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/docs/semantic-search.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/",
        "summary": "Summary: In this video, the presenter introduces the Pinecone serverless, a newly redesigned version of the Pinecone vector database. This version focuses on offering more flexibility, scalability, and significant cost reductions for users. The presenter outlines the cost benefits, explaining that the serverless model allows for separate payments for storage and querying, making it more economical compared to the previous pod-based system. With serverless, users can expect to pay based on usage rather than fixed pod costs, resulting in substantial savings, especially for users with lower query volumes. The video also demonstrates how to use the Pinecone serverless via the Python client, including setting up an index, uploading vectors, and performing semantic searches. The presenter highlights that the serverless model is not only cost-effective but also simplifies the process by removing the need for environment variables and offering a more streamlined setup. Although there is no free tier yet, the presenter mentions a $100 credit for trying the new serverless system. This video is particularly useful for AI practitioners interested in vector databases, semantic search, and optimizing costs in AI applications. The presenter shares practical insights on transitioning to Pinecone serverless and anticipates future enhancements, such as support for more cloud services like GCP and Azure. Overall, this video provides a comprehensive overview of Pinecone serverless, its benefits, and practical usage tips for AI applications.",
        "categories": [
            "Vector Databases",
            "Querying Data",
            "Search",
            "Framework or Library"
        ],
        "url": "https://www.youtube.com/watch?v=9k7JTOY3Zjg",
        "published_at": "2024-01-16T21:33:01Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Faster LLM Function Calling \u2014 Dynamic Routes",
        "description": "LLM function calling can be slow, particularly for AI agents. Using Semantic Router's dynamic routes, we can make this much faster and scale to thousands of tools and functions. Here we see how to use it with OpenAI's GPT-3.5 Turbo, but the library also supports Cohere and Llama.cpp for local deployments.\n\nIn semantic router there are two types of routes that can be chosen. Both routes belong to the Route object, the only difference between them is that static routes return a Route.name when chosen, whereas dynamic routes use an LLM call to produce parameter input values.\n\nFor example, a static route will tell us if a query is talking about mathematics by returning the route name (which could be \"math\" for example). A dynamic route can generate additional values, so it may decide a query is talking about maths, but it can also generate Python code that we can later execute to answer the user's query, this output may look like \"math\", \"import math; output = math.sqrt(64).\n\n\u2b50 GitHub Repo:\nhttps://github.com/aurelio-labs/semantic-router/\n\n\ud83d\udccc Code:\nhttps://github.com/aurelio-labs/semantic-router/blob/main/docs/02-dynamic-routes.ipynb\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Consulting:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Fast LLM Function Calling\n00:56 Semantic Router Setup for LLMs\n02:20 Function Calling Schema\n04:04 Dynamic Routes for Function Calling\n05:51 How we can use Faster Agents",
        "summary": "In this video, the presenter demonstrates how to use dynamic routes in the Semantic Router library to improve function calling for AI agents. The video provides a detailed explanation of the differences between static and dynamic routes, highlighting the latter's ability to dynamically generate parameters for function calls. The presenter uses OpenAI's GPT-3.5 Turbo for the demonstration, with support for Cohere and Llama.cpp mentioned. The video includes a step-by-step setup of dynamic routes, focusing on the creation and importance of a function schema. This tutorial aims to enhance the efficiency and scalability of AI workflows using the Semantic Router library.",
        "categories": [
            "Framework or Library",
            "Executing code",
            "APIs",
            "Agents"
        ],
        "url": "https://www.youtube.com/watch?v=QsZm0XCysoQ",
        "published_at": "2024-01-15T13:00:18Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "NEW AI Framework - Steerable Chatbots with Semantic Router",
        "description": "Semantic Router is a superfast decision layer for your LLMs and agents that integrates with LangChain, improves RAG, and supports OpenAI and Cohere.\n\n\u2b50 Repo: https://www.github.com/aurelio-labs/semantic-router\n\nRather than waiting for slow LLM generations to make tool-use decisions, we use the magic of semantic vector space to make those decisions \u2014 routing our requests using semantic meaning. This approach unlocks incredibly fast agentic decision-making, the ability to use *literally* millions of tools, and provides much greater steerability and AI safety using semantics.\n\n\ud83d\udccc Code:\nIntro: https://github.com/aurelio-labs/semantic-router/blob/main/docs/00-introduction.ipynb\nLangChain Agents: https://github.com/aurelio-labs/semantic-router/blob/main/docs/03-basic-langchain-agent.ipynb\n\n\ud83d\udd25 Semantic Router Course:\nhttps://www.aurelio.ai/course/semantic-router\n\n\ud83d\udc4b\ud83c\udffc AI Dev:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 New Python Library for Better AI\n01:57 Using Semantic Router\n02:26 Semantic Router in Python\n03:08 Defining Guardrails and Routes for LLMs\n04:34 Initializing a RouteLayer\n07:39 Using the Router with LangChain Agents\n11:47 What else can Semantic Router do\n12:40 Final Notes on the Library",
        "summary": "In this video, the presenter introduces a new Python library called the Semantic Router, which is designed to improve the decision-making process for AI and large language models (LLMs) by integrating with LangChain and supporting OpenAI and Cohere. The Semantic Router acts as a fast decision-making layer for LLMs, enabling rapid tool-use decisions using semantic vector space rather than waiting for slow LLM generations. This approach allows for the use of millions of tools and provides greater steerability and AI safety.\n\nThe video details how the Semantic Router can be used to define guardrails and routes for LLMs, allowing for the creation of specific response triggers based on semantic meaning. By defining routes for different topics, such as politics or general chitchat, the Semantic Router can control the dialogue with AI and prevent certain discussions. The presenter demonstrates how to initialize a RouteLayer, which contains different routes and handles the decision-making process.\n\nMoreover, the Semantic Router can be integrated with LangChain agents to enhance their capabilities. It can also be used to protect against specific queries, perform function calls without slow agent processing time, and improve retrieval-augmented generation (RAG) methods by providing a fast and powerful alternative. The presenter emphasizes the benefits of using the Semantic Router, such as improved control over AI behaviors and enhanced productivity in production environments.\n\nThe video concludes with a discussion of future enhancements, including Dynamic routing and the hybrid layer, which will combine semantic space with traditional vector space for specialized domains like medicine and finance. The presenter encourages viewers to explore the open-source library, contribute, and stay tuned for further updates on additional features.\n\n## Video Description\nSemantic Router is a superfast decision layer for your LLMs and agents that integrates with LangChain, improves RAG, and supports OpenAI and Cohere.\n\n## Topics: \n- In-context learning\n- Agents\n- Vector Databases\n- Prompting\n- Framework or Library\n- APIs\n- Infrastructure",
        "categories": [
            "In-context learning",
            "Agents",
            "Vector Databases",
            "Prompting",
            "Framework or Library",
            "APIs",
            "Infrastructure"
        ],
        "url": "https://www.youtube.com/watch?v=ro312jDqAh0",
        "published_at": "2024-01-02T13:00:10Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "Mixtral 8X7B \u2014 Deploying an *Open* AI Agent",
        "description": "Mistral AI's new model \u2014 Mixtral 8x7B \u2014 is pretty impressive. We'll see how to get set up and deploy Mixtral 8X7B, the prompt format it requires, and how it performs when being used as an Agent \u2014 we even add in some Mixtral RAG at the end.\n\nAs a bit of a spoiler, Mixtral is probably the first open-source LLM that is truly very very good \u2014 I say this considering the following key points:\n\n- Benchmarks show it to perform better than GPT-3.5.\n- My own testing shows Mixtral to be the first open weights model we can reliably use as an agent.\n- Due to MoE architecture it is very fast given its size. If you can afford to run on 2x A100s and latency is good enough to be used in chatbot use cases.\n\n\ud83d\udcd5 Mixtral 8X7B Page:\nhttps://www.pinecone.io/learn/mixtral-8x7b\n\n\ud83d\udccc Code Notebook:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/mistral-ai/mixtral-8x7b/00-mixtral-8x7b-agent.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Dev:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Mixtral 8X7B is better than GPT 3.5\n00:50 Deploying Mixtral 8x7B\n03:21 Mixtral Code Setup\n08:17 Using Mixtral Instructions\n10:04 Mixtral Special Tokens\n13:29 Parsing Multiple Agent Tools\n14:28 RAG with Mixtral\n17:01 Final Thoughts on Mixtral\n\n#artificialintelligence #nlp #ai #chatbot #opensource",
        "summary": "In this video, the presenter discusses the performance of the new Mixtral 8x7B PR model from Mistral AI, highlighting its advantages over other open-source language models like GPT-3.5. The video provides a comprehensive guide on deploying Mixtral 8x7B using RunPod, a platform for running AI models, and details the set-up process including hardware requirements and cost considerations. The presenter emphasizes the model's speed and usability, particularly when deployed on two A100 GPUs, making it suitable for chatbot applications.\n\nThe video covers the technical setup involving Hugging Face Transformers, Accelerate, and the initialization of the tokenizer and text generation pipeline for Mixtral. A significant focus is placed on using Mixtral in an agent-like flow, where it can utilize different tools, such as a web search tool, to enhance its functionality.\n\nThe presenter also explores the unique instruction format and special tokens required by Mixtral to optimize its performance. They demonstrate how to prevent repetitive outputs by adjusting parameters, and how to structure inputs effectively for better results.\n\nTowards the end, the video compares Mixtral\u2019s performance with GPT-3.5, noting that Mixtral is more reliable as it does not require extensive output parsing or handling of malformed JSON. The presenter expresses optimism about Mixtral's capabilities, especially given its Mixture of Experts (MoE) architecture which contributes to its high performance and speed. The video concludes with plans to explore Mixtral further and its potential applications in AI development.\n\n- Summary: The video demonstrates the deployment and utilization of Mixtral 8x7B, an open-source AI model, emphasizing its superior performance and speed compared to GPT-3.5. It provides insights into setup, operational efficiency, and practical use cases, highlighting Mixtral's potential in AI applications.",
        "categories": [
            "In-context learning",
            "Agents",
            "Prompting",
            "Data, Text and Code generation",
            "Executing code",
            "Infrastructure",
            "APIs"
        ],
        "url": "https://www.youtube.com/watch?v=aCRvIPpFyEI",
        "published_at": "2023-12-15T11:00:01Z"
    },
    {
        "channel": "James Briggs",
        "channelIcon": "/assets/icons/JamesBriggs.jpg",
        "title": "LangChain Expression Language (LCEL) Explained!",
        "description": "The LangChain Expression Language (LCEL) is an abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.\n\nLCEL comes with strong support for:\n1. Superfast development of chains.\n2. Advanced features such as streaming, async, parallel execution, and more.\n3. Easy integration with LangSmith and LangServe.\n\nTo understand LCEL we'll dive into its syntax, how it works under the hood, and a few examples of how it is used alongside RunnableParallel, RunnablePassthrough, and RunnableLambda objects.\n\nArticle:\nhttps://www.pinecone.io/learn/series/langchain/langchain-expression-language/\n\n\ud83d\udccc Code:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/langchain/handbook/11-langchain-expression-language.ipynb\n\n\ud83c\udf32 Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\n\ud83d\udc4b\ud83c\udffc AI Development:\nhttps://aurelio.ai\n\n\ud83d\udc7e Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 LangChain Expression Language LCEL\n01:06 Getting Started with LCEL\n06:11 How LCEL Pipe Operator Works\n12:21 Using LangChain Expression Language\n14:16 LCEL Runnables\n19:36 LCEL Runnable Lambdas\n23:36 Pros and Cons of LCEL\n\n#ai #artificialintelligence #langchain #nlp #anthropic",
        "summary": "### Summary:\n\nThe video discusses the LangChain Expression Language (LCEL), a new abstraction in the LangChain ecosystem that simplifies the process of building chains of LangChain components. This language allows for minimalist code, offering a streamlined way to implement complex functionalities like parallel execution, asynchronous processing, and streaming. The presenter demonstrates how LCEL replaces traditional methods with a pipe operator for chaining components, which, while less pythonic, offers flexibility and simplicity.\n\nThe video explains the mechanics of the pipe operator, showing how it passes outputs from one component to the next, effectively creating a sequence of operations. The presenter provides examples using LCEL's RunnableParallel and RunnableLambda objects to illustrate how parallel processes can be executed efficiently without extensive coding.\n\nKey features of LCEL include superfast development of chains, advanced features that work out-of-the-box, and seamless integration with other LangChain products like LangSmith and LangServe. The video also touches on the pros and cons of LCEL, noting its abstraction and non-standard syntax as potential downsides.\n\nThe presenter emphasizes that while LCEL might be challenging for those unfamiliar with its syntax, it offers significant advantages in prototyping and potentially in production environments. The discussion concludes by acknowledging the pros of minimalist style and advanced feature support against the cons of added abstraction and syntax unfamiliarity, ultimately recommending experimentation and learning.\n\n**Video Title:** LangChain Expression Language LCEL\n\n**Video Description:** The LangChain Expression Language (LCEL) is an abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components. LCEL comes with strong support for superfast development of chains, advanced features such as streaming, async, parallel execution, and more. It also offers easy integration with LangSmith and LangServe. The video dives into LCEL's syntax, its workings, and examples of usage alongside RunnableParallel, RunnablePassthrough, and RunnableLambda objects. The presenter provides insights into the pros and cons of using this new abstraction for AI and LLMs.",
        "categories": [
            "Framework or Library",
            "Executing code",
            "Infrastructure",
            "APIs"
        ],
        "url": "https://www.youtube.com/watch?v=O0dUOtOIrfs",
        "published_at": "2023-12-07T11:19:20Z"
    }
]