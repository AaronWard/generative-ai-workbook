{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM, Ollama and Autogen\n",
    "\n",
    "**LiteLLM** provides a simplified interface to interact with language models, offering synchronous and asynchronous text generation capabilities, and supporting streaming outputs for efficient generation of longer texts.\n",
    "\n",
    "**Ollama** lets you run, manage, and interact with a variety of LLMs locally, allowing for enhanced control, customization, and application of language models without the necessity for constant internet connectivity once models are downloaded and stored locally.\n",
    "\n",
    "Being able to leverage all these together, will help reduce cost as you work with autogen, as you don't need to make API calls to a provider. \n",
    "Also a nice bonus, no API keys.\n",
    "\n",
    "---\n",
    "Links:\n",
    "- [Ollama](https://ollama.ai/)\n",
    "- [LiteLLM](https://litellm.ai/)\n",
    "- [Autogen opensource Capability - PR](https://github.com/microsoft/autogen/pull/95)\n",
    "\n",
    "Here all the different models Ollama supports, so you can use this and play around will all different model types:\n",
    "- [Link](https://ollama.ai/library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in the terminal:\n",
    ">  litellm --model ollama/llama2 --api_base http://localhost:11434/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \" Of course! Here's a fun story about llamas for you:\\n\\nOnce upon a time, in a small village nestled in the Andes mountains, there lived a group of friendly llamas. These llamas were known far and wide for their kind hearts and playful antics.\\n\\nOne sunny day, a young llama named Luna decided to explore the nearby forest. She wandered deeper and deeper into the woods until she came across a clearing filled with the most beautiful wildflowers. Luna was so mesmerized by their colors that she didn't notice the tall trees closing in around her.\\n\\nSuddenly, she heard a rustling in the bushes. She turned to see a mischievous fox peeking out from behind a fern. The fox had a sly grin on his face and Luna knew he was up to something.\\n\\n\\\"Hello there, little llama,\\\" said the fox in a smooth voice. \\\"What are you doing so far from home?\\\"\\n\\nLuna hesitated for a moment before answering. She didn't want to be rude, but she also didn't trust this strange animal. \\\"I'm just exploring,\\\" she replied cautiously.\\n\\nThe fox chuckled and said, \\\"Well, you've come to the right place. I know all the best spots in these woods. Would you like to see them?\\\"\\n\\nLuna's curiosity got the better of her, so she agreed to follow the fox on an adventure through the forest. They trotted through the trees, the fox leading the way and Luna following close behind.\\n\\nAs they walked, the fox told Luna stories about the different plants and animals that lived in the forest. He even showed her a hidden waterfall that cascaded into a clear pool. Luna was amazed by all the wonders the forest held.\\n\\nBut as the sun began to set, Luna realized she had to return home. She thanked the fox for his help and started back down the mountain. As she reached the village, she saw her friends waiting for her, curious about her adventure.\\n\\nLuna told them all about her day with the mischievous fox, and they listened with wide eyes. From that day on, Luna knew that there was a whole world of magic and mystery hidden in the Andes mountains, just waiting to be explored.\\n\\nAnd so, every time she ventured into the forest, Luna would think back on her adventure with the fox and know that anything was possible if you were brave enough to seek it out. The end!\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"logprobs\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"id\": \"chatcmpl-3a039946-80aa-43cc-bd55-2e4cca92eb55\",\n",
      "  \"created\": 1697153865.8232698,\n",
      "  \"model\": \"ollama/llama2\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 14,\n",
      "    \"completion_tokens\": 488,\n",
      "    \"total_tokens\": 502\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/llama2\", \n",
    "    messages=[{ \"content\": \"Tell me a story about llamas.\", \"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"chat.completion\",\n",
      "  \"choices\": [\n",
      "    {\n",
      "      \"finish_reason\": \"stop\",\n",
      "      \"index\": 0,\n",
      "      \"message\": {\n",
      "        \"content\": \"\\nOnce upon a time, in the high Andes Mountains of South America, there lived a group of llamas. Llamas are known for their gentle and friendly nature, and these animals were no exception. They spent their days grazing on the lush grasses that grew in the mountain meadows, taking leisurely naps in the warm sun, and playing with one another.\\n\\nOne day, a group of hikers came through the area. The hikers were amazed by the beauty of the landscape, and they were eager to learn more about the animals that called this place home. They approached the llamas, hoping to get a closer look. To their surprise, the llamas did not run away, but instead came closer to investigate the newcomers.\\n\\nThe hikers were delighted by the friendly demeanor of the llamas, and they spent hours talking with them and learning about their way of life. They discovered that llamas live in herds, led by a dominant male known as a \\\"mama llama\\\". The mama llama is responsible for protecting the members of the herd and making sure they have enough food and water.\\n\\nAs the sun began to set, the hikers reluctantly said their goodbyes to the llamas and continued on their journey. They left with a newfound appreciation for these gentle animals and the incredible beauty of the Andes Mountains. From that day on, whenever they saw a llama, they would smile and remember their wonderful encounter in the high Andes.\",\n",
      "        \"role\": \"assistant\",\n",
      "        \"logprobs\": null\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"id\": \"chatcmpl-c028551f-59a5-4bc0-b4a2-413ce3b6fd02\",\n",
      "  \"created\": 1697153828.435919,\n",
      "  \"model\": \"ollama/mistral\",\n",
      "  \"usage\": {\n",
      "    \"prompt_tokens\": 14,\n",
      "    \"completion_tokens\": 297,\n",
      "    \"total_tokens\": 311\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/mistral\", \n",
    "    messages=[{ \"content\": \"Tell me a story about llamas.\", \"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "clear_cache()\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        # \"model\": \"ollama/mistral\",\n",
    "        \"model\": \"ollama/llama2\",\n",
    "        \"api_base\": \"http://0.0.0.0:8000\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "import pprint as pp\n",
    "pp.pprint(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.8,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key.\")\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Multiple Models\n",
    "\n",
    "> litellm --model ollama/llama2 --api_base http://localhost:11434/ \\\n",
    "> litellm --model codellama:7b-code --api_base http://localhost:11434/ --port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting cache...\n",
      "[{'api_base': 'http://0.0.0.0:8000',\n",
      "  'api_type': 'litellm',\n",
      "  'model': 'ollama/mistral'}]\n",
      "[{'api_base': 'http://0.0.0.0:8001',\n",
      "  'api_type': 'litellm',\n",
      "  'model': 'ollama/codellama'}]\n",
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key. Write python of SH code to achieve the problem\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m coding_assistant \u001b[39m=\u001b[39m AssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_assistant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     llm_config\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m coding_runner \u001b[39m=\u001b[39m UserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_runner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNEVER\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m coding_runner\u001b[39m.\u001b[39;49minitiate_chat(coding_assistant, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCalculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt use any approach that requires an API key. Write python of SH code to achieve the problem\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# clear_cache(clear_previous_work=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:779\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 779\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    781\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:790\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    788\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 790\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    791\u001b[0m         context,\n\u001b[1;32m    792\u001b[0m         use_cache,\n\u001b[1;32m    793\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    794\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    797\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:821\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    820\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 821\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:216\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    214\u001b[0m api_type \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m api_type \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-zA-Z0-9]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, api_type)\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlitellm\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 216\u001b[0m     response \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39;49mcompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:700\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m    699\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    701\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/timeout.py:53\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     local_timeout_duration \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mlocal_timeout_duration)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m futures\u001b[39m.\u001b[39mTimeoutError:\n\u001b[1;32m     55\u001b[0m     thread\u001b[39m.\u001b[39mstop_loop()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "clear_cache()\n",
    "\n",
    "proxy_config_list = [\n",
    "    {\n",
    "        \"model\": \"ollama/mistral\",\n",
    "        # \"model\": \"ollama/llama2\",\n",
    "        \"api_base\": \"http://0.0.0.0:8000\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "coding_config_list = [\n",
    "    {\n",
    "        # \"model\": \"ollama/mistral\",\n",
    "        \"model\": \"ollama/codellama\",\n",
    "        \"api_base\": \"http://0.0.0.0:8001\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "import pprint as pp\n",
    "pp.pprint(proxy_config_list)\n",
    "pp.pprint(coding_config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": coding_config_list,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": proxy_config_list,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key. Write python of SH code to achieve the problem\")\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
