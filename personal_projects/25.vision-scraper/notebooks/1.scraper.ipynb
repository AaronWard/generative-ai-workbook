{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper\n",
    "\n",
    "The purpose of this notebook is to build a webscraper for r/LocalLlama using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Example: scroll 10 times\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m     \u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Wait for more posts to load\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_datetime\u001b[39m(timestamp):\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m datetime\u001b[38;5;241m.\u001b[39mstrptime(timestamp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.\u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m+0000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up the Selenium driver (example with Chrome)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run in the background\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the subreddit's new posts\n",
    "url = 'https://www.reddit.com/r/LocalLLaMA/hot/'\n",
    "\n",
    "driver.get(url)\n",
    "sleep(2)  # Wait for the initial page to load\n",
    "\n",
    "# Scroll down to ensure all posts from the last day are loaded\n",
    "# Adjust the range or conditions based on your needs\n",
    "for _ in range(10):  # Example: scroll 10 times\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(2)  # Wait for more posts to load\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    return datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%f+0000\")\n",
    "\n",
    "# Now that we have the page loaded, let's use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "print(soup.title.text)\n",
    "posts = soup.find_all('article')\n",
    "print(f\"Found {len(posts)} posts on the page.\")\n",
    "\n",
    "# If posts are found, attempt to print the first post's HTML\n",
    "if posts:\n",
    "    print(posts[0].prettify())\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "now = datetime.now()\n",
    "\n",
    "# Dictionary to hold post details\n",
    "posts_details = {}\n",
    "\n",
    "# Loop through each post article\n",
    "for article in soup.find_all('article'):\n",
    "    # Extract title\n",
    "    title_tag = article.find('a', {'slot': 'title'})\n",
    "    if title_tag:\n",
    "        title = title_tag.text.strip()\n",
    "        link = title_tag['href']\n",
    "        \n",
    "        # Extract timestamp\n",
    "        timestamp_tag = article.find('faceplate-timeago')\n",
    "        if timestamp_tag:\n",
    "            post_datetime = parse_datetime(timestamp_tag['ts'])\n",
    "            # Check if the post is from the last day\n",
    "            if now - post_datetime <= timedelta(days=1):\n",
    "                # Extract content if available\n",
    "                content = article.find('div', {'data-post-click-location': 'text-body'})\n",
    "                content_text = content.text.strip() if content else \"No content\"\n",
    "                \n",
    "                # Store details in dictionary\n",
    "                posts_details[title] = {'link': link, 'content': content_text}\n",
    "\n",
    "# Print the details of posts from the last day\n",
    "for title, details in posts_details.items():\n",
    "    print(f\"Title: {title}\\nLink: {details['link']}\\nContent: {details['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: High-VRAM GPUS for us nerds.\n",
      "Link: /r/LocalLLaMA/comments/1asfe83/highvram_gpus_for_us_nerds/\n",
      "Content: There are currently no (reasonably priced) graphics cards with a lot of VRAM (>= 64GB) to run large models.      My expectation is, at some point, some manufacturer will make those happen. But I'm wondering if we (as a community) can make it happen sooner.      VRAM is not that expensive (https://www.tomshardware.com/news/gddr6-vram-prices-plummet), so something like a 1060 with 64 or 128GB of RAM shouldn't be too expensive. Unless there is some technical reason this can't be done cheaply (or at all) that I'm missing, please enlighten my naive ass.      Personally, if I'm going to put 900 euros into a graphics card, I'd rather it has fewer CUDA cores than a 3090 but more RAM than a 3090. Not sure about others here.      Here are some solutions I can imagine:  1. Harass large manufacturers.    If we all collectively email (or social-media-spam) large manufacturers of GPUs / graphics card, we might get them to understand there is a significant demand for these cards, and push them to release a product.  2. Get a smaller manufacturer to do a Kickstarter.    Maybe we could find a smaller manufacturer of graphics card to understand this demand exists, and motivate them to get into that niche.      They'd potentially do a Kickstarter for the board, so there wouldn't be too much of an upfront cost for them. And we as a community would be able to help/put our money where our mouth is.  3. Get an Open-Source project started.    Maybe we could find somebody who has already done some kind of graphics-card / advanced board as an open-source project, and motivate them to design this board for us. Maybe we can support them through some kind of donation thing as they do the work, and/or they can do a Kickstarter to finance the design and the early production.      Maybe that person is on this sub, maybe that person is you?      An option here for the Open-Source project, would be to use old/outdated GPUs/VRAM that is being sold at a discount, which would enable for a cheaper board (with lesser token-per-second, but still allowing us to run models we normally wouldn't be able to run).      Any other ideas of how to get this off the ground? (a number 4 in this list?)      Any recommendations of whom to contact for each of the 3 categories?      Any reason why this is a terrible idea?      Would you be interested in such a board?      Thanks a lot in advance. Cheers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the subreddit\n",
    "url = 'https://www.reddit.com/r/LocalLLaMA/hot/'\n",
    "\n",
    "# Add headers to mimic a real browser visit. Reddit checks for User-Agent.\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    now = datetime.now()\n",
    "    posts_details = {}\n",
    "\n",
    "    # Loop through each post article\n",
    "    for article in soup.find_all('article'):\n",
    "        # Extract title\n",
    "        title_tag = article.find('a', {'slot': 'title'})\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            link = title_tag['href']\n",
    "\n",
    "            content = article.find('div', {'data-post-click-location': 'text-body'})\n",
    "            content_text = content.text.strip() if content else \"No content\"\n",
    "            \n",
    "            # Store details in dictionary\n",
    "            posts_details[title] = {'link': link, 'content': content_text.rstrip().replace(\"\\n\", \"\")}\n",
    "\n",
    "    # Print the details of posts from the last day\n",
    "    for title, details in posts_details.items():\n",
    "        print(f\"Title: {title}\\nLink: {details['link']}\\nContent: {details['content']}\\n\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['High-VRAM GPUS for us nerds.'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_details.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
