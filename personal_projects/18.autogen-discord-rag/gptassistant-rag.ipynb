{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import autogen\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent\n",
    "from autogen.retrieve_utils  import query_vector_db\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATING VECTOR STORE\n",
    "docs_path = Path(os.getcwd(), 'docs')\n",
    "client = chromadb.PersistentClient(path=f\"{os.getcwd()}/chromadb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATIONS\n",
    "\n",
    "config_list = autogen.config_list_from_dotenv(\n",
    "    dotenv_file_path='../../.env',\n",
    "    model_api_key_map={\n",
    "        \"gpt-4-1106-preview\": \"OPENAI_API_KEY\",\n",
    "    },\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4-1106-preview\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "query_vector_db_tool_config = {\n",
    "    \"name\": \"query_vector_db\",\n",
    "    \"description\": \"Function to query the Chroma vector database.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query_texts\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
    "            \"n_results\": {\"type\": \"integer\"}\n",
    "        },\n",
    "        \"required\": [\"query_texts\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"assistant_id\": None,\n",
    "    \"tools\": [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": query_vector_db_tool_config\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gpt-4-1106-preview\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_ASSISTANT_SYSTEM_MESSAGE = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
    "context provided by the user. You should follow the following steps to answer a question:\n",
    "\n",
    "Step 1: you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
    "a question answering task.\n",
    "Step 2: you reply based on the intent.\n",
    "\n",
    "Use the provided tool `query_vector_db` to get the context from a Chroma vectorstore.\n",
    "If user's intent is code generation you must follow the formats below to write your code:\n",
    "```language\n",
    "# your code\n",
    "```\n",
    "\n",
    "If user's intent is question answering, you must give as short an answer as possible.\n",
    "Use the provided tool `query_vector_db` to get the context.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTANTIATE AGENTS\n",
    "\n",
    "sql_assistant = GPTAssistantAgent(\n",
    "    name=\"Chroma_SQL_Assistant\",\n",
    "    instructions=RAG_ASSISTANT_SYSTEM_MESSAGE,\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "sql_assistant.register_function(\n",
    "    function_map={\n",
    "        \"query_vector_db\": lambda query_texts, n_results=10: query_vector_db(client, query_texts, n_results)\n",
    "    }\n",
    ")\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda msg: \"TERMINATE\" in msg[\"content\"],\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"./autogen_results\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3\n",
    ")\n",
    "\n",
    "initial_message = \"How can you use Chainlit with Autogen\"\n",
    "user_proxy.initiate_chat(sql_assistant, message=initial_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
