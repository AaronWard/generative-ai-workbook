{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastChat\n",
    "\n",
    "FastChat is an open-source platform designed to facilitate the training, serving, and evaluation of large language models (LLMs) such as chatbots. It provides functionalities like state-of-the-art training and evaluation methodologies, a distributed multi-model serving system, OpenAI-compatible RESTful APIs, and a web UI to interact with the models.\n",
    "\n",
    "### Core Features\n",
    "- Training and Evaluation: Offers the ability to train models with state-of-the-art methodologies and to evaluate them using sets of challenging multi-turn open-ended questions. It automates the evaluation process by leveraging strong LLMs to act as judges and assess the quality of the models' responses.\n",
    "- Multi-Model Serving System: Provides a distributed system to serve multiple models simultaneously, allowing users to interact with different LLMs effectively.\n",
    "- Web UI and APIs: Incorporates a web UI for user interaction and OpenAI-compatible RESTful APIs, making it a local drop-in replacement for OpenAI APIs.\n",
    "\n",
    "### Integration with AutoGen\n",
    "AutoGen is a tool that can leverage FastChat to initiate endpoints and perform inference on various models locally. It can be used to interact with multiple LLMs on a local machine and provides a seamless interface to perform tasks like text completion and chat completion using different models, making it suitable for LLM applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download Model Checkpoints and FastChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Download Checkpoints: Model checkpoints, such as those of ChatGLM-6B, need to be downloaded and properly set up.\n",
    "git clone https://huggingface.co/THUDM/chatglm2-6b\n",
    "\n",
    "\n",
    "# Clone FastChat: FastChat needs to be cloned and properly configured to function as a local drop-in replacement for OpenAI APIs.\n",
    "git clone https://github.com/lm-sys/FastChat.git\n",
    "cd FastChat\n",
    "pip3 install --upgrade pip  # enable PEP 660 support\n",
    "pip3 install -e \".[model_worker,webui]\"\n",
    "\n",
    "    # OR\n",
    "\n",
    "pip install fschat[model_worker,webui]\n",
    "\n",
    "\n",
    "# If you're on Mac\n",
    "brew install rust cmake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initiate Server\n",
    "Ensure that the servers are properly configured and that any encountered errors are resolved.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m fastchat.serve.controller\n",
    "python -m fastchat.serve.model_worker --model-path chatglm2-6b\n",
    "python -m fastchat.serve.openai_api_server --host localhost --port 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Interact with Model using AutoGen: \n",
    "Once the servers are up and running, models can be directly accessed through the `openai-python` library as well as `autogen.oai.Completion` and `autogen.oai.ChatCompletion`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import oai\n",
    "\n",
    "# create a text completion request\n",
    "response = oai.Completion.create(\n",
    "    config_list=[{\n",
    "        \"model\": \"chatglm2-6b\",\n",
    "        \"api_base\": \"http://localhost:8000/v1\",\n",
    "        \"api_type\": \"open_ai\",\n",
    "        \"api_key\": \"NULL\", # just a placeholder\n",
    "    }],\n",
    "    prompt=\"Hi\",\n",
    ")\n",
    "print(response)\n",
    "\n",
    "\n",
    "# create a chat completion request\n",
    "response = oai.ChatCompletion.create(\n",
    "    config_list=[{\"model\": \"chatglm2-6b\", \"api_base\": \"http://localhost:8000/v1\", \"api_type\": \"open_ai\", \"api_key\": \"NULL\"}],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Interacting with Multiple Local LLMs\n",
    "AutoGen can be configured to interact with multiple LLMs simultaneously, and inference code can be written to specify which models to interact with.\n",
    "To interact with multiple models, launch the multi-model worker specifying each model path and then interact using Autogen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m fastchat.serve.multi_model_worker --model-path lmsys/vicuna-7b-v1.3 --model-names vicuna-7b-v1.3 --model-path chatglm2-6b --model-names chatglm2-6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oai.ChatCompletion.create(\n",
    "    config_list=[\n",
    "        {\n",
    "            \"model\": \"chatglm2-6b\",\n",
    "            \"api_base\": \"http://localhost:8000/v1\",\n",
    "            \"api_type\": \"open_ai\",\n",
    "            \"api_key\": \"NULL\",\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"vicuna-7b-v1.3\",\n",
    "            \"api_base\": \"http://localhost:8000/v1\",\n",
    "            \"api_type\": \"open_ai\",\n",
    "            \"api_key\": \"NULL\",\n",
    "        }\n",
    "    ],\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hi\"}]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "FastChat serves as a comprehensive platform for deploying and interacting with various language models, and Autogen leverages FastChat's capabilities to perform local LLM applications. By integrating Autogen with FastChat, users can seamlessly interact with models like ChatGLMv2-6b, conduct inferences, and manage multiple local LLMs effectively, all the while utilizing the OpenAI-compatible API provided by FastChat."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
