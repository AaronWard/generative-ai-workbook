{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamafile x Autogen\n",
    "\n",
    "This notebook aims to show how you can run mixtral 8x7B with Autogen.\n",
    "\n",
    "**Step to run locally:**\n",
    "- Download a llamafile of the [HuggingFace Repo](https://huggingface.co/award40/mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile/tree/main) - no installations needed, read the model card.  \n",
    "- `chmod +x mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile`\n",
    "- `./mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile --port 8081`\n",
    "<!-- ./llamafile -m mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf -->\n",
    "\n",
    "Like many of you, i am GPU poor. The goal behind this approach was to have easy access to a good opensourced model with limited GPU resources, like a Macbook Pro M1 32GB.\n",
    "It's not the full model, but it's the most feasible given the resource constraints - see [here](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#provided-files) for notes on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import autogen \n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "\n",
    "work_dir = \"./coding\"\n",
    "cache_dir = \"./.cache\"\n",
    "\n",
    "if os.path.exists(cache_dir):\n",
    "    print('Deleting previous work...')\n",
    "    shutil.rmtree(cache_dir)\n",
    "    shutil.rmtree(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_list': [{'base_url': 'http://127.0.0.1:8081/v1', 'api_key': 'NULL'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API Configurations\n",
    "config_list=[\n",
    "    {\n",
    "        \"base_url\": \"http://127.0.0.1:8081/v1\",\n",
    "        \"api_key\": \"NULL\", # just a placeholder\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config_mistral={\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "llm_config_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for the two agents\n",
    "system_messages = {\n",
    "    \"USER_PROXY_SYSTEM_MESSAGE\": (\n",
    "        \"\"\"\n",
    "        Your job is to act as an natural language AI interface between the human use and the coding assistant.\"\n",
    "        Your job is NOT to take part in conversations, but to act as an intermediary for the human and the coding assistant.\n",
    "        Say TERMINATE when all tasks are finished.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    \"CODING_ASSISTANT_SYSTEM_MESSAGE\" : (\n",
    "        \"\"\"\n",
    "        As a expert programmer, your role involves generating answer a question. \n",
    "        \\n\\n\n",
    "        When a task is presented, you should:\n",
    "        Step 1: Determine the user's intent by considering the question and its context. The intent might involve generating code or answering a question, or both.\n",
    "        Step 2: Formulate a response based on the identified intent.\n",
    "        Step 3: If your code doesn't work, use the subsequent messages to refactor and fix the issues.\n",
    "        This approach will provide a well-informed basis for your responses.\n",
    "         \\n\\n\n",
    "\n",
    "        REMEMBER:\n",
    "        - You MUST provide full working code for the user to run.\n",
    "        - You MUST write code for in your answer while adhering to this format:\n",
    "        ```language\n",
    "        <code>\n",
    "        ```\n",
    "        - You MUST present the code in a single code block, do NOT split up into sections.\n",
    "        - Do NOT engage in trivial conversations, just do your job!\n",
    "        \\n\\n\n",
    "        Say TERMINATE when all tasks are finished.\n",
    "        \"\"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config=llm_config_mistral,\n",
    "    system_message=system_messages['CODING_ASSISTANT_SYSTEM_MESSAGE']\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    llm_config=llm_config_mistral,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": work_dir, \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    system_message=system_messages['USER_PROXY_SYSTEM_MESSAGE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate a chat with a simple python problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent an infitinate gratitude loop\n",
    "def get_additional_termination_notice():\n",
    "    \"\"\"Extra instructions for terminating when goal is finished.\"\"\"\n",
    "    \n",
    "    termination_notice = (\n",
    "        '\\n\\nDo not show appreciation in your responses, say only what is necessary. '\n",
    "        'if \"Thank you\" or \"You\\'re welcome\" are said in the conversation, then say TERMINATE '\n",
    "        'to indicate the conversation is finished'\n",
    "        # \"Say TERMINATE when no further instructions are given to indicate the task is complete\"\n",
    "    )\n",
    "    return termination_notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Write a fibonacci sequence function in python and run it to print 20 numbers.\"\n",
    "# user_message = user_message + f\"{get_additional_termination_notice()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_runner.initiate_chat(coding_assistant, \n",
    "                            message=user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
