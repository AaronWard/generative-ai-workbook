{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Splitters\n",
    "\n",
    "Text splitters are used to divide long pieces of text into smaller, semantically meaningful chunks. This allows for better handling and processing of the text, while maintaining the context between the chunks. Here's an overview of text splitters and their customization options. Here is the text to be used in this example notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Large Language Models (LLMs) are a class of deep learning models designed to process and understand vast amounts of natural language data. \n",
    "They are built on neural network architectures, particularly the transformer architecture, which allows them to capture complex language \n",
    "patterns and relationships between words or phrases in large-scale text datasets. \n",
    "\n",
    "As a matter of fact, LLM can also be understood as variants of transformer.  The transformer architecture relies on a mechanism called self-attention,\n",
    "which allows the model to weigh the importance of different words or phrases in a given context. This has proven to be particularly effective in capturing \n",
    "long-range dependencies and understanding the nuances of natural language. \n",
    "\n",
    "Recall that the transformer architecture represents the neural network model for natural language processing tasks based on encoder-decoder architecture, \n",
    "which was introduced in the paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al. in 2017. The key component of the transformer architecture is \n",
    "the self-attention mechanism, which enables the model to attend to different parts of the input sequence to compute a representation for each position. \n",
    "The transformer consists of two main components: the encoder network and the decoder network. \n",
    "The encoder network takes an input sequence and produces a sequence of hidden states, while the decoder network takes a target sequence and uses the encoder‚Äôs \n",
    "output to generate a sequence of predictions. Both the encoder and decoder are composed of multiple layers of self-attention and feedforward neural networks. \n",
    "The picture given below represents the original transformer architecture.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Character Text Splitter splits text into chunks based on a specified number of characters. It is a simple and straightforward method to divide text based on character count. Use the Character Text Splitter when you need to split text into fixed-size chunks based on the number of characters. This approach can be useful when working with models or systems that have specific character-based limitations or requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 361, which is longer than the specified 100\n",
      "Created a chunk of size 382, which is longer than the specified 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Large Language Models (LLMs) are a class of deep learning models designed to process and understand vast amounts of natural language data. \\nThey are built on neural network architectures, particularly the transformer architecture, which allows them to capture complex language \\npatterns and relationships between words or phrases in large-scale text datasets.',\n",
       " 'As a matter of fact, LLM can also be understood as variants of transformer.  The transformer architecture relies on a mechanism called self-attention,\\nwhich allows the model to weigh the importance of different words or phrases in a given context. This has proven to be particularly effective in capturing \\nlong-range dependencies and understanding the nuances of natural language.',\n",
       " 'Recall that the transformer architecture represents the neural network model for natural language processing tasks based on encoder-decoder architecture, \\nwhich was introduced in the paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al. in 2017. The key component of the transformer architecture is \\nthe self-attention mechanism, which enables the model to attend to different parts of the input sequence to compute a representation for each position. \\nThe transformer consists of two main components: the encoder network and the decoder network. \\nThe encoder network takes an input sequence and produces a sequence of hidden states, while the decoder network takes a target sequence and uses the encoder‚Äôs \\noutput to generate a sequence of predictions. Both the encoder and decoder are composed of multiple layers of self-attention and feedforward neural networks. \\nThe picture given below represents the original transformer architecture.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=100, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(text)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[359, 381, 938]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(chunk) for chunk in texts]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PythonCodeTextSplitter is specifically designed to handle code snippets or programming languages. It considers code-specific patterns and structures to split the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['# Proprietary LLM from e.g. OpenAI\\n# pip install openai\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(model_name=\"text-davinci-003\")\\n\\n# Alternatively, open-source LLM hosted on Hugging Face\\n# pip install huggingface_hub\\nfrom langchain import HuggingFaceHub\\nllm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\")\\n\\n# The LLM takes a prompt as an input and outputs a completion\\nprompt = \"Alice has a parrot. What animal is Alice\\'s pet?\"\\ncompletion = llm(prompt)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import PythonCodeTextSplitter\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "# Proprietary LLM from e.g. OpenAI\n",
    "# pip install openai\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Alternatively, open-source LLM hosted on Hugging Face\n",
    "# pip install huggingface_hub\n",
    "from langchain import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\")\n",
    "\n",
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "prompt = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "completion = llm(prompt)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "text_splitter = PythonCodeTextSplitter()\n",
    "texts = text_splitter.split_text(PYTHON_CODE)\n",
    "\n",
    "texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK (Natural Language Toolkit) is a popular library for natural language processing tasks. NLTK provides various tools and utilities, including tokenizers and text splitting functionalities. Use the NLTK Text Splitter when you require more advanced text processing and analysis, such as sentence or word-level tokenization. It is suitable for tasks that involve natural language understanding, language modeling, or sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/award40/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Large Language Models (LLMs) are a class of deep learning models designed to process and understand vast amounts of natural language data.\\n\\nThey are built on neural network architectures, particularly the transformer architecture, which allows them to capture complex language \\npatterns and relationships between words or phrases in large-scale text datasets.\\n\\nAs a matter of fact, LLM can also be understood as variants of transformer.\\n\\nThe transformer architecture relies on a mechanism called self-attention,\\nwhich allows the model to weigh the importance of different words or phrases in a given context.\\n\\nThis has proven to be particularly effective in capturing \\nlong-range dependencies and understanding the nuances of natural language.\\n\\nRecall that the transformer architecture represents the neural network model for natural language processing tasks based on encoder-decoder architecture, \\nwhich was introduced in the paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al.\\n\\nin 2017.\\n\\nThe key component of the transformer architecture is \\nthe self-attention mechanism, which enables the model to attend to different parts of the input sequence to compute a representation for each position.\\n\\nThe transformer consists of two main components: the encoder network and the decoder network.\\n\\nThe encoder network takes an input sequence and produces a sequence of hidden states, while the decoder network takes a target sequence and uses the encoder‚Äôs \\noutput to generate a sequence of predictions.\\n\\nBoth the encoder and decoder are composed of multiple layers of self-attention and feedforward neural networks.\\n\\nThe picture given below represents the original transformer architecture.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_splitter = NLTKTextSplitter()\n",
    "texts = text_splitter.split_text(text)\n",
    "texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Recursive Character Text Splitter is a more sophisticated text splitter that recursively splits text based on a list of characters. It attempts to keep semantically related pieces of text together, such as paragraphs, sentences, and words. Use the Recursive Character Text Splitter when you want to split text while maintaining the integrity of paragraphs, sentences, or words. This approach can be useful for tasks that involve text summarization, topic modeling, or any scenario where preserving the semantic structure of the text is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Large Language Models (LLMs) are a class of deep learning models designed to process and understand',\n",
       " 'and understand vast amounts of natural language data.',\n",
       " 'They are built on neural network architectures, particularly the transformer architecture, which',\n",
       " 'architecture, which allows them to capture complex language',\n",
       " 'patterns and relationships between words or phrases in large-scale text datasets.',\n",
       " 'As a matter of fact, LLM can also be understood as variants of transformer.  The transformer',\n",
       " 'The transformer architecture relies on a mechanism called self-attention,',\n",
       " 'which allows the model to weigh the importance of different words or phrases in a given context.',\n",
       " 'in a given context. This has proven to be particularly effective in capturing',\n",
       " 'long-range dependencies and understanding the nuances of natural language.',\n",
       " 'Recall that the transformer architecture represents the neural network model for natural language',\n",
       " 'natural language processing tasks based on encoder-decoder architecture,',\n",
       " 'which was introduced in the paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al. in 2017. The key',\n",
       " 'in 2017. The key component of the transformer architecture is',\n",
       " 'the self-attention mechanism, which enables the model to attend to different parts of the input',\n",
       " 'parts of the input sequence to compute a representation for each position.',\n",
       " 'The transformer consists of two main components: the encoder network and the decoder network.',\n",
       " 'The encoder network takes an input sequence and produces a sequence of hidden states, while the',\n",
       " 'states, while the decoder network takes a target sequence and uses the encoder‚Äôs',\n",
       " 'output to generate a sequence of predictions. Both the encoder and decoder are composed of multiple',\n",
       " 'of multiple layers of self-attention and feedforward neural networks.',\n",
       " 'The picture given below represents the original transformer architecture.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "texts = text_splitter.split_text(text)\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='# Proprietary LLM from e.g. OpenAI\\n# pip install openai\\nfrom langchain.llms import OpenAI\\nllm = OpenAI(model_name=\"text-davinci-003\")', metadata={}),\n",
       " Document(page_content='# Alternatively, open-source LLM hosted on Hugging Face\\n# pip install huggingface_hub\\nfrom langchain import HuggingFaceHub\\nllm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\")', metadata={}),\n",
       " Document(page_content='# The LLM takes a prompt as an input and outputs a completion\\nprompt = \"Alice has a parrot. What animal is Alice\\'s pet?\"\\ncompletion = llm(prompt)', metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, Language\n",
    "\n",
    "\n",
    "PYTHON_CODE = \"\"\"\n",
    "# Proprietary LLM from e.g. OpenAI\n",
    "# pip install openai\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Alternatively, open-source LLM hosted on Hugging Face\n",
    "# pip install huggingface_hub\n",
    "from langchain import HuggingFaceHub\n",
    "llm = HuggingFaceHub(repo_id = \"google/flan-t5-xl\")\n",
    "\n",
    "# The LLM takes a prompt as an input and outputs a completion\n",
    "prompt = \"Alice has a parrot. What animal is Alice's pet?\"\n",
    "completion = llm(prompt)\n",
    "\"\"\"\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.PYTHON, chunk_size=200, chunk_overlap=0\n",
    ")\n",
    "python_docs = python_splitter.create_documents([PYTHON_CODE])\n",
    "python_docs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is a popular open-source library for advanced natural language processing. The spaCy Text Splitter utilizes the spaCy tokenizer to split text into chunks based on a specified chunk size.\n",
    "\n",
    "üî¥ Keep getting an error when trying to load in the spacy module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# text_splitter = SpacyTextSplitter(nlp, chunk_size=1000)\n",
    "# texts = text_splitter.split_text(text)\n",
    "# texts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face is a popular platform for natural language processing, providing a wide range of pre-trained models and tokenizers. The Hugging Face tokenizer, such as GPT2TokenizerFast, allows you to tokenize text and measure the chunk size in terms of tokens. Use the Hugging Face Tokenizer when you want to work with pre-trained models or utilize specific tokenization features provided by the Hugging Face library. It is particularly useful when dealing with transformer-based models like GPT-2 or BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 67, which is longer than the specified 60\n",
      "Created a chunk of size 74, which is longer than the specified 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Large Language Models (LLMs) are a class of deep learning models designed to process and understand vast amounts of natural language data. \\nThey are built on neural network architectures, particularly the transformer architecture, which allows them to capture complex language \\npatterns and relationships between words or phrases in large-scale text datasets.',\n",
       " 'As a matter of fact, LLM can also be understood as variants of transformer.  The transformer architecture relies on a mechanism called self-attention,\\nwhich allows the model to weigh the importance of different words or phrases in a given context. This has proven to be particularly effective in capturing \\nlong-range dependencies and understanding the nuances of natural language.',\n",
       " 'Recall that the transformer architecture represents the neural network model for natural language processing tasks based on encoder-decoder architecture, \\nwhich was introduced in the paper ‚ÄúAttention Is All You Need‚Äù by Vaswani et al. in 2017. The key component of the transformer architecture is \\nthe self-attention mechanism, which enables the model to attend to different parts of the input sequence to compute a representation for each position. \\nThe transformer consists of two main components: the encoder network and the decoder network. \\nThe encoder network takes an input sequence and produces a sequence of hidden states, while the decoder network takes a target sequence and uses the encoder‚Äôs \\noutput to generate a sequence of predictions. Both the encoder and decoder are composed of multiple layers of self-attention and feedforward neural networks. \\nThe picture given below represents the original transformer architecture.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "text_splitter = CharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=60, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(text)\n",
    "texts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
