{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guidance Basics\n",
    "\n",
    "To be honest, Guidance is a cool project but right now it's not looking promising on maintainability. See [here](https://github.com/guidance-ai/guidance/issues/382). Not sure whats going on with this project. They released a new version on today (14th November 2023) which drastically [updated](https://github.com/guidance-ai/guidance/discussions/429) how the package works, rendering all the example notebooks as useless until they update them. \n",
    "\n",
    "- [GitHub Repo](https://github.com/guidance-ai/guidance/tree/main)\n",
    "- [Docs](https://guidance.readthedocs.io/en/latest/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import guidance\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>My favorite flavor is {flavor}</pre>"
      ],
      "text/plain": [
       "<guidance.models._openai.OpenAICompletion at 0x172b4fe80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"\"\"My favorite flavor is {flavor}\"\"\"\n",
    "\n",
    "# this relies on the environment variable OPENAI_API_KEY being set\n",
    "llm = guidance.models.OpenAI(model=\"gpt-4-1106-preview\", api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "llm + prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a cat expert.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>What are the smallest cats?</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>The</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> smallest</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> cat</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> breed</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> is</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> the</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'> Sing</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>apur</span><span style='background-color: rgba(0, 165, 0, 0.15); border-radius: 3px;' title='0.0'>a</span></div></div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'guidance.models._openai.OpenAIChat'>\n"
     ]
    }
   ],
   "source": [
    "llm = guidance.models.OpenAI(model=\"gpt-3.5-turbo\", api_key=os.environ.get('OPENAI_API_KEY'))\n",
    "\n",
    "with guidance.system():\n",
    "    openai_chat = llm + \"You are a cat expert.\"\n",
    "\n",
    "with guidance.user():\n",
    "    openai_chat += \"What are the smallest cats?\"\n",
    "\n",
    "with guidance.assistant():\n",
    "    openai_chat += guidance.gen(\"answer\", stop=\".\")\n",
    "\n",
    "openai_chat\n",
    "print(type(openai_chat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'>I like to play with my </pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (_start_generator_stream):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_remote.py\", line 99, in _start_generator_stream\n",
      "    for chunk in generator:\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_openai.py\", line 207, in _generator\n",
      "    raise e\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_openai.py\", line 197, in _generator\n",
      "    generator = self.client.chat.completions.create(\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 299, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 594, in create\n",
      "    return self._post(\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/_base_client.py\", line 1055, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/_base_client.py\", line 834, in request\n",
      "    return self._request(\n",
      "  File \"/Users/award40/anaconda3/envs/masterclass/lib/python3.10/site-packages/openai/_base_client.py\", line 877, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.BadRequestError: Error code: 400 - {'error': {'message': \"[] is too short - 'messages'\", 'type': 'invalid_request_error', 'param': None, 'code': None}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/github/generative-ai-workbook/learning/guidance/1.basics.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/learning/guidance/1.basics.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m llm \u001b[39m=\u001b[39m guidance\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mOpenAI(model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/learning/guidance/1.basics.ipynb#X36sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m openai_chat \u001b[39m=\u001b[39m llm \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mI like to play with my \u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m guidance\u001b[39m.\u001b[39;49mgen(stop\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m in\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m guidance\u001b[39m.\u001b[39mgen(stop\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_model.py:204\u001b[0m, in \u001b[0;36mModel.__add__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m# run stateless functions (grammar nodes)\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, StatelessFunction):\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m lm\u001b[39m.\u001b[39;49mrun_stateless(value)\n\u001b[1;32m    206\u001b[0m \u001b[39m# run stateful functions\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[39mreturn\u001b[39;00m value(lm)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_model.py:303\u001b[0m, in \u001b[0;36mModel.run_stateless\u001b[0;34m(lm, stateless_function, max_tokens, temperature, top_p, n)\u001b[0m\n\u001b[1;32m    301\u001b[0m delayed_bytes \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m \u001b[39m# last_is_generated = False\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m \u001b[39mfor\u001b[39;00m new_bytes, is_generated, new_bytes_log_prob, capture_groups, capture_group_log_probs, new_token_count \u001b[39min\u001b[39;00m gen_obj:\n\u001b[1;32m    304\u001b[0m     \u001b[39m# convert the bytes to a string (delaying if we don't yet have a valid unicode string)\u001b[39;00m\n\u001b[1;32m    305\u001b[0m     lm\u001b[39m.\u001b[39m_token_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_token_count\n\u001b[1;32m    306\u001b[0m     new_bytes \u001b[39m=\u001b[39m delayed_bytes \u001b[39m+\u001b[39m new_bytes\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_local.py:235\u001b[0m, in \u001b[0;36mLocal.__call__\u001b[0;34m(self, grammar, max_tokens, n, top_p, temperature, ensure_bos_token, log_probs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         new_bytes_log_prob \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m    233\u001b[0m \u001b[39m# otherwise we need to compute the logits and sample a valid token\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_logits(token_ids, parser\u001b[39m.\u001b[39;49mbytes[start_pos:forced_pos])\n\u001b[1;32m    237\u001b[0m     \u001b[39m# if requested we compute the log probabilities so we can track the probabilities of each node\u001b[39;00m\n\u001b[1;32m    238\u001b[0m     \u001b[39m# TODO: we should lower this step to C++ with pybind11\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[39mif\u001b[39;00m log_probs:\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/guidance/models/_remote.py:184\u001b[0m, in \u001b[0;36mRemote._get_logits\u001b[0;34m(self, token_ids, forced_bytes)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_new_stream(prompt)\n\u001b[1;32m    182\u001b[0m     \u001b[39m# we wait for the running stream to put something in the queue\u001b[39;00m\n\u001b[1;32m    183\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 184\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shared_state[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shared_state[\u001b[39m\"\u001b[39;49m\u001b[39mdata_queue\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    186\u001b[0m \u001b[39m# # if we don't have the next byte of data yet then we wait for it (from the streaming thread)\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m# if len(self._shared_state[\"data\"]) == len(prompt):\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[39m#     self._shared_state[\"data\"] += self._shared_state[\"data\"]_queue.get() \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    191\u001b[0m \n\u001b[1;32m    192\u001b[0m \u001b[39m# set the logits to the next byte the model picked\u001b[39;00m\n\u001b[1;32m    193\u001b[0m logits \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokens)) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mnan\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnot_empty\u001b[39m.\u001b[39;49mwait()\n\u001b[1;32m    172\u001b[0m \u001b[39melif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be a non-negative number\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = guidance.models.OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "openai_chat = llm + 'I like to play with my ' + guidance.gen(stop=' ') + ' in' + guidance.gen(stop=['\\n', '.', '!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm + '19, 18,' + guidance.gen(max_tokens=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists of options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List\n",
    "people = ['John', 'Mary', 'Bob', 'Alice']\n",
    "\n",
    "# Dictionaries\n",
    "ideas = [\n",
    "    {\n",
    "        'name': 'truth', \n",
    "        'description': 'the state of being the case'\n",
    "    },\n",
    "    {\n",
    "        'name': 'love', \n",
    "        'description': 'a strong feeling of affection'\n",
    "    }\n",
    "]\n",
    "\n",
    "# For lists\n",
    "program1 = guidance('''List of people:\n",
    "{{#each people}}- {{this}}\n",
    "{{~! This is a comment. The ~ removes adjacent whitespace either before or after a tag, depending on where you place it}}\n",
    "{{/each~}}''')\n",
    "\n",
    "program1(people=people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For list of dictionaries\n",
    "program2 = guidance('''List of ideas:\n",
    "{{#each ideas}}{{this.name}}: {{this.description}}\n",
    "{{/each}}''')\n",
    "\n",
    "program2(ideas=ideas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joining prompts together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that {{>prog_name}} is the same include syntax as in Handlebars\n",
    "program3 = guidance('''{{>program1}}\n",
    "List of ideas:\n",
    "{{#each ideas}}{{this.name}}: {{this.description}}\n",
    "{{/each}}''')\n",
    "\n",
    "program3(program1=program1, people=people, ideas=ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use the {{gen}} command to generate text from the language model\n",
    "# note that we used a ~ at the start of the command tag to remove the whitespace before it (just like in Handlebars)\n",
    "program = guidance('''The best thing about the beach is {{~gen 'best' temperature=0.7 max_tokens=7}}''', llm=llm)\n",
    "program(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the {{#select}} command allows you to use the LLM to select from a set of options\n",
    "program = guidance('''Is the following sentence offensive? Please answer with a single word, either \"Yes\", \"No\", or \"Maybe\".\n",
    "Sentence: {{example}}\n",
    "Answer:{{#select \"answer\" logprobs='logprobs'}} Yes{{or}} No{{or}} Maybe{{/select}}''')\n",
    "executed_program = program(example='I hate tacos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade guidance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
