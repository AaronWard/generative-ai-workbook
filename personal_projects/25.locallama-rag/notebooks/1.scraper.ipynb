{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraper\n",
    "\n",
    "The purpose of this notebook is to build a webscraper for r/LocalLlama using beautiful soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just using BeautiflSoup\n",
    "\n",
    "NOTE: because of the elements being dynamically named by reddit, it's extremely difficult to have consistent extraction of html elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the URL of the subreddit\n",
    "url = 'https://www.reddit.com/r/LocalLLaMA/hot/'\n",
    "\n",
    "# Add headers to mimic a real browser visit. Reddit checks for User-Agent.\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    now = datetime.now()\n",
    "    posts_details = {}\n",
    "\n",
    "    # Loop through each post article\n",
    "    for article in soup.find_all('article'):\n",
    "        # Extract title\n",
    "        title_tag = article.find('a', {'slot': 'title'})\n",
    "        if title_tag:\n",
    "            title = title_tag.text.strip()\n",
    "            link = title_tag['href']\n",
    "\n",
    "            content = article.find('div', {'data-post-click-location': 'text-body'})\n",
    "            content_text = content.text.strip() if content else \"No content\"\n",
    "            \n",
    "            # Store details in dictionary\n",
    "            posts_details[title] = {'link': link, 'content': content_text.rstrip().replace(\"\\n\", \"\")}\n",
    "\n",
    "    # Print the details of posts from the last day\n",
    "    for title, details in posts_details.items():\n",
    "        print(f\"Title: {title}\\nLink: {details['link']}\\nContent: {details['content']}\\n\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage: Status code {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just using BeautiflSoup and Selenium\n",
    "\n",
    "NOTE: this got me blocked because it came across as bot activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the Selenium driver (example with Chrome)\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--headless')  # Run in the background\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# URL of the subreddit's new posts\n",
    "url = 'https://www.reddit.com/r/LocalLLaMA/hot/'\n",
    "\n",
    "driver.get(url)\n",
    "sleep(2)  # Wait for the initial page to load\n",
    "\n",
    "# Scroll down to ensure all posts from the last day are loaded\n",
    "# Adjust the range or conditions based on your needs\n",
    "for _ in range(10):  # Example: scroll 10 times\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    sleep(2)  # Wait for more posts to load\n",
    "\n",
    "def parse_datetime(timestamp):\n",
    "    return datetime.strptime(timestamp, \"%Y-%m-%dT%H:%M:%S.%f+0000\")\n",
    "\n",
    "# Now that we have the page loaded, let's use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "print(soup.title.text)\n",
    "posts = soup.find_all('article')\n",
    "print(f\"Found {len(posts)} posts on the page.\")\n",
    "\n",
    "# If posts are found, attempt to print the first post's HTML\n",
    "if posts:\n",
    "    print(posts[0].prettify())\n",
    "\n",
    "# Close the Selenium browser\n",
    "driver.quit()\n",
    "now = datetime.now()\n",
    "\n",
    "# Dictionary to hold post details\n",
    "posts_details = {}\n",
    "\n",
    "# Loop through each post article\n",
    "for article in soup.find_all('article'):\n",
    "    # Extract title\n",
    "    title_tag = article.find('a', {'slot': 'title'})\n",
    "    if title_tag:\n",
    "        title = title_tag.text.strip()\n",
    "        link = title_tag['href']\n",
    "        \n",
    "        # Extract timestamp\n",
    "        timestamp_tag = article.find('faceplate-timeago')\n",
    "        if timestamp_tag:\n",
    "            post_datetime = parse_datetime(timestamp_tag['ts'])\n",
    "            # Check if the post is from the last day\n",
    "            if now - post_datetime <= timedelta(days=1):\n",
    "                # Extract content if available\n",
    "                content = article.find('div', {'data-post-click-location': 'text-body'})\n",
    "                content_text = content.text.strip() if content else \"No content\"\n",
    "                \n",
    "                # Store details in dictionary\n",
    "                posts_details[title] = {'link': link, 'content': content_text}\n",
    "\n",
    "# Print the details of posts from the last day\n",
    "for title, details in posts_details.items():\n",
    "    print(f\"Title: {title}\\nLink: {details['link']}\\nContent: {details['content']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PRAW For Reddit\n",
    "\n",
    "1. Go to https://www.reddit.com/prefs/apps\n",
    "2. Click \"create app\" or \"create another app\"\n",
    "3. Fill out the form:\n",
    "    - name: Give your app a name.\n",
    "    - application type: Select \"script\".\n",
    "    - redirect uri: Use http://localhost:8080 or a similar placeholder.\n",
    "4. After creation, note down the `client_id` (just under the app name) and `client_secret`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(\"../../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    user_agent=f\"Comment Extraction (by u/{os.environ.get('REDDIT_USERNAME')})\",\n",
    "    client_id= os.environ.get(\"REDDIT_APP\"),\n",
    "    client_secret= os.environ.get(\"REDDIT_SECRET\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit(\"LocalLLaMA\")\n",
    "new_posts = subreddit.new(limit=1000)  # Adjust limit as needed\n",
    "\n",
    "posts_data = {}\n",
    "\n",
    "def get_comments_with_replies(comment, depth=0):\n",
    "    \"\"\"Recursively get comments and their nested replies.\"\"\"\n",
    "    comments_list = []\n",
    "    spacer = \"  \" * depth  # Indentation for nested comments\n",
    "    comments_list.append(f\"{spacer}- {comment.body}\")\n",
    "    if not hasattr(comment, \"replies\"):\n",
    "        comment.replies = []\n",
    "    for reply in comment.replies:\n",
    "        if isinstance(reply, praw.models.MoreComments):\n",
    "            continue\n",
    "        comments_list.extend(get_comments_with_replies(reply, depth + 1))\n",
    "    return comments_list\n",
    "\n",
    "\n",
    "for post in new_posts:\n",
    "    post.comments.replace_more(limit=0)  # Load all comments; limit=0 to fully expand the comment tree\n",
    "    comments = []\n",
    "    for top_level_comment in post.comments:\n",
    "        comments.extend(get_comments_with_replies(top_level_comment))\n",
    "\n",
    "    posts_data[post.id] = {\n",
    "        \"title\": post.title,\n",
    "        \"content\": post.selftext,\n",
    "        \"comments\": comments,\n",
    "        \"link\": post.shortlink,\n",
    "        \"flair\": post.flair, \n",
    "        \"media\": post.media,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"./_output/new/localllama-new-{datetime.now().strftime('%d-%m-%Y')}.txt\", \"w\") as file:\n",
    "    # Print the scraped data\n",
    "    for post_id, post_info in posts_data.items():\n",
    "        file.writelines(f\"Post ID: {post_id}\\n\")\n",
    "        file.writelines(f\"Title: {post_info['title']}\\n\")\n",
    "        file.writelines(f\"Link: {post_info['link']}\\n\")\n",
    "        file.writelines(f\"Content: {post_info['content']}\\n\")\n",
    "        if len(post_info['comments']) > 0:\n",
    "            file.writelines(\"Comments:\\n\")\n",
    "            for comment in post_info['comments']:\n",
    "                file.writelines(comment + \"\\n\")  # Comments are already formatted with spacers for nesting\n",
    "        file.writelines(\"\\n---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
