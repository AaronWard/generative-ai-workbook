{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Discord JSON Data\n",
    "\n",
    "This notebook is for preprocessing the extracted messages from the Autogen Discord. The purpose is to format and filter the data before putting it into a format that can be stored within a vector store for RAG operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from utils import api_utils\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Discord message JSON files\n",
    "path = '../data/chat_logs/*.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for \n",
    "def process_file(file_path):\n",
    "    file_name = os.path.basename(file_path).split('.')[0]\n",
    "    with open(file_path, 'r') as file:\n",
    "        try:\n",
    "            messages = json.load(file)\n",
    "            return pd.DataFrame([{\n",
    "                'channel': file_name,\n",
    "                'author_username': item['author']['username'],\n",
    "                'timestamp': item['timestamp'],\n",
    "                'content': item['content'],\n",
    "                'embeds': item['embeds']\n",
    "            } for item in messages])\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Error decoding JSON from {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Get number of tokens per string\n",
    "def get_token_len(text):\n",
    "    return len([word for word in text.split(' ')])\n",
    "\n",
    "try:\n",
    "    df = pd.concat((process_file(fp) for fp in glob.glob(path) if process_file(fp) is not None), ignore_index=True)\n",
    "    df['timestamp'] =  pd.to_datetime(df['timestamp'],  errors='coerce').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df['num_tokens'] = df.content.apply(get_token_len)\n",
    "    df['has_embedding'] = df.embeds.apply(lambda x: False if x == [] else True)\n",
    "    # Remove redundant short messages by token lengths \n",
    "    df = df[(df['has_embedding']) | (~df['has_embedding'] & (df['num_tokens'] >= 5))]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.channel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['channel', 'timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write_cols = ['channel', 'author_username', 'timestamp', 'content', 'embeds', has_embedding]\n",
    "output_file = \"../data/docs/22112023_chat_history.txt\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    for index, row in df.iterrows():\n",
    "\n",
    "        additional_context = \"\"\n",
    "        try:\n",
    "            if row.has_embedding:\n",
    "                additional_context (f\"\"\"Additional information about the content linked by this user: \n",
    "                - Link title: {row.embeds[0].title}\n",
    "                - Link description: {row.embeds[0].description}\n",
    "                \"\"\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        formatted_text = (f\"\"\"In {row.channel}, at {row.timestamp} a user named {row.author_username} said ```{row.content}```.\\n {additional_context}\"\"\").strip()\n",
    "        file.write(formatted_text.strip().rstrip() + '\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = '''\n",
    "In issues-and-help, at 2023-11-15 10:41:54 a user named Lega said ```im novice w programming so i may not have explained it in the best way. But I basically am trying to figure out the same thing as this member in the server https://discord.com/channels/1153072414184452236/1153072414184452241/1174253851285667911```.\n",
    "In issues-and-help, at 2023-11-14 19:49:12 a user named razahin said ```Hi @.beibinli, thank you very much for your offer of assistance. I have attached the two files here. I've also included an example of the output.\n",
    "\n",
    "The design.jpg files is located under a folder called `coding` which is located at the same level as main.py. \n",
    "\n",
    "I've also tried `user_proxy.initiate_chat(designAnalyzer, message=\"\"\"\n",
    "Load the image from the <img ./coding/design.jpg> file location for processing by an analyzer\"\"\")` in case of there being a location issue but recieved the same results.```.\n",
    "In issues-and-help, at 2023-11-14 19:01:34 a user named sonichi said ```https://microsoft.github.io/autogen/docs/Installation#python```.\n",
    "In issues-and-help, at 2023-11-14 18:11:25 a user named ariel.andres said ```Hi, I am trying to run the following example code on my computer:\n",
    "\n",
    "https://github.com/microsoft/autogen/blob/main/notebook/agentchat_auto_feedback_from_code_execution.ipynb\n",
    "\n",
    "If I try to use Autogen 0.1.14 with Openai 0.28.1 (by just using a pip install pyautogen), I get the following error:\n",
    "\n",
    "raise self.handle_error_response(\n",
    "openai.error.InvalidRequestError: Invalid URL (POST /v1/openai/deployments/InnovationGPT4-32/chat/completions)\n",
    "\n",
    "If I instead do a pip install autogen==0.2.0b5 (which also installs Openai 1.2.4), it runs perfectly fine.\n",
    "\n",
    "This is the structure for my OAI_CONFIG_LIST.json:\n",
    "\n",
    "[\n",
    "    {\n",
    "        \"model\": \"InnovationGPT4-32\",\n",
    "        \"api_key\": \"xxxxx\",\n",
    "        \"base_url\": \"https://innovationopenaiservice.openai.azure.com/\",\n",
    "        \"api_type\": \"azure\",\n",
    "        \"api_version\": \"2023-06-01-preview\"\n",
    "    }\n",
    "]\n",
    "\n",
    "The problem is, while it runs fine with Autogen 0.2+ and Openai 1.0+, we are trying to integrate Autogen in a project that already uses Openai 0.27.4, so we would like to use Autogen 0.1.14.```.\n",
    "In issues-and-help, at 2023-11-14 15:50:34 a user named aaronward_ said ```It was my fault, i didn't format the tool config correctly. Have shared an example notebook here: https://discord.com/channels/1153072414184452236/1173957465285611551```.\n",
    "In issues-and-help, at 2023-11-14 12:39:00 a user named razahin said ```I've been taking inspiration from the https://github.com/microsoft/autogen/blob/v0.2.0b4/notebook/agentchat_lmm_gpt-4v.ipynb notebook for using GPT4 with vision. Running the examples as written in the notebook works. Passing a public image file, such as one hosted on openAI, also works.\n",
    "\n",
    "When I attempt to modify the commander, coder, and critic prompts to read a local file I am consistently met with messages like \n",
    "\n",
    "`I'm sorry for any confusion, but it appears there may have been a misunderstanding. As an AI text-based interface, I don't have the capability to interpret images or run code.` \n",
    "\n",
    "Otherwise the commander coder workflow will often go completely off the rails. \n",
    "\n",
    "I'm looking for suggestions on how I should approach feeding an image which is located in the local working directory into the MultimodalConversableAgent. The user prompt I am trying is\n",
    "\n",
    "```\n",
    "user_proxy.initiate_chat(designAnalyzer, message=\"\"\"\n",
    "Load the image from the design.jpg file location for processing by an analyzer\"\"\")\n",
    "``````.\n",
    "In issues-and-help, at 2023-11-14 11:56:28 a user named aaronward_ said ```Nevermind, got it working - it was the assistand_id that was messing it up. I passed None and it worked. \n",
    "\n",
    "One thing i noticed so far: its fast! it doesn't waste time talking back and foward with the UserProxy before writing the query - potentially saving money on token costs, i'll need to look into this further. It also seems counter intuitive because usually the UserProxy is the one who is executing the code. it's a great addition to autogen from what i can tell so far.```.\n",
    "In issues-and-help, at 2023-11-14 09:28:20 a user named aaronward_ said ```i'm having issues with my agents not using the provided tool, wondering if someone could give me help - the user proxy keeps trying to run the sql as a file rather than pass it as a string to a python function which is registered.\n",
    "\n",
    "I'm testing out the new **GPTAssistantAgent** with a postgres sql operation. \n",
    "\n",
    "https://github.com/AaronWard/generative-ai-workbook/blob/main/personal_projects/14.openai-assistant-api/OpenAi-assistant-with-autogen.ipynb```.\n",
    "In issues-and-help, at 2023-11-14 04:12:31 a user named levre said ```My group chat eventually turns into user_proxy repeatedly calling GPT4 and (I guess?) getting no response so calling it again.  Is there some way to add logic to terminate when this happens?  It appears to be sending the full context every time, so I'm getting charged for input tokens.```.\n",
    "In issues-and-help, at 2023-11-11 22:40:45 a user named pika.c said ```For now to deal with this I'm creating a new GroupChat object with messages from the previous one and a new GroupChatManager. The problem is that the new agents in the group chat do not have context of what is going on. \n",
    "I'm pretty sure there's a better way to go about this. I'll keep experimenting and update this thread if I make any progress.\n",
    "https://github.com/toshNaik/TaleCraft/tree/main```.\n",
    "In issues-and-help, at 2023-11-11 19:44:40 a user named ab.z said ```add --pre flag to see pre-releases```.\n",
    "In issues-and-help, at 2023-11-11 19:42:12 a user named ab.z said ```Because it is not released yet, it is a pre-release```.\n",
    "In issues-and-help, at 2023-11-11 13:10:24 a user named malicor said ```i'm having this code:\n",
    "\n",
    "https://pastebin.com/faWpJi6T\n",
    "\n",
    "but it's still throwing the same error```.\n",
    "In issues-and-help, at 2023-11-10 04:06:40 a user named reporter said ```https://discord.com/channels/1153072414184452236/1162811675762753589/1171829828232695870\n",
    "\n",
    "Might be helpful but let me know if it isn't.```.\n",
    "In issues-and-help, at 2023-11-09 19:20:12 a user named sonichi said ```Check this: https://microsoft.github.io/autogen/docs/Installation#python```.\n",
    "In issues-and-help, at 2023-11-09 18:46:49 a user named yigitkonur said ```for those who are facing with this issue, here is how I fixed this:\n",
    "\n",
    "you should install autogen by following command:\n",
    "\n",
    "```\n",
    "pip install pyautogen==0.2.0b2\n",
    "```\n",
    "\n",
    "here is how to load config to fix this problem:\n",
    "\n",
    "```\n",
    "import autogen\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        \"model\": \"YOUR_DEPLOYMENT_NAME\",  \n",
    "        \"base_url\": \"https://xxx.openai.azure.com\", \n",
    "        \"api_type\": \"azure\", \n",
    "        \"api_version\": \"2023-07-01-preview\", \n",
    "        \"api_key\": \"xxx\"\n",
    " }\n",
    "]\n",
    "``````.\n",
    "In issues-and-help, at 2023-11-09 16:06:01 a user named c_bonadio said ```Hi @wadymc I spent some time with function_call and I think I got some reasonable understanding.\n",
    "\n",
    "I even created an agent that can self execute its own function call\n",
    "https://gist.github.com/bonadio/96435a1b6ccc32297aa8cc1db7cfc381\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f'''// Goal: Extract useful Q&A pairs from a given text.\n",
    "Here is the text snippet of interest:\n",
    "{context}\n",
    "\n",
    "\n",
    "// Requirements:\n",
    "// 1. Identify and compile 10 pairs of questions and answers.\n",
    "// 2. Exclude irrelevant conversational speech.\n",
    "// 3. Include complete URLs where relevant.\n",
    "// 4. Omit specific usernames, channels, or timestamps.\n",
    "// 5. Focus on general-use content, avoiding overly specific user conversations.\n",
    "// 6. When citing error messages, include the complete error while omitting personal identifiers like IDs or usernames.\n",
    "// 7. Where relevant, supplement answers with complete code snippets in a code block format.\n",
    "// Restrictions:\n",
    "// - Do not invent or create answers; rely solely on the provided text.\n",
    "\n",
    "// Instructions for AI:\n",
    "// Analyze the provided text, adhering to the above guidelines, to extract relevant and general Q&A pairs that would be beneficial for a broader audience. Ensure that each answer is clearly connected to its question, maintaining the integrity and context of the original discussion.\n",
    "\n",
    "// Examples:\n",
    "```\n",
    "Question: How do I load an image for processing by an analyzer in Python?\n",
    "Answer: Use the following code to load an image from a specified file location for processing:\n",
    "```\n",
    "user_proxy.initiate_chat(designAnalyzer, message=\"\"\"Load the image from the <img ./coding/design.jpg> file location for processing by an analyzer\"\"\")\n",
    "```\n",
    "\n",
    "Question: How do I install a specific version of Autogen with OpenAI?\n",
    "Answer: To install Autogen 0.2.0b5 with OpenAI 1.2.4, use the command:\n",
    "```\n",
    "pip install autogen==0.2.0b5\n",
    "```\n",
    "This resolves the issue with the error: InvalidRequestError: Invalid URL (POST /v1/openai/deployments/InnovationGPT4-32/chat/completions) encountered with earlier versions.\n",
    "\n",
    "Question: How do I integrate Autogen with a project using an older version of OpenAI?\n",
    "Answer: To integrate Autogen with a project using OpenAI 0.27.4, you might face compatibility issues. Autogen 0.1.14 raises an InvalidRequestError with OpenAI 0.28.1, while Autogen 0.2+ runs fine with OpenAI 1.0+. The compatibility must be checked between specific versions.\n",
    "```\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_token_len(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"../data/docs/22112023_qa.txt\"\n",
    "with open(output_file, 'w') as file:\n",
    "    # iterate over chunks of the text file\n",
    "    # with a size of 3000 tokens\n",
    "    # chunk = ...\n",
    "\n",
    "    prompt_response = api_utils.prompt(chunk)\n",
    "    print(prompt_response)\n",
    "\n",
    "    file.write(formatted_text.strip().rstrip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
