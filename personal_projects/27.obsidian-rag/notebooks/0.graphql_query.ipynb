{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphQL query with Obsidian Notes\n",
    "\n",
    "I am exposing the notes as a local GraphQL server using this [plugin](https://github.com/TwIStOy/obsidian-local-graphql). <br>\n",
    "Access the Apollo client UI by going to this link [http://localhost:28123/](http://localhost:28123/)\n",
    "\n",
    "### Notes:\n",
    "- The score returned indicating the match relevancy to the search query is arbitrary, there is no description of how this is derived from the obsidian docs. \n",
    "- I set a minimum negative score for results to be displayed, the closer to 0 the better.\n",
    "- I have also added a `top_n` parameter, as many results may be under the threshold for negative score. This is to ensure we are not passing massive amounts of context to the LLM\n",
    "- The simple search really only works when words are spelled exactly as they can be found within the documents - Probably should only be used when a list of the titles or specific tags are give to the LLM for context, so they can do a top down search by category. *For example: search \"generative AI\" and get all the child documents to that topic.*\n",
    "- The fuzzy search works well for somewhat similar spelling. Good for specific searches that may not be broad topics or categories.\n",
    "- Queries should use search terms like \"QLoRA\", and not questions like \"What is QLoRA?\" as the search functionality will look for \"What is...\" which may not be in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! curl --request POST \\\n",
    "#   --header 'content-type: application/json' \\\n",
    "#   --url http://localhost:28123/ \\\n",
    "#   --data '{\"query\":\"query { __typename }\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "\n",
    "from gql import gql, Client\n",
    "from gql.transport.requests import RequestsHTTPTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure the client\n",
    "transport = RequestsHTTPTransport(\n",
    "    url='http://localhost:28123/',  # Make sure this URL is correct\n",
    "    use_json=True,\n",
    "    headers={'Content-Type': 'application/json'},\n",
    "    verify=True,\n",
    "    retries=3,\n",
    ")\n",
    "\n",
    "client = Client(transport=transport, fetch_schema_from_transport=True)\n",
    "\n",
    "\n",
    "def define_query_string(search_type):\n",
    "    # Define your query with the variable\n",
    "    string = f\"\"\" \\\n",
    "      query ExampleQuery($query: String!) {{\n",
    "        vault {{\n",
    "          {search_type}(query: $query) {{\n",
    "            file {{\n",
    "              name\n",
    "              readContent\n",
    "              stat {{\n",
    "                size\n",
    "              }}\n",
    "            }}\n",
    "            score\n",
    "          }}\n",
    "        }}\n",
    "      }}\n",
    "    \"\"\"\n",
    "    return gql(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_graph(question, result, search_type, min_neg_score=-3):\n",
    "    # Create a graph\n",
    "    G = nx.Graph()\n",
    "    G.add_node(question, size=20, title=question)\n",
    "    # Parse JSON and add nodes and edges\n",
    "    for item in result[\"vault\"][search_type]:\n",
    "        if item[\"score\"] > min_neg_score:\n",
    "            file_name = item[\"file\"][\"name\"]\n",
    "            score = item[\"score\"]\n",
    "\n",
    "            # Adding each file as a node\n",
    "            G.add_node(file_name, size=10, title=f\"Score: {score}\")\n",
    "            \n",
    "            # Adding an edge from each file to the central query node, weight is the inverse of score for visualization\n",
    "            G.add_edge(question, file_name, weight=score)\n",
    "\n",
    "    # Convert to a PyVis network for visualization\n",
    "    nt = Network(\"500px\", \"1000px\", notebook=True)\n",
    "    nt.from_nx(G)\n",
    "    nt.show(f\"fuzzy_search_graph_{search_type}.html\")\n",
    "\n",
    "def print_result(result, min_neg_score, search_type, top_n):\n",
    "    # Filter results based on min_neg_score and sort by score descending (closest to 0)\n",
    "    top_items = sorted(\n",
    "        (item for item in result[\"vault\"][search_type] if item[\"score\"] > min_neg_score),\n",
    "        key=lambda x: x[\"score\"], \n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Get top N results\n",
    "    if top_n != -1:\n",
    "        top_items = top_items[:top_n]\n",
    "\n",
    "    for item in top_items:\n",
    "        file_name = item[\"file\"][\"name\"]\n",
    "        score = item[\"score\"]\n",
    "        read_content = item['file']['readContent']\n",
    "\n",
    "        print(f\"File Name: {file_name}\")\n",
    "        print(f\"Score: {score}\")\n",
    "        print(f\"Content:\\n{read_content}\")\n",
    "        print(\">>>>-----------------------------\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIMPLE_SEARCH_TYPE = \"simpleSearch\"\n",
    "FUZZY_SEARCH_TYPE = \"fuzzySearch\"\n",
    "MIN_NEG_SCORE = -3\n",
    "TOP_N = 3\n",
    "\n",
    "simple_search_query = define_query_string(SIMPLE_SEARCH_TYPE)\n",
    "fuzzy_search_query = define_query_string(FUZZY_SEARCH_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Search vs Fuzzy Search\n",
    "\n",
    "Fuzzy search is a feature that returns results based on likely relevance even if the search terms are not an exact match. This means that even if the words or spellings do not precisely match, the search results will still be displayed based on relevance. Fuzzy search is useful when searching for terms where variations or misspellings might occur, ensuring that relevant results are still provided. It helps save time by offering suggestions and matches that are closely related to the search query, even if not identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Quantization.md\n",
      "Score: -0.08\n",
      "Content:\n",
      "> [[Large Language Model]], [[QLoRA]],\n",
      "### What is Quantization?\n",
      "\n",
      "![[Screenshot 2024-03-05 at 13.40.43.png]]\n",
      "\n",
      "- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\n",
      "- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions\n",
      ">>>>-----------------------------\n",
      "File Name: Pearl.md\n",
      "Score: -0.5349\n",
      "Content:\n",
      "\n",
      "Pearl is a production-ready Reinforcement Learning (RL) AI agent library developed by Meta's Applied Reinforcement Learning team. It's designed to enable researchers and practitioners to develop RL AI agents that prioritize long-term over immediate feedback and adapt to environments with limited observability, sparse feedback, and high stochasticity. Key features include modular design, dynamic action spaces, offline learning, intelligent neural exploration, safe decision making, history summarization, and data augmentation. Pearl supports real-world applications like recommender systems, auction bidding, and creative selection, offering a flexible platform for developing state-of-the-art RL agents.\n",
      ">>>>-----------------------------\n",
      "File Name: Parameter-Efficient Fine-Tuning (PEFT).md\n",
      "Score: -2.8599\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - generative-ai\n",
      "  - llm\n",
      "---\n",
      "> [[Large Language Model]], [[Generative AI]], [[Fine Tuning]]\n",
      "\n",
      "https://github.com/huggingface/peft\n",
      "\n",
      "**Parameter-Efficient Fine-Tuning (PEFT):**\n",
      "   - PEFT techniques aim to reduce the computational costs of fine-tuning by limiting parameter updates.\n",
      "   - One approach, Low-Rank Adaptation (LoRA), posits that not all parameters need updating for effective fine-tuning. By focusing on a low-dimension matrix representative of the task, LoRA can substantially reduce fine-tuning costs.\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with variables\n",
    "variables = {\n",
    "    \"query\": \"LoRA\"\n",
    "}\n",
    "\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Quantization.md\n",
      "Score: -0.18000000000000002\n",
      "Content:\n",
      "> [[Large Language Model]], [[QLoRA]],\n",
      "### What is Quantization?\n",
      "\n",
      "![[Screenshot 2024-03-05 at 13.40.43.png]]\n",
      "\n",
      "- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\n",
      "- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions\n",
      ">>>>-----------------------------\n",
      "File Name: LoRA.md\n",
      "Score: -0.25470000000000004\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - llm\n",
      "  - generative-ai\n",
      "---\n",
      "> [[Large Language Model]], [[Generative AI]], [[Fine Tuning]]\n",
      "\n",
      "LoRA Fine-tuning refers to a technique for improving the performance of a pre-trained language model by fine-tuning it on a specific task or domain. LoRA stands for \"Language-oriented Recurrent Attention\", which is a neural architecture that has been used for pre-training language models.\n",
      "\n",
      "Fine-tuning a pre-trained language model involves further training the model on a specific task or dataset to adapt it to that task or dataset. This is done by adding a task-specific layer on top of the pre-trained model and training the entire model end-to-end on the task-specific data.\n",
      "\n",
      "Fine-tuning a language model on a specific task or domain can improve its performance on that task or domain. For example, if you have a pre-trained language model that has been trained on general text data, you can fine-tune it on a specific text classification task, such as sentiment analysis or named entity recognition, to improve its performance on that task.\n",
      "\n",
      "LoRA Fine-tuning is a specific implementation of this technique that uses the LoRA architecture for pre-training language models and fine-tuning them on specific tasks or domains.\n",
      ">>>>-----------------------------\n",
      "File Name: QLoRA.md\n",
      "Score: -0.3177\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - generative-ai\n",
      "---\n",
      "> [[Fine Tuning]], [[Fine Tuning]], [[Generative AI]], [[Quantization]]\n",
      "### QLoRA\n",
      "\n",
      "There are 4 components to QLoRA:\n",
      "1. **4-bit NormalFloat**\n",
      "\t- This is essentially just a way to bucket values\n",
      "\t- Representing a value within 4 binary values\n",
      "\t- *ex: `0101`*\n",
      "\t- 16 Unique unique combinations ![[Screenshot 2024-03-05 at 13.48.24.png]]\n",
      "2. **Double Quantization**\n",
      "\t- \n",
      "3. **Paged Optimizers**\n",
      "\t- Used to provide higher utilization of memory across resources\n",
      "\t- Instead of being restricted to GPU only memory, you can move pages of memory to the CPU memory, and retrieve when required.\n",
      "4. **LoRA**\n",
      "\t- Full finetuning would involved reconfiguring weights for entire model - LoRA allows you to  add a small number of trainable parameters to the \n",
      "\n",
      "![[Screenshot 2024-03-05 at 14.22.14.png]]\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming plot_graph is a function you have defined earlier to create and display the graph\n",
    "# plot_graph(variables[\"query\"], result, FUZZY_SEARCH_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Quantization.md\n",
      "Score: -0.079\n",
      "Content:\n",
      "> [[Large Language Model]], [[QLoRA]],\n",
      "### What is Quantization?\n",
      "\n",
      "![[Screenshot 2024-03-05 at 13.40.43.png]]\n",
      "\n",
      "- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\n",
      "- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions\n",
      ">>>>-----------------------------\n",
      "File Name: QLoRA.md\n",
      "Score: -1.5467\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - generative-ai\n",
      "---\n",
      "> [[Fine Tuning]], [[Fine Tuning]], [[Generative AI]], [[Quantization]]\n",
      "### QLoRA\n",
      "\n",
      "There are 4 components to QLoRA:\n",
      "1. **4-bit NormalFloat**\n",
      "\t- This is essentially just a way to bucket values\n",
      "\t- Representing a value within 4 binary values\n",
      "\t- *ex: `0101`*\n",
      "\t- 16 Unique unique combinations ![[Screenshot 2024-03-05 at 13.48.24.png]]\n",
      "2. **Double Quantization**\n",
      "\t- \n",
      "3. **Paged Optimizers**\n",
      "\t- Used to provide higher utilization of memory across resources\n",
      "\t- Instead of being restricted to GPU only memory, you can move pages of memory to the CPU memory, and retrieve when required.\n",
      "4. **LoRA**\n",
      "\t- Full finetuning would involved reconfiguring weights for entire model - LoRA allows you to  add a small number of trainable parameters to the \n",
      "\n",
      "![[Screenshot 2024-03-05 at 14.22.14.png]]\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    \"query\": \"QLoRA\"\n",
    "}\n",
    "\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Quantization.md\n",
      "Score: -0.079\n",
      "Content:\n",
      "> [[Large Language Model]], [[QLoRA]],\n",
      "### What is Quantization?\n",
      "\n",
      "![[Screenshot 2024-03-05 at 13.40.43.png]]\n",
      "\n",
      "- Quantizing involves splitting a range of numbers (which is infinite in floating point) into bins\n",
      "- Quantization is needed when you want to represent a value from an infinite range in a physically constrained system, as you need infinite bytes - you need to make some assumptions\n",
      ">>>>-----------------------------\n",
      "File Name: QLoRA.md\n",
      "Score: -0.2167\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - generative-ai\n",
      "---\n",
      "> [[Fine Tuning]], [[Fine Tuning]], [[Generative AI]], [[Quantization]]\n",
      "### QLoRA\n",
      "\n",
      "There are 4 components to QLoRA:\n",
      "1. **4-bit NormalFloat**\n",
      "\t- This is essentially just a way to bucket values\n",
      "\t- Representing a value within 4 binary values\n",
      "\t- *ex: `0101`*\n",
      "\t- 16 Unique unique combinations ![[Screenshot 2024-03-05 at 13.48.24.png]]\n",
      "2. **Double Quantization**\n",
      "\t- \n",
      "3. **Paged Optimizers**\n",
      "\t- Used to provide higher utilization of memory across resources\n",
      "\t- Instead of being restricted to GPU only memory, you can move pages of memory to the CPU memory, and retrieve when required.\n",
      "4. **LoRA**\n",
      "\t- Full finetuning would involved reconfiguring weights for entire model - LoRA allows you to  add a small number of trainable parameters to the \n",
      "\n",
      "![[Screenshot 2024-03-05 at 14.22.14.png]]\n",
      ">>>>-----------------------------\n",
      "File Name: GraphQL.md\n",
      "Score: -2.4351\n",
      "Content:\n",
      "> [[Data]], [[Graph Databases]]\n",
      "\n",
      "**GraphQL** is a query language for APIs and a runtime for executing those queries by using a type system you define for your data. Unlike REST, which uses multiple endpoints to access data, GraphQL uses a single endpoint with queries to access different data. The query language looks a lot like JSON, but with only the keys and no values.\n",
      "\n",
      "```gql\n",
      "{\n",
      "  allBooks {\n",
      "    title\n",
      "    authors {\n",
      "      name\n",
      "    }\n",
      "  }\n",
      "}\n",
      "```\n",
      "\n",
      "### Benefits of Using GraphQL\n",
      "- **Single Request**: Fetch all the data you need in a single query, which can decrease the amount of data transferred over the network.\n",
      "- **No Over-fetching**: You can specify exactly which data you need, avoiding the extra load and bandwidth usage.\n",
      "### Cons of Using GraphQL\n",
      "- **Caching**: Since it uses POST requests for queries, it can be harder to implement caching compared to REST, which can use standard HTTP caching mechanisms.\n",
      "- **Rate Limiting**: It's more complex because the cost of a query varies greatly depending on its structure.\n",
      "### Practical Applications\n",
      "- **Aggregating Data from Multiple Sources**: With GraphQL, you can create a single API endpoint that pulls in data from various databases and third-party APIs.\n",
      "- **Building Dynamic and Real-time UIs**: GraphQL's flexibility makes it well-suited for dynamic UIs that require real-time data updates.\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"query\": \"What is QLoRA?\"\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: 24 - Choosing the Best Performing Model.md\n",
      "Score: -2.1782\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "  - mlops\n",
      "  - ml-pipelines\n",
      "---\n",
      "> [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[Model Evaluation]]\n",
      "### Choose the Best Performing Model\n",
      "- Go to table visualisation for a group of experiment runs\n",
      "- There are many columns, you can customise\n",
      "- Sort by loss/accuracy\n",
      "- You can get the parameters for the best run\n",
      "\n",
      "![[download-2 2.png]]\n",
      ">>>>-----------------------------\n",
      "File Name: 4. Error analysis and Performance Auditing.md\n",
      "Score: -2.1866999999999996\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - mlops\n",
      "  - ml\n",
      "---\n",
      "> [[ML Operations]],  [[Machine Learning]]\n",
      "> \n",
      "Using the covariates (tags) in the test data set can be useful for error analysis. You can also use supplementary meta-data to determine implicit root causes of errors\n",
      "\n",
      "Questions to ask:\n",
      "- What % of errors has a given tag?\n",
      "- Of all data with that tag, what fraction is misclassified?\n",
      "- What fraction of data has that tag?\n",
      "\n",
      "![[download-4 1.png]]\n",
      "## Performance Auditing\n",
      "\n",
      "Auditing framework\n",
      "1. Brainstorm ways system can go wrong\n",
      "\t- Subsets of data (ethnicity, gender)\n",
      "\t- How common are certain errors (FP, FN)\n",
      "\t- Performance of rare classes\n",
      "2. Establish metrics to asses performance against these issues on appropriate slices of data\n",
      "\t- Analyse these slices of data\n",
      "3. Get business owner to asses these problems\n",
      ">>>>-----------------------------\n",
      "File Name: 5 - Model Cards.md\n",
      "Score: -2.1990999999999996\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "  - mlops\n",
      "  - ml-pipelines\n",
      "  - deployment\n",
      "---\n",
      "> > [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[Deployment]], \n",
      "### Model Cards\n",
      "- Encapsulate all the key details about the model for future reference\n",
      "- It’s like a memo for your model\n",
      "- Should be understandable\n",
      "- Should display how the model:\n",
      "\t- Data used\n",
      "\t- How to use it\n",
      "\t- And its shortcomings\n",
      "\t- Metrics\n",
      "\t- Parameters\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    \"query\": \"Machine Learning Operations\"\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: 24 - Choosing the Best Performing Model.md\n",
      "Score: -2.1782\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "  - mlops\n",
      "  - ml-pipelines\n",
      "---\n",
      "> [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[Model Evaluation]]\n",
      "### Choose the Best Performing Model\n",
      "- Go to table visualisation for a group of experiment runs\n",
      "- There are many columns, you can customise\n",
      "- Sort by loss/accuracy\n",
      "- You can get the parameters for the best run\n",
      "\n",
      "![[download-2 2.png]]\n",
      ">>>>-----------------------------\n",
      "File Name: 5 - Model Cards.md\n",
      "Score: -2.1990999999999996\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "  - mlops\n",
      "  - ml-pipelines\n",
      "  - deployment\n",
      "---\n",
      "> > [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[Deployment]], \n",
      "### Model Cards\n",
      "- Encapsulate all the key details about the model for future reference\n",
      "- It’s like a memo for your model\n",
      "- Should be understandable\n",
      "- Should display how the model:\n",
      "\t- Data used\n",
      "\t- How to use it\n",
      "\t- And its shortcomings\n",
      "\t- Metrics\n",
      "\t- Parameters\n",
      ">>>>-----------------------------\n",
      "File Name: 4. Versioning Data and Artifacts.md\n",
      "Score: -2.2108999999999996\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - mlops\n",
      "  - ml\n",
      "  - ml-pipelines\n",
      "  - data\n",
      "---\n",
      "> [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[3.  Using MLflow for Experiment Tracking]]\n",
      "### Run:\n",
      "- One script of notebook execution\n",
      "- Multiple runs per script are possible\n",
      "- You can attach:\n",
      "\t- Args\n",
      "\t- Metrics\n",
      "\t- Artifacts\n",
      "\t- Images\n",
      "\n",
      "### Experiment\n",
      "- A grouping of runs\n",
      "### Project:\n",
      "- Highest level of grouping in W&B\n",
      "\t- An Artifact is any data file or directory produced within a run. Every artefact is automatically versioned.\n",
      "\t- The run needs to finish using .finish() before the ratification is uploaded to the W&B server\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with variables\n",
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Reinforcement Learning from Human Feedback (RLHF).md\n",
      "Score: -2.2328\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - fine-tuning\n",
      "  - llm\n",
      "  - generative-ai\n",
      "---\n",
      " > [[Large Language Model]], [[Generative AI]], [[Fine Tuning]]\n",
      "\n",
      "**Reinforcement Learning from Human Feedback (RLHF):**\n",
      "   - RLHF is an advanced technique wherein human reviewers rate model outputs to guide its fine-tuning.\n",
      "   - It's a complex process, often reserved for organizations with substantial resources.\n",
      "   - A prominent example of RLHF is ChatGPT by OpenAI, which underwent several stages of fine-tuning based on human feedback.\n",
      "[OpenAI's InstructGPT Paper](https://www.openai.com/research/)\n",
      ">>>>-----------------------------\n",
      "File Name: Untitled.md\n",
      "Score: -2.3185\n",
      "Content:\n",
      "\n",
      "Learning embedded systems often involves understanding both software and hardware. It requires knowledge of electrical engineering, computer architecture, programming (typically in C or C++), and sometimes real-time operating systems. The challenge in embedded systems lies in dealing with resource constraints (like memory and processing power), understanding hardware specifics, and often needing to have a good grasp of both high-level and low-level programming concepts.\n",
      ">>>>-----------------------------\n",
      "File Name: Natural Language Processing (NLP).md\n",
      "Score: -2.7308999999999997\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "---\n",
      "> [[Machine Learning]]\n",
      "### What is NLP?\n",
      "- Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.\n",
      "\n",
      "In fact, a typical interaction between humans and machines using Natural Language Processing could go as follows:\n",
      "1. A human talks to the machine\n",
      "2. The machine captures the audio\n",
      "3. Audio to text conversion takes place\n",
      "4. Processing of the text’s data\n",
      "5. Data to audio conversion takes place\n",
      "6. The machine responds to the human by playing the audio file\n",
      "### What is NLP used for?\n",
      "\n",
      "Natural Language Processing is the driving force behind the following common applications:\n",
      "- Language translation applications such as Google Translate\n",
      "- Word Processors such as Microsoft Word and Grammarly that employ NLP to check grammatical accuracy of texts.\n",
      "- Interactive Voice Response (IVR) applications used in call centers to respond to certain users’ requests.\n",
      "- Personal assistant applications such as OK Google, Siri, Cortana, and Alexa.\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "variables = {\n",
    "    \"query\": \"PU Learning\"\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: Positive-Unlabelled (PU) Learning.md\n",
      "Score: -1.1520000000000001\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - ml\n",
      "---\n",
      "> [[Machine Learning]]\n",
      "### What is PU Learning?\n",
      "PU learning, which stands for positive and unlabelled learning, is a semi-supervised binary classification method that recovers labels from unknown cases in the data. It does this by learning from the positive cases in the data and applying what it has learned to relabel the unknown cases. This approach provides benefits to any machine learning problem that requires binary classification on unreliable data, regardless of the domain.\n",
      "\n",
      "There are two main approaches to applying PU learning. These include:\n",
      "- PU bagging \n",
      "- Two-step approach\n",
      "\n",
      "[https://heartbeat.fritz.ai/positive-and-unlabelled-learning-recovering-labels-for-data-using-machine-learning-59c1def5452f](https://heartbeat.fritz.ai/positive-and-unlabelled-learning-recovering-labels-for-data-using-machine-learning-59c1def5452f)\n",
      ">>>>-----------------------------\n",
      "File Name: Apple Neural Engine (ANE).md\n",
      "Score: -1.7685\n",
      "Content:\n",
      "> [[Hardware]]\n",
      "\n",
      "The Apple Neural Engine (ANE) is a component within Apple devices, specifically designed for efficiently executing deep neural network operations. \n",
      "- It is a cluster of specialized computing cores that accelerates machine learning (ML) and artificial intelligence (AI) algorithms, providing significant speed, memory, and power advantages over the main CPU or GPU. \n",
      "- ANE first appeared in the A11 chip in the iPhone X in 2017 and has evolved since then, with the latest version in the A15 Bionic chip being significantly faster.\n",
      "- ANE enables a range of features like natural language processing, image analysis, and advanced camera effects, all without needing to rely on cloud processing or excessive power consumption. \n",
      "- Developers can access ANE through Apple's Core ML framework, allowing them to build, train, and run ML models directly on the device, leveraging not only ANE but also CPU and GPU for optimal performance​\n",
      "\n",
      "[https://tipsmake.com/what-is-apples-neural-engine-how-does-it-work](https://tipsmake.com/what-is-apples-neural-engine-how-does-it-work)​.\n",
      ">>>>-----------------------------\n",
      "File Name: Zero-Shot.md\n",
      "Score: -2.6983\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - generative-ai\n",
      "  - llm\n",
      "  - prompting-strategies\n",
      "---\n",
      "> [[Large Language Model]], [[Generative AI]], [[Prompting Strategies]]\n",
      "\n",
      "In the context of machine learning, zero-shot refers to a type of learning where a model is trained to perform a task without being explicitly trained on that task. Instead, the model is trained on related tasks and then can perform the new task without additional training data. This is achieved by encoding the task description or instructions as part of the input to the model, allowing it to generalize to new tasks that share similar characteristics. Zero-shot learning is often used in natural language processing tasks, such as language translation, where the model can translate between two languages it has never seen before by relying on its understanding of the structure and semantics of languages it has been trained on.\n",
      "\n",
      "\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with variables\n",
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = {\n",
    "    \"query\": \"Model Bias\"\n",
    "}\n",
    "\n",
    "# Execute the query with variables\n",
    "result = client.execute(simple_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, SIMPLE_SEARCH_TYPE, TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name: 8. Aequitas.md\n",
      "Score: -1.1400000000000001\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - data\n",
      "  - ml\n",
      "  - model-bias\n",
      "  - mlops\n",
      "  - model-testing\n",
      "---\n",
      "> [[Model Evaluation]], [[ML Pipelines]], [[Model Bias (Fairness)]], [[7.  Model Bias]]\n",
      "### Aequitas\n",
      "\n",
      "Aequitas is an open-source bias audit toolkit that allows users to evaluate their machine learning models for fairness. It can be used both as a Python library or via a web interface. Aequitas offers various fairness metrics and visualizations to help you understand and address bias in your model. Here are the requirements for using Aequitas:\n",
      "- Model score\n",
      "- Label\n",
      "- Binary classification only\n",
      "- At least one categorical feature OR a numerical feature\n",
      "\n",
      "There are 3 reports generated:\n",
      "- Create cross-tabulation using Group() class(this is the basis for all analysis)\n",
      "- After the cross tab we compute bias using Bias() class\n",
      "- Then we compute fairness using Fairness() class\n",
      "- This can result in a beefy data frame \n",
      "\n",
      "\n",
      "\n",
      "[http://aequitas.dssg.io/](http://aequitas.dssg.io/) \n",
      "### Other Tools:\n",
      "- What-IF tool\n",
      "- Fairlearn\n",
      "- FairML\n",
      ">>>>-----------------------------\n",
      "File Name: 7.  Model Bias.md\n",
      "Score: -1.329\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - data\n",
      "  - ml\n",
      "  - mlops\n",
      "  - model-testing\n",
      "  - model-bias\n",
      "---\n",
      "> [[Machine Learning]], [[ML Operations]], [[ML Pipelines]], [[Model Evaluation]], \n",
      "### Model Bias\n",
      "- Bias is multifaceted, and isn’t as concrete as model performance metrics\n",
      "- More accurately: data bias\n",
      "\t- Note the same as bias-variance tradeoff\n",
      "- Slice performance may help indicate bias\n",
      "- Bias is distinct, but occasionally related to model explainabilty \n",
      "\n",
      "### Types of data bias:\n",
      "- **Human error**\n",
      "\t- Sampling error: mismatch between same and intended population (too small)\n",
      "\t- Exclusion bias: group excluded from survey/data\n",
      "\t- Recall bias: Unreliability of getting people to recall events from the past\n",
      "- **Society error**\n",
      "\t- Implicit bias\n",
      "\t- Unjust policy\n",
      "\n",
      "### When bias can appear:\n",
      "- Collection stage\n",
      "\t- Ex: images from web not match user input\n",
      "- Annotation\n",
      "\t- Labeler bias\n",
      "- Preprocessing\n",
      "\t- Data cleaning \n",
      "\t- example: truncating something for memory issues\n",
      "\n",
      "**Bias can creep in at any part of the process**\n",
      "\n",
      "\n",
      ">>>>-----------------------------\n",
      "File Name: Model Bias (Fairness).md\n",
      "Score: -2.0702\n",
      "Content:\n",
      "---\n",
      "tags:\n",
      "  - model-bias\n",
      "  - ml\n",
      "  - data-science\n",
      "---\n",
      "> [[Machine Learning]], [[Data Science]]\n",
      "\n",
      "The concepts of \"systematically unfair\" and \"fairness\" in the context of machine learning and AI are indeed complex and multifaceted. While there's no universally accepted definition or objective measure of fairness, there have been significant efforts in the research community to define and measure fairness in algorithmic systems. Several fairness metrics and definitions have been proposed, but each has its own set of assumptions, advantages, and limitations.\n",
      "\n",
      "Here are some commonly referenced fairness metrics:\n",
      "### 2. Equalized Odds:\n",
      "- This metric requires that both true positive rates and false positive rates are equal across groups. It's especially relevant for binary classification problems.\n",
      "### 3. Disparate Impact:\n",
      "- Compares the selection rates of unprivileged and privileged groups. If they are roughly equal (within a certain range), then there's no disparate impact.\n",
      "### 4. Individual Fairness:\n",
      "- It posits that similar individuals should be treated similarly. This definition requires a notion of similarity, which can be context-dependent.\n",
      "### 5. Counterfactual Fairness:\n",
      "- A decision is considered counterfactually fair if the same decision would have been made had an individual belonged to a different demographic group, holding all other characteristics constant.\n",
      "\n",
      "### 6. Treatment Equality:\n",
      "- This metric requires that individuals who receive the same outcome (either positive or negative) are treated similarly across different groups.\n",
      "\n",
      "### However, even with these metrics:\n",
      "- **Trade-offs Exist:**\n",
      "\t- Sometimes, it's not possible to satisfy all fairness definitions simultaneously. For example, achieving demographic parity might come at the expense of equalized odds.\n",
      "- **Context Matters:**\n",
      "\t- The appropriate fairness metric can vary based on the domain (e.g., hiring vs. lending) and societal values.\n",
      "- **Quantitative Limits:**\n",
      "\t- Fairness isn't just a quantitative issue. It involves ethical, legal, and societal considerations, which aren't always easily captured in metrics.\n",
      "- **Dangers of Oversimplification:**\n",
      "\t-  Reducing fairness to a single number or metric can be an oversimplification. It's essential to consider the broader context, gather feedback from stakeholders, and ensure continuous monitoring and iteration.\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "## Bias Assessment:\n",
      "\n",
      "Bias is a systematic error from an erroneous assumption in the machine learning algorithm’s modelling.  The algorithm tends to systematically learn the wrong signals by not considering all the information contained within the data. Model bias may lead an algorithm to miss the relevant relationship between data inputs (features) and targeted outputs (predictions).  In essence, bias arises when an algorithm has insufficient capability in learning the appropriate signal from the dataset.\n",
      "\n",
      "**Sample Bias:** Also known as selection bias, sampling bias occurs when the training data is not sampled randomly enough from the collected data, creating a preference towards some populations.\n",
      "\n",
      "**Exclusion Bias**: During data pre-processing, features or subsets of data that are considered irrelevant end up being removed. This can consist of removing null values, outliers, or other extraneous data points. The removal process may lead to exclusion bias and the removed features may end up being underrepresented when the data is applied to a real-world problem and result in the loss of the true accuracy of the data collected.\n",
      "\n",
      "\n",
      "**Measurement Bias**: This type of bias occurs when the data collected for training differs from that collected in the real world, or when faulty measurements result in data distortion. A good example of this bias occurs in image recognition datasets, where the training data is collected with one type of camera, but the production data is collected with a different camera\n",
      "\n",
      "**Observer Bias**: Also known as confirmation bias, observer bias is the effect of seeing what you expect to see or want to see in data. This can happen when researchers go into a project with subjective thoughts about their study, either conscious or unconscious. We can also see this when labellers let their subjective thoughts control their labelling habits, resulting in inaccurate data. An example can be seen in data labelling tasks where one data worker chooses a different label based on their subjective thoughts as opposed to other workers who follow the provided objective guidelines. Imagine the guidelines suggest that all tomato images should be tagged as fruit, yet one labeller believes that it should be classified as a vegetable and labels it accordingly. This would result in inaccurate data.\n",
      "\n",
      "**Demographic Bias**: Demographic bias occurs when the training data is reflecting a certain demographic, such as a particular race or gender. Racial and Gender bias occurs when data skews in favour of particular demographics. When a model is trained on racially biased data, the outcome itself can be skewed.\n",
      "\n",
      "**Association Bias**: This bias occurs when the data for a machine learning model reinforces and/or multiplies a cultural bias. Association bias skews, misleads, or distorts the way a model learns to associate certain features to be true based on the training data. Essentially, this reinforces a cultural bias if the data was not collected thoughtfully. For example, a dataset may have a collection of jobs in which all men are doctors and all women are nurses. This does not mean that women cannot be doctors, and men cannot be nurses. However, as far as the model is concerned, female doctors and male nurses do not exist. Association bias is best known for creating gender bias.\n",
      "\n",
      "### Disparity Testing:\n",
      "\n",
      "Demographic data such as race, gender, and age are not directly used in training. Yet, these factors can influence data. Even if demographic information isn't used directly for training, there could be specific patterns within the data that are associated with certain demographics. If these associations are due to biases in data collection or representation, the model might unintentionally learn these biases. A series of tests have been performed in order to identify and remedy any potential bias in the data. here are difference metrics to assess for when assessing for bias - just like with performance metrics as seen in [[4. Define Data and Establish baseline]], the optimal metric for bias depends on your specific use-case:\n",
      "\n",
      "- **Disparate Impact:** This test measures whether different demographic groups (e.g., race, age, gender) receive similar favourable outcomes. The aim is to ensure that all demographic groups, especially the disproportionately demographics recieve equitale outcomes from the result of this model being in production. \n",
      "- **Precision**: Also known as the Positive Predictive Value, it measures the proportion of true positives among the predicted positives. ​A high precision indicates that most of the positive predictions made by the model are correct.​\n",
      "- **Predicted Positive Rate (PPR)**: This metric represents the fraction of entities predicted as positive that belong to a particular group. It helps in understanding if the model disproportionately classifies certain groups as high-risk and needs intervention. For fairness, this should be balanced across groups. A high PPR for one group relative to others may indicate that the model disproportionately classifies that group as high-risk.​\n",
      "- **False Positive Rate (FPR):** This metric measures the fraction of false positives within a group relative to the total number of actual negatives in that group. A high FPR could lead to unnecessary medical treatments and costs for the individuals within that group.​ You'd want to ensure this is balanced across all groups.​\n",
      "-  **False Negative Rate (FNR)**: This metric measures the fraction of false negatives within a group relative to the total number of actual positives in that group. A high FNR means the model is failing to identify the positive class\n",
      "- **True Positive Rate (TPR)**: This metric measures the proportion of true positives within a group relative to the total number of actual positives in that group. A high TPR indicates that the model is good at correctly identifying individuals who actually require medical intervention.​\n",
      "- **True Negative Rate (TNR)**: This metric measures the proportion of true negatives within a group relative to the total number of actual negatives in that group.  A high TNR means the model is good at correctly identifying individuals who do not require medical intervention. A high TNR suggests that the model is effective in correctly identifying individuals who do not require medical intervention.​\n",
      "- **False Omission Rate (FOR)**: This metric represents the fraction of false negatives of a group within the predicted negative of the group. A high FOR could mean that the model is missing individuals who actually need medical attention but are labelled as low risk.​\n",
      "-  **False Discovery Rate (FDR)**: This metric measures the fraction of false positives within a group relative to the total number of predicted positives for that group. A high FDR suggests that the model is generating too many false positives, leading to unnecessary medical interventions.​\n",
      "- **Negative Predictive Value (NPV)**: This metric represents the proportion of true negatives among the predicted negatives.​ With NPV higher is generally better. A high NPV indicates that the model is reliable in its negative predictions, meaning individuals flagged as low-risk are indeed low-risk.​\n",
      "- **Predicted Prevalence (PPrev)**: This metric measures the fraction of samples within a group which were predicted as positive. ​It provides an overall picture of how often the model predicts a positive outcome within each group. For fairness, should be balanced across groups. A high PPrev for one group compared to others might suggest that the model is biased towards flagging that specific group as positive.​\n",
      "-  **Prevalence (Prev)**: This metric measures the actual proportion of positive cases within each group.​ It serves as a baseline to evaluate how well the model's predictions align with real-world occurrences of the event being predicted. This is more of a descriptive statistic to understand the baseline rate of the outcome in different groups. It is not about \"high\" or \"low\" being better but should be used to compare with PPrev for assessing model calibration.​\n",
      "\n",
      "\n",
      "\n",
      ">>>>-----------------------------\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with variables\n",
    "result = client.execute(fuzzy_search_query, variable_values=variables)\n",
    "print_result(result, MIN_NEG_SCORE, FUZZY_SEARCH_TYPE, TOP_N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
