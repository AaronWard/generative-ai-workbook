{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llamafile x Autogen\n",
    "\n",
    "This notebook aims to show how you can run mixtral 8x7B with Autogen.\n",
    "\n",
    "**Step to run locally:**\n",
    "- Download a llamafile of the [HuggingFace Repo](https://huggingface.co/award40/mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile/tree/main) - no installations needed, read the model card.  \n",
    "- `chmod +x mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile`\n",
    "- `./mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile --port 8081`\n",
    "<!-- ./llamafile -m mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf -->\n",
    "\n",
    "Like many of you, i am GPU poor. The goal behind this approach was to have easy access to a good opensourced model with limited GPU resources, like a Macbook Pro M1 32GB.\n",
    "It's not the full model, but it's the most feasible given the resource constraints - see [here](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#provided-files) for notes on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import autogen \n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "\n",
    "work_dir = \"./coding\"\n",
    "cache_dir = \"./.cache\"\n",
    "\n",
    "if os.path.exists(cache_dir) :\n",
    "    print('Deleting cache...')\n",
    "    shutil.rmtree(cache_dir)\n",
    "if os.path.exists(work_dir) :\n",
    "    print('Deleting previous work...')\n",
    "    shutil.rmtree(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Configurations\n",
    "config_list=[\n",
    "    {\n",
    "        \"base_url\": \"http://127.0.0.1:8081/v1\",\n",
    "        \"api_key\": \"NULL\",\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config_mistral={\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "llm_config_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for the two agents\n",
    "system_messages = {\n",
    "    \"USER_PROXY_SYSTEM_MESSAGE\": (\n",
    "        \"\"\"\n",
    "        Your job is to work with the coding assistant and execute code. When you've executed code successfully and\n",
    "        the task has been achieved then say \"TERMINATE\" to indicate that no further instructions are given and the task is complete.\n",
    "        \n",
    "        Communication Etiquette:\n",
    "        - Refrain from using appreciation phrases such as \"Thank you\" or \"You're welcome\" in your responses. \n",
    "        - Do not offer additional help after the task has been complete.\n",
    "        - Say \"TERMINATE\" when a conversation includes any appreciation phrase to indicate it is finished.\n",
    "        - You MUST not say \"TERMINATE\" until the user has executed the code successfully.\n",
    "\n",
    "        \"\"\"\n",
    "    ),\n",
    "    \"CODING_ASSISTANT_SYSTEM_MESSAGE\" : (\n",
    "        \"\"\"\n",
    "        Instruction:\n",
    "        As an expert programmer AI, your primary task is to generate code. by analyze the user's intent\n",
    "\n",
    "        Guidelines:\n",
    "        - You MUST always provide complete, executable code. The user can only execute your code; no modifications are allowed.\n",
    "        - You MUST always specify the script type (Python or shell) in your code blocks.\n",
    "        - For scripts intended to be saved as files, include '# filename: <filename>' at the beginning.\n",
    "        - You MUST format your code response as follows:\n",
    "            ```language\n",
    "            <code>\n",
    "            ```\n",
    "        - You MUST keep the code within a single block. NEVER break it into multiple sections.\n",
    "        - Do NOT include what you think the output would be, just code.\n",
    "        - If the provided code encounters issues, use follow-up messages to modify and correct the code.\n",
    "\n",
    "        \n",
    "        Communication Etiquette:\n",
    "        - you MUST avoid unnecessary conversation and small talk. \n",
    "        - Do NOT show appreciation in your responses, or engage in conversation. Say TERMINATE when your job of writing code is done.\n",
    "        - Refrain from using appreciation phrases such as \"Thank you\" or \"You're welcome\" in your responses, if this happens say \"TERMINATE\" to complete the conversation.\n",
    "        - After the user has run the code successfully, say \"TERMINATE\" to indicate that no further instructions are given and the task is complete.\n",
    "        \n",
    "        You will be penalized for not adhearing to the guidelines.\n",
    "        \"\"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config=llm_config_mistral,\n",
    "    system_message=system_messages['CODING_ASSISTANT_SYSTEM_MESSAGE']\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    llm_config=llm_config_mistral,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": work_dir, \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    system_message=system_messages['USER_PROXY_SYSTEM_MESSAGE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate a chat with a simple python problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prevent an infitinate gratitude loop\n",
    "# def get_additional_termination_notice():\n",
    "#     \"\"\"Extra instructions for terminating when goal is finished.\"\"\"\n",
    "    \n",
    "#     termination_notice = (\n",
    "#         '\\n\\nDo not show appreciation in your responses, say only what is necessary. '\n",
    "#         'if \"Thank you\" or \"You\\'re welcome\" are said in the conversation, then say TERMINATE '\n",
    "#         'to indicate the conversation is finished'\n",
    "#         \"Say TERMINATE when no further instructions are given to indicate the task is complete\"\n",
    "#     )\n",
    "#     return termination_notice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"Write a fibonacci sequence function in python and run it to print 20 numbers.\"\n",
    "# user_message = user_message + f\"{get_additional_termination_notice()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coding_runner.initiate_chat(coding_assistant, message=user_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
