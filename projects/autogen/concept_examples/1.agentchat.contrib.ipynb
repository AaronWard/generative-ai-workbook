{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentchat Module\n",
    "\n",
    "AutoGen offers several classes allowing developers to work with LLMs and solve tasks, particularly in the domain of mathematics and retrieval-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models to use:  ['gpt-4', 'gpt-3.5-turbo']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import openai\n",
    "import autogen\n",
    "import tempfile\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.api_utils import load_config_list_from_dotenv\n",
    "\n",
    "config_list = load_config_list_from_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "retrieve_assistant_agent\n",
    "The RetrieveAssistantAgent belongs to the agentchat.contrib.retrieve_assistant_agent module and is designed to solve tasks with LLMs, including suggesting Python code blocks and debugging. This agent does not execute code by default and expects the user to execute the code. This class is a subclass of AssistantAgent, configured with a default system message to solve tasks with LLMs.\n",
    "\n",
    "Purpose: The RetrieveAssistantAgent is a specialized agent designed to solve tasks with Language Learning Models (LLMs) like GPT. It is configured with a default system message and is specialized in suggesting Python code blocks and assisting with debugging. By default, human_input_mode is set to \"NEVER\", and code_execution_config is set to False, meaning this agent does not execute code by default and expects the user to execute the code. It is particularly useful when the user needs suggestions or guidance in writing or debugging Python code, but the execution of the code is left to the user.\n",
    "\n",
    "retrieve_user_proxy_agent\n",
    "The RetrieveUserProxyAgent class is part of the agentchat.contrib.retrieve_user_proxy_agent module and seems to be designed to interact with users, asking for human inputs every time a message is received based on its configuration. It also allows the generation of initial messages with given problems and prompts and can be configured with various parameters like the task type, client, docs_path, collection_name, and model to use for the retrieve chat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve_assistant_agent\n",
    "\n",
    "The `RetrieveAssistantAgent` belongs to the `agentchat.contrib.retrieve_assistant_agent` module and is designed to solve tasks with LLMs, including suggesting Python code blocks and debugging. This agent does not execute code by default and expects the user to execute the code. This class is a subclass of AssistantAgent, configured with a default system message to solve tasks with LLMs.\n",
    "\n",
    "**Purpose:**\n",
    "The RetrieveAssistantAgent is a specialized agent designed to solve tasks with Language Learning Models (LLMs) like GPT. It is configured with a default system message and is specialized in suggesting Python code blocks and assisting with debugging. By default, human_input_mode is set to \"NEVER\", and `code_execution_config` is set to False, meaning this agent does not execute code by default and expects the user to execute the code. It is particularly useful when the user needs suggestions or guidance in writing or debugging Python code, but the execution of the code is left to the user.\n",
    "\n",
    "\n",
    "#### retrieve_user_proxy_agent\n",
    "The `RetrieveUserProxyAgent` class is part of the `agentchat.contrib.retrieve_user_proxy_agent` module and seems to be designed to interact with users, asking for human inputs every time a message is received based on its configuration. It also allows the generation of initial messages with given problems and prompts and can be configured with various parameters like the task type, client, docs_path, collection_name, and model to use for the retrieve chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Dict, Tuple\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import glob\n",
    "import tiktoken\n",
    "import chromadb\n",
    "from chromadb.api import API\n",
    "import chromadb.utils.embedding_functions as ef\n",
    "import logging\n",
    "import PyPDF2\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "TEXT_FORMATS = [\"txt\", \"json\", \"csv\", \"tsv\", \"md\", \"html\", \"htm\", \"rtf\", \"rst\", \"jsonl\", \"log\", \"xml\", \"yaml\", \"yml\", \"pdf\"]\n",
    "\n",
    "\n",
    "def num_tokens_from_text(\n",
    "    text: str, model: str = \"gpt-3.5-turbo-0613\", return_tokens_per_name_and_message: bool = False\n",
    ") -> Union[int, Tuple[int, int, int]]:\n",
    "    \"\"\"Return the number of tokens used by a text.\"\"\"\n",
    "    # https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        logger.debug(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model in {\n",
    "        \"gpt-3.5-turbo-0613\",\n",
    "        \"gpt-3.5-turbo-16k-0613\",\n",
    "        \"gpt-4-0314\",\n",
    "        \"gpt-4-32k-0314\",\n",
    "        \"gpt-4-0613\",\n",
    "        \"gpt-4-32k-0613\",\n",
    "    }:\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif \"gpt-3.5-turbo\" in model or \"gpt-35-turbo\" in model:\n",
    "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613.\")\n",
    "        return num_tokens_from_text(text, model=\"gpt-3.5-turbo-0613\")\n",
    "    elif \"gpt-4\" in model:\n",
    "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
    "        return num_tokens_from_text(text, model=\"gpt-4-0613\")\n",
    "    else:\n",
    "        raise NotImplementedError(\n",
    "            f\"\"\"num_tokens_from_text() is not implemented for model {model}. See \"\"\"\n",
    "            f\"\"\"https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are \"\"\"\n",
    "            f\"\"\"converted to tokens.\"\"\"\n",
    "        )\n",
    "    if return_tokens_per_name_and_message:\n",
    "        return len(encoding.encode(text)), tokens_per_message, tokens_per_name\n",
    "    else:\n",
    "        return len(encoding.encode(text))\n",
    "\n",
    "\n",
    "def num_tokens_from_messages(messages: dict, model: str = \"gpt-3.5-turbo-0613\"):\n",
    "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        for key, value in message.items():\n",
    "            _num_tokens, tokens_per_message, tokens_per_name = num_tokens_from_text(\n",
    "                value, model=model, return_tokens_per_name_and_message=True\n",
    "            )\n",
    "            num_tokens += _num_tokens\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "        num_tokens += tokens_per_message\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "def split_text_to_chunks(\n",
    "    text: str,\n",
    "    max_tokens: int = 4000,\n",
    "    chunk_mode: str = \"multi_lines\",\n",
    "    must_break_at_empty_line: bool = True,\n",
    "    overlap: int = 10,\n",
    "):\n",
    "    \"\"\"Split a long text into chunks of max_tokens.\"\"\"\n",
    "    assert chunk_mode in {\"one_line\", \"multi_lines\"}\n",
    "    if chunk_mode == \"one_line\":\n",
    "        must_break_at_empty_line = False\n",
    "    chunks = []\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines_tokens = [num_tokens_from_text(line) for line in lines]\n",
    "    sum_tokens = sum(lines_tokens)\n",
    "    while sum_tokens > max_tokens:\n",
    "        if chunk_mode == \"one_line\":\n",
    "            estimated_line_cut = 2\n",
    "        else:\n",
    "            estimated_line_cut = int(max_tokens / sum_tokens * len(lines)) + 1\n",
    "        cnt = 0\n",
    "        prev = \"\"\n",
    "        for cnt in reversed(range(estimated_line_cut)):\n",
    "            if must_break_at_empty_line and lines[cnt].strip() != \"\":\n",
    "                continue\n",
    "            if sum(lines_tokens[:cnt]) <= max_tokens:\n",
    "                prev = \"\\n\".join(lines[:cnt])\n",
    "                break\n",
    "        if cnt == 0:\n",
    "            logger.warning(\n",
    "                f\"max_tokens is too small to fit a single line of text. Breaking this line:\\n\\t{lines[0][:100]} ...\"\n",
    "            )\n",
    "            if not must_break_at_empty_line:\n",
    "                split_len = int(max_tokens / lines_tokens[0] * 0.9 * len(lines[0]))\n",
    "                prev = lines[0][:split_len]\n",
    "                lines[0] = lines[0][split_len:]\n",
    "                lines_tokens[0] = num_tokens_from_text(lines[0])\n",
    "            else:\n",
    "                logger.warning(\"Failed to split docs with must_break_at_empty_line being True, set to False.\")\n",
    "                must_break_at_empty_line = False\n",
    "        chunks.append(prev) if len(prev) > 10 else None  # don't add chunks less than 10 characters\n",
    "        lines = lines[cnt:]\n",
    "        lines_tokens = lines_tokens[cnt:]\n",
    "        sum_tokens = sum(lines_tokens)\n",
    "    text_to_chunk = \"\\n\".join(lines)\n",
    "    chunks.append(text_to_chunk) if len(text_to_chunk) > 10 else None  # don't add chunks less than 10 characters\n",
    "    return chunks\n",
    "\n",
    "def extract_text_from_pdf(file: str) -> str:\n",
    "    \"\"\"Extract text from PDF files\"\"\"\n",
    "    text = \"\"\n",
    "    with open(file, \"rb\") as f:  \n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        if reader.is_encrypted:  # Check if the PDF is encrypted\n",
    "            try:\n",
    "                reader.decrypt('')\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not decrypt PDF {file}, {e}\")\n",
    "                return text  # Return empty text if PDF could not be decrypted\n",
    "                \n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "            \n",
    "    if not text.strip():  # Debugging line to check if text is empty\n",
    "        logger.warning(f\"Could not decrypt PDF {file}\")\n",
    "        \n",
    "    return text\n",
    "\n",
    "\n",
    "def split_files_to_chunks(\n",
    "        files: list, max_tokens: int = 4000, chunk_mode: str = \"multi_lines\", must_break_at_empty_line: bool = True\n",
    "    ):\n",
    "    chunks = []\n",
    "    \n",
    "    for file in files:\n",
    "        _, file_extension = os.path.splitext(file)\n",
    "        file_extension = file_extension.lower()\n",
    "        \n",
    "        if file_extension == \".pdf\":\n",
    "            text = extract_text_from_pdf(file)\n",
    "        else:  # For non-PDF text-based files\n",
    "            with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "                text = f.read()\n",
    "                \n",
    "        if not text.strip():  # Debugging line to check if text is empty after reading\n",
    "            logger.warning(f\"No text available in file: {file}\")\n",
    "            continue  # Skip to the next file if no text is available\n",
    "                \n",
    "        chunks += split_text_to_chunks(text, max_tokens, chunk_mode, must_break_at_empty_line)\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_files_from_dir(dir_path: str, types: list = TEXT_FORMATS, recursive: bool = True):\n",
    "    \"\"\"Return a list of all the files in a given directory.\"\"\"\n",
    "    if len(types) == 0:\n",
    "        raise ValueError(\"types cannot be empty.\")\n",
    "    types = [t[1:].lower() if t.startswith(\".\") else t.lower() for t in set(types)]\n",
    "    types += [t.upper() for t in types]\n",
    "\n",
    "    # If the path is a file, return it\n",
    "    if os.path.isfile(dir_path):\n",
    "        return [dir_path]\n",
    "\n",
    "    # If the path is a url, download it and return the downloaded file\n",
    "    if is_url(dir_path):\n",
    "        return [get_file_from_url(dir_path)]\n",
    "\n",
    "    files = []\n",
    "    if os.path.exists(dir_path):\n",
    "        for type in types:\n",
    "            files += glob.glob(os.path.join(dir_path, f\"**/*.{type}\"), recursive=recursive)\n",
    "    else:\n",
    "        logger.error(f\"Directory {dir_path} does not exist.\")\n",
    "        raise ValueError(f\"Directory {dir_path} does not exist.\")\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_file_from_url(url: str, save_path: str = None):\n",
    "    \"\"\"Download a file from a URL.\"\"\"\n",
    "    if save_path is None:\n",
    "        save_path = os.path.join(\"/tmp/chromadb\", os.path.basename(url))\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "    return save_path\n",
    "\n",
    "\n",
    "def is_url(string: str):\n",
    "    \"\"\"Return True if the string is a valid URL.\"\"\"\n",
    "    try:\n",
    "        result = urlparse(string)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def create_vector_db_from_dir(\n",
    "    dir_path: str,\n",
    "    max_tokens: int = 4000,\n",
    "    client: API = None,\n",
    "    db_path: str = \"/tmp/chromadb.db\",\n",
    "    collection_name: str = \"all-my-documents\",\n",
    "    get_or_create: bool = False,\n",
    "    chunk_mode: str = \"multi_lines\",\n",
    "    must_break_at_empty_line: bool = True,\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "):\n",
    "    \"\"\"Create a vector db from all the files in a given directory.\"\"\"\n",
    "    if client is None:\n",
    "        client = chromadb.PersistentClient(path=db_path)\n",
    "    try:\n",
    "        embedding_function = ef.SentenceTransformerEmbeddingFunction(embedding_model)\n",
    "        collection = client.create_collection(\n",
    "            collection_name,\n",
    "            get_or_create=get_or_create,\n",
    "            embedding_function=embedding_function,\n",
    "            metadata={\"hnsw:space\": \"ip\", \"hnsw:construction_ef\": 30, \"hnsw:M\": 32},  # ip, l2, cosine\n",
    "        )\n",
    "\n",
    "        chunks = split_files_to_chunks(get_files_from_dir(dir_path), max_tokens, chunk_mode, must_break_at_empty_line)\n",
    "        \n",
    "        # Upsert in batch of 40000 or less if the total number of chunks is less than 40000\n",
    "        for i in range(0, len(chunks), min(40000, len(chunks))):\n",
    "            end_idx = i + min(40000, len(chunks) - i)\n",
    "            collection.upsert(\n",
    "                documents=chunks[i:end_idx],\n",
    "                ids=[f\"doc_{j}\" for j in range(i, end_idx)],  # unique for each doc\n",
    "            )\n",
    "    except ValueError as e:\n",
    "        logger.warning(f\"{e}\")\n",
    "\n",
    "\n",
    "def query_vector_db(\n",
    "    query_texts: List[str],\n",
    "    n_results: int = 10,\n",
    "    client: API = None,\n",
    "    db_path: str = \"/tmp/chromadb.db\",\n",
    "    collection_name: str = \"all-my-documents\",\n",
    "    search_string: str = \"\",\n",
    "    embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"Query a vector db.\"\"\"\n",
    "    if client is None:\n",
    "        client = chromadb.PersistentClient(path=db_path)\n",
    "    # the collection's embedding function is always the default one, but we want to use the one we used to create the\n",
    "    # collection. So we compute the embeddings ourselves and pass it to the query function.\n",
    "    collection = client.get_collection(collection_name)\n",
    "    embedding_function = ef.SentenceTransformerEmbeddingFunction(embedding_model)\n",
    "    query_embeddings = embedding_function(query_texts)\n",
    "    # Query/search n most similar results. You can also .get by id\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embeddings,\n",
    "        n_results=n_results,\n",
    "        where_document={\"$contains\": search_string} if search_string else None,  # optional filter\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import chromadb\n",
    "from autogen.agentchat.agent import Agent\n",
    "from autogen.agentchat import UserProxyAgent\n",
    "# from autogen.retrieve_utils import create_vector_db_from_dir, query_vector_db, num_tokens_from_text\n",
    "from autogen.code_utils import extract_code\n",
    "\n",
    "from typing import Callable, Dict, Optional, Union, List, Tuple, Any\n",
    "from IPython import get_ipython\n",
    "\n",
    "try:\n",
    "    from termcolor import colored\n",
    "except ImportError:\n",
    "\n",
    "    def colored(x, *args, **kwargs):\n",
    "        return x\n",
    "\n",
    "\n",
    "PROMPT_DEFAULT = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
    "context provided by the user. You should follow the following steps to answer a question:\n",
    "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
    "a question answering task.\n",
    "Step 2, you reply based on the intent.\n",
    "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
    "If user's intent is code generation, you must obey the following rules:\n",
    "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
    "Rule 2. You must follow the formats below to write your code:\n",
    "```language\n",
    "# your code\n",
    "```\n",
    "\n",
    "If user's intent is question answering, you must give as short an answer as possible.\n",
    "\n",
    "User's question is: {input_question}\n",
    "\n",
    "Context is: {input_context}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_CODE = \"\"\"You're a retrieve augmented coding assistant. You answer user's questions based on your own knowledge and the\n",
    "context provided by the user.\n",
    "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
    "For code generation, you must obey the following rules:\n",
    "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
    "Rule 2. You must follow the formats below to write your code:\n",
    "```language\n",
    "# your code\n",
    "```\n",
    "\n",
    "User's question is: {input_question}\n",
    "\n",
    "Context is: {input_context}\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_QA = \"\"\"You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
    "context provided by the user.\n",
    "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
    "You must give as short an answer as possible.\n",
    "\n",
    "User's question is: {input_question}\n",
    "\n",
    "Context is: {input_context}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _is_termination_msg_retrievechat(message):\n",
    "    \"\"\"Check if a message is a termination message.\"\"\"\n",
    "    if isinstance(message, dict):\n",
    "        message = message.get(\"content\")\n",
    "        if message is None:\n",
    "            return False\n",
    "    cb = extract_code(message)\n",
    "    contain_code = False\n",
    "    for c in cb:\n",
    "        if c[0] == \"python\":\n",
    "            contain_code = True\n",
    "            break\n",
    "    return not contain_code\n",
    "\n",
    "\n",
    "class RetrieveUserProxyAgent(UserProxyAgent):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name=\"RetrieveChatAgent\",  # default set to RetrieveChatAgent\n",
    "        is_termination_msg: Optional[Callable[[Dict], bool]] = _is_termination_msg_retrievechat,\n",
    "        human_input_mode: Optional[str] = \"ALWAYS\",\n",
    "        retrieve_config: Optional[Dict] = None,  # config for the retrieve agent\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            name (str): name of the agent.\n",
    "            human_input_mode (str): whether to ask for human inputs every time a message is received.\n",
    "                Possible values are \"ALWAYS\", \"TERMINATE\", \"NEVER\".\n",
    "                (1) When \"ALWAYS\", the agent prompts for human input every time a message is received.\n",
    "                    Under this mode, the conversation stops when the human input is \"exit\",\n",
    "                    or when is_termination_msg is True and there is no human input.\n",
    "                (2) When \"TERMINATE\", the agent only prompts for human input only when a termination message is received or\n",
    "                    the number of auto reply reaches the max_consecutive_auto_reply.\n",
    "                (3) When \"NEVER\", the agent will never prompt for human input. Under this mode, the conversation stops\n",
    "                    when the number of auto reply reaches the max_consecutive_auto_reply or when is_termination_msg is True.\n",
    "            retrieve_config (dict or None): config for the retrieve agent.\n",
    "                To use default config, set to None. Otherwise, set to a dictionary with the following keys:\n",
    "                - task (Optional, str): the task of the retrieve chat. Possible values are \"code\", \"qa\" and \"default\". System\n",
    "                    prompt will be different for different tasks. The default value is `default`, which supports both code and qa.\n",
    "                - client (Optional, chromadb.Client): the chromadb client.\n",
    "                    If key not provided, a default client `chromadb.Client()` will be used.\n",
    "                - docs_path (Optional, str): the path to the docs directory. It can also be the path to a single file,\n",
    "                    or the url to a single file. If key not provided, a default path `./docs` will be used.\n",
    "                - collection_name (Optional, str): the name of the collection.\n",
    "                    If key not provided, a default name `autogen-docs` will be used.\n",
    "                - model (Optional, str): the model to use for the retrieve chat.\n",
    "                    If key not provided, a default model `gpt-4` will be used.\n",
    "                - chunk_token_size (Optional, int): the chunk token size for the retrieve chat.\n",
    "                    If key not provided, a default size `max_tokens * 0.4` will be used.\n",
    "                - context_max_tokens (Optional, int): the context max token size for the retrieve chat.\n",
    "                    If key not provided, a default size `max_tokens * 0.8` will be used.\n",
    "                - chunk_mode (Optional, str): the chunk mode for the retrieve chat. Possible values are\n",
    "                    \"multi_lines\" and \"one_line\". If key not provided, a default mode `multi_lines` will be used.\n",
    "                - must_break_at_empty_line (Optional, bool): chunk will only break at empty line if True. Default is True.\n",
    "                    If chunk_mode is \"one_line\", this parameter will be ignored.\n",
    "                - embedding_model (Optional, str): the embedding model to use for the retrieve chat.\n",
    "                    If key not provided, a default model `all-MiniLM-L6-v2` will be used. All available models\n",
    "                    can be found at `https://www.sbert.net/docs/pretrained_models.html`. The default model is a\n",
    "                    fast model. If you want to use a high performance model, `all-mpnet-base-v2` is recommended.\n",
    "                - customized_prompt (Optional, str): the customized prompt for the retrieve chat. Default is None.\n",
    "                - customized_answer_prefix (Optional, str): the customized answer prefix for the retrieve chat. Default is \"\".\n",
    "                    If not \"\" and the customized_answer_prefix is not in the answer, `Update Context` will be triggered.\n",
    "                - no_update_context (Optional, bool): if True, will not apply `Update Context` for interactive retrieval. Default is False.\n",
    "            **kwargs (dict): other kwargs in [UserProxyAgent](../user_proxy_agent#__init__).\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            name=name,\n",
    "            is_termination_msg=is_termination_msg,\n",
    "            human_input_mode=human_input_mode,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._retrieve_config = {} if retrieve_config is None else retrieve_config\n",
    "        self._task = self._retrieve_config.get(\"task\", \"default\")\n",
    "        self._client = self._retrieve_config.get(\"client\", chromadb.Client())\n",
    "        self._docs_path = self._retrieve_config.get(\"docs_path\", \"./docs\")\n",
    "        self._collection_name = self._retrieve_config.get(\"collection_name\", \"autogen-docs\")\n",
    "        self._model = self._retrieve_config.get(\"model\", \"gpt-4\")\n",
    "        self._max_tokens = self.get_max_tokens(self._model)\n",
    "        self._chunk_token_size = int(self._retrieve_config.get(\"chunk_token_size\", self._max_tokens * 0.4))\n",
    "        self._chunk_mode = self._retrieve_config.get(\"chunk_mode\", \"multi_lines\")\n",
    "        self._must_break_at_empty_line = self._retrieve_config.get(\"must_break_at_empty_line\", True)\n",
    "        self._embedding_model = self._retrieve_config.get(\"embedding_model\", \"all-MiniLM-L6-v2\")\n",
    "        self.customized_prompt = self._retrieve_config.get(\"customized_prompt\", None)\n",
    "        self.customized_answer_prefix = self._retrieve_config.get(\"customized_answer_prefix\", \"\").upper()\n",
    "        self.no_update_context = self._retrieve_config.get(\"no_update_context\", False)\n",
    "        self._context_max_tokens = self._max_tokens * 0.8\n",
    "        self._collection = False  # the collection is not created\n",
    "        self._ipython = get_ipython()\n",
    "        self._doc_idx = -1  # the index of the current used doc\n",
    "        self._results = {}  # the results of the current query\n",
    "        self._intermediate_answers = set()  # the intermediate answers\n",
    "        self._doc_contents = []  # the contents of the current used doc\n",
    "        self._doc_ids = []  # the ids of the current used doc\n",
    "        self.register_reply(Agent, RetrieveUserProxyAgent._generate_retrieve_user_reply)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_tokens(model=\"gpt-3.5-turbo\"):\n",
    "        if \"32k\" in model:\n",
    "            return 32000\n",
    "        elif \"16k\" in model:\n",
    "            return 16000\n",
    "        elif \"gpt-4\" in model:\n",
    "            return 8000\n",
    "        else:\n",
    "            return 4000\n",
    "\n",
    "    def _reset(self, intermediate=False):\n",
    "        self._doc_idx = -1  # the index of the current used doc\n",
    "        self._results = {}  # the results of the current query\n",
    "        if not intermediate:\n",
    "            self._intermediate_answers = set()  # the intermediate answers\n",
    "            self._doc_contents = []  # the contents of the current used doc\n",
    "            self._doc_ids = []  # the ids of the current used doc\n",
    "\n",
    "    def _get_context(self, results):\n",
    "        doc_contents = \"\"\n",
    "        current_tokens = 0\n",
    "        _doc_idx = self._doc_idx\n",
    "        _tmp_retrieve_count = 0\n",
    "        for idx, doc in enumerate(results[\"documents\"][0]):\n",
    "            if idx <= _doc_idx:\n",
    "                continue\n",
    "            if results[\"ids\"][0][idx] in self._doc_ids:\n",
    "                continue\n",
    "            _doc_tokens = num_tokens_from_text(doc)\n",
    "            if _doc_tokens > self._context_max_tokens:\n",
    "                func_print = f\"Skip doc_id {results['ids'][0][idx]} as it is too long to fit in the context.\"\n",
    "                print(colored(func_print, \"green\"), flush=True)\n",
    "                self._doc_idx = idx\n",
    "                continue\n",
    "            if current_tokens + _doc_tokens > self._context_max_tokens:\n",
    "                break\n",
    "            func_print = f\"Adding doc_id {results['ids'][0][idx]} to context.\"\n",
    "            print(colored(func_print, \"green\"), flush=True)\n",
    "            current_tokens += _doc_tokens\n",
    "            doc_contents += doc + \"\\n\"\n",
    "            self._doc_idx = idx\n",
    "            self._doc_ids.append(results[\"ids\"][0][idx])\n",
    "            self._doc_contents.append(doc)\n",
    "            _tmp_retrieve_count += 1\n",
    "            if _tmp_retrieve_count >= self.n_results:\n",
    "                break\n",
    "        return doc_contents\n",
    "\n",
    "    def _generate_message(self, doc_contents, task=\"default\"):\n",
    "        if not doc_contents:\n",
    "            print(colored(\"No more context, will terminate.\", \"green\"), flush=True)\n",
    "            return \"TERMINATE\"\n",
    "        if self.customized_prompt:\n",
    "            message = self.customized_prompt.format(input_question=self.problem, input_context=doc_contents)\n",
    "        elif task.upper() == \"CODE\":\n",
    "            message = PROMPT_CODE.format(input_question=self.problem, input_context=doc_contents)\n",
    "        elif task.upper() == \"QA\":\n",
    "            message = PROMPT_QA.format(input_question=self.problem, input_context=doc_contents)\n",
    "        elif task.upper() == \"DEFAULT\":\n",
    "            message = PROMPT_DEFAULT.format(input_question=self.problem, input_context=doc_contents)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"task {task} is not implemented.\")\n",
    "        return message\n",
    "\n",
    "    def _generate_retrieve_user_reply(\n",
    "        self,\n",
    "        messages: Optional[List[Dict]] = None,\n",
    "        sender: Optional[Agent] = None,\n",
    "        config: Optional[Any] = None,\n",
    "    ) -> Tuple[bool, Union[str, Dict, None]]:\n",
    "        \"\"\"In this function, we will update the context and reset the conversation based on different conditions.\n",
    "        We'll update the context and reset the conversation if no_update_context is False and either of the following:\n",
    "        (1) the last message contains \"UPDATE CONTEXT\",\n",
    "        (2) the last message doesn't contain \"UPDATE CONTEXT\" and the customized_answer_prefix is not in the message.\n",
    "        \"\"\"\n",
    "        if config is None:\n",
    "            config = self\n",
    "        if messages is None:\n",
    "            messages = self._oai_messages[sender]\n",
    "        message = messages[-1]\n",
    "        update_context_case1 = (\n",
    "            \"UPDATE CONTEXT\" in message.get(\"content\", \"\")[-20:].upper()\n",
    "            or \"UPDATE CONTEXT\" in message.get(\"content\", \"\")[:20].upper()\n",
    "        )\n",
    "        update_context_case2 = (\n",
    "            self.customized_answer_prefix and self.customized_answer_prefix not in message.get(\"content\", \"\").upper()\n",
    "        )\n",
    "        if (update_context_case1 or update_context_case2) and not self.no_update_context:\n",
    "            print(colored(\"Updating context and resetting conversation.\", \"green\"), flush=True)\n",
    "            # extract the first sentence in the response as the intermediate answer\n",
    "            _message = message.get(\"content\", \"\").split(\"\\n\")[0].strip()\n",
    "            _intermediate_info = re.split(r\"(?<=[.!?])\\s+\", _message)\n",
    "            self._intermediate_answers.add(_intermediate_info[0])\n",
    "\n",
    "            if update_context_case1:\n",
    "                # try to get more context from the current retrieved doc results because the results may be too long to fit\n",
    "                # in the LLM context.\n",
    "                doc_contents = self._get_context(self._results)\n",
    "\n",
    "                # Always use self.problem as the query text to retrieve docs, but each time we replace the context with the\n",
    "                # next similar docs in the retrieved doc results.\n",
    "                if not doc_contents:\n",
    "                    for _tmp_retrieve_count in range(1, 5):\n",
    "                        self._reset(intermediate=True)\n",
    "                        self.retrieve_docs(self.problem, self.n_results * (2 * _tmp_retrieve_count + 1))\n",
    "                        doc_contents = self._get_context(self._results)\n",
    "                        if doc_contents:\n",
    "                            break\n",
    "            elif update_context_case2:\n",
    "                # Use the current intermediate info as the query text to retrieve docs, and each time we append the top similar\n",
    "                # docs in the retrieved doc results to the context.\n",
    "                for _tmp_retrieve_count in range(5):\n",
    "                    self._reset(intermediate=True)\n",
    "                    self.retrieve_docs(_intermediate_info[0], self.n_results * (2 * _tmp_retrieve_count + 1))\n",
    "                    self._get_context(self._results)\n",
    "                    doc_contents = \"\\n\".join(self._doc_contents)  # + \"\\n\" + \"\\n\".join(self._intermediate_answers)\n",
    "                    if doc_contents:\n",
    "                        break\n",
    "\n",
    "            self.clear_history()\n",
    "            sender.clear_history()\n",
    "            return True, self._generate_message(doc_contents, task=self._task)\n",
    "        else:\n",
    "            return False, None\n",
    "\n",
    "    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\"):\n",
    "        if not self._collection:\n",
    "            print(\"Trying to create collection.\")\n",
    "            create_vector_db_from_dir(\n",
    "                dir_path=self._docs_path,\n",
    "                max_tokens=self._chunk_token_size,\n",
    "                client=self._client,\n",
    "                collection_name=self._collection_name,\n",
    "                chunk_mode=self._chunk_mode,\n",
    "                must_break_at_empty_line=self._must_break_at_empty_line,\n",
    "                embedding_model=self._embedding_model,\n",
    "            )\n",
    "            self._collection = True\n",
    "\n",
    "        results = query_vector_db(\n",
    "            query_texts=[problem],\n",
    "            n_results=n_results,\n",
    "            search_string=search_string,\n",
    "            client=self._client,\n",
    "            collection_name=self._collection_name,\n",
    "            embedding_model=self._embedding_model,\n",
    "        )\n",
    "        self._results = results\n",
    "        print(\"doc_ids: \", results[\"ids\"])\n",
    "\n",
    "    def generate_init_message(self, problem: str, n_results: int = 20, search_string: str = \"\"):\n",
    "        \"\"\"Generate an initial message with the given problem and prompt.\n",
    "\n",
    "        Args:\n",
    "            problem (str): the problem to be solved.\n",
    "            n_results (int): the number of results to be retrieved.\n",
    "            search_string (str): only docs containing this string will be retrieved.\n",
    "\n",
    "        Returns:\n",
    "            str: the generated prompt ready to be sent to the assistant agent.\n",
    "        \"\"\"\n",
    "        self._reset()\n",
    "        self.retrieve_docs(problem, n_results, search_string)\n",
    "        self.problem = problem\n",
    "        self.n_results = n_results\n",
    "        doc_contents = self._get_context(self._results)\n",
    "        message = self._generate_message(doc_contents, self._task)\n",
    "        return message\n",
    "\n",
    "    def run_code(self, code, **kwargs):\n",
    "        lang = kwargs.get(\"lang\", None)\n",
    "        if code.startswith(\"!\") or code.startswith(\"pip\") or lang in [\"bash\", \"shell\", \"sh\"]:\n",
    "            return (\n",
    "                0,\n",
    "                \"You MUST NOT install any packages because all the packages needed are already installed.\",\n",
    "                None,\n",
    "            )\n",
    "        if self._ipython is None or lang != \"python\":\n",
    "            return super().run_code(code, **kwargs)\n",
    "        else:\n",
    "            result = self._ipython.run_cell(code)\n",
    "            log = str(result.result)\n",
    "            exitcode = 0 if result.success else 1\n",
    "            if result.error_before_exec is not None:\n",
    "                log += f\"\\n{result.error_before_exec}\"\n",
    "                exitcode = 1\n",
    "            if result.error_in_exec is not None:\n",
    "                log += f\"\\n{result.error_in_exec}\"\n",
    "                exitcode = 1\n",
    "            return exitcode, log, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "# from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "\n",
    "# Start logging\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# Create an instance of RetrieveAssistantAgent\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "path = Path(os.getcwd(), 'docs')\n",
    "str(path)\n",
    "\n",
    "client = chromadb.PersistentClient(path=f\"{os.getcwd()}/chromadb\")\n",
    "\n",
    "# Create an instance of RetrieveUserProxyAgent\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"default\",\n",
    "        \"docs_path\": str(path), \n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": client, \n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Delete dir each instantiation\n",
    "#  client.delete_collection('autogen-docs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ragproxyagent.retrieve_docs(\"some query string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From a txt document \n",
    "user_question = \"Who was the blacksmith?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=user_question) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### math_user_proxy_agent\n",
    "\n",
    "The `MathUserProxyAgent` is a part of the `agentchat.contrib.math_user_proxy_agent` module, and it is designed to handle math problems. It seems to be experimental at this stage. This class can generate initial messages, execute Python code, and run queries through Wolfram Alpha to solve mathematical problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
