{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TaskWeaver\n",
    "\n",
    "This notebook aims to condence all requirements needed for coding up a TaskWeaver project.  <br>\n",
    "https://export.arxiv.org/pdf/2311.17541 <br>\n",
    "\n",
    "#### Step 1:\n",
    "- Firstly, you need to make a project folder. this is a prerequisite to running TaskWeaver. See [starter_project](\"../_starter_projects/\") <br>\n",
    "\n",
    "#### Step 2:\n",
    "- Then we need to make a session:\n",
    "    > TaskWeaver supports a basic session management to keep different users' <br>\n",
    "    > data separate. The code execution is separated into different processes <br>\n",
    "    > in order not to interfere with each other.\n",
    "\n",
    "#### Step 3:\n",
    "- We also need to enable modules that the code interpreter can use and code verification.\n",
    "    > Code verification - TaskWeaver is designed to verify the generated code before execution. <br>\n",
    "    > It can detect potential issues in the generated code and provide suggestions to fix them.\n",
    "    \n",
    "#### Step 4:\n",
    "- TaskWeaver has restricted library use to mitigate against unforeseen consequences of running code, thus you need to call out the permitted libraries expicitly.\n",
    "- Add this to the `taskweaver_config.json`:\n",
    "    ```\n",
    "    \"code_verification.allowed_modules\": [\"pandas\", \"matplotlib\", \"numpy\", \"sklearn\", \"scipy\", \"seaborn\", \"datetime\", \"typing\"],\n",
    "    \"code_verification.code_verification_on\": true,\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Optional\n",
    "\n",
    "import pprint as pp\n",
    "from taskweaver.app.app import TaskWeaverApp\n",
    "\n",
    "# Create a session\n",
    "# would prefer if the expected folder structure was autogenerated\n",
    "app_dir = \"./_starter_projects/\"\n",
    "app = TaskWeaverApp(app_dir=app_dir)\n",
    "session = app.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_manager.store_type': AppConfigItem(name='session_manager.store_type', value='in_memory', type='enum', sources=[AppConfigSourceValue(source='default', value='in_memory')]),\n",
       " 'llm.api_type': AppConfigItem(name='llm.api_type', value='openai', type='str', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.embedding_api_type': AppConfigItem(name='llm.embedding_api_type', value='openai', type='str', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.api_base': AppConfigItem(name='llm.api_base', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.api_key': AppConfigItem(name='llm.api_key', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.model': AppConfigItem(name='llm.model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.backup_model': AppConfigItem(name='llm.backup_model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.embedding_model': AppConfigItem(name='llm.embedding_model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.response_format': AppConfigItem(name='llm.response_format', value='json_object', type='enum', sources=[AppConfigSourceValue(source='default', value='json_object')]),\n",
       " 'llm.openai.api_base': AppConfigItem(name='llm.openai.api_base', value='https://api.openai.com/v1', type='str', sources=[AppConfigSourceValue(source='default', value='https://api.openai.com/v1')]),\n",
       " 'llm.openai.api_key': AppConfigItem(name='llm.openai.api_key', value='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', type='str', sources=[AppConfigSourceValue(source='default', value='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')]),\n",
       " 'llm.openai.model': AppConfigItem(name='llm.openai.model', value='gpt-4-1106-preview', type='str', sources=[AppConfigSourceValue(source='default', value='gpt-4-1106-preview')]),\n",
       " 'llm.openai.backup_model': AppConfigItem(name='llm.openai.backup_model', value='gpt-4-1106-preview', type='str', sources=[AppConfigSourceValue(source='default', value='gpt-4-1106-preview')]),\n",
       " 'llm.openai.embedding_model': AppConfigItem(name='llm.openai.embedding_model', value='text-embedding-ada-002', type='str', sources=[AppConfigSourceValue(source='default', value='text-embedding-ada-002')]),\n",
       " 'llm.openai.api_version': AppConfigItem(name='llm.openai.api_version', value='2023-12-01-preview', type='str', sources=[AppConfigSourceValue(source='default', value='2023-12-01-preview')]),\n",
       " 'llm.openai.api_auth_type': AppConfigItem(name='llm.openai.api_auth_type', value='openai', type='enum', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.openai.aad_auth_mode': AppConfigItem(name='llm.openai.aad_auth_mode', value='device_login', type='enum', sources=[AppConfigSourceValue(source='default', value='device_login')]),\n",
       " 'llm.openai.aad_tenant_id': AppConfigItem(name='llm.openai.aad_tenant_id', value='common', type='str', sources=[AppConfigSourceValue(source='default', value='common')]),\n",
       " 'llm.openai.aad_api_resource': AppConfigItem(name='llm.openai.aad_api_resource', value='https://cognitiveservices.azure.com/', type='str', sources=[AppConfigSourceValue(source='default', value='https://cognitiveservices.azure.com/')]),\n",
       " 'llm.openai.aad_api_scope': AppConfigItem(name='llm.openai.aad_api_scope', value='.default', type='str', sources=[AppConfigSourceValue(source='default', value='.default')]),\n",
       " 'llm.openai.aad_client_id': AppConfigItem(name='llm.openai.aad_client_id', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'llm.openai.aad_client_secret': AppConfigItem(name='llm.openai.aad_client_secret', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'llm.openai.aad_use_token_cache': AppConfigItem(name='llm.openai.aad_use_token_cache', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'llm.openai.aad_token_cache_path': AppConfigItem(name='llm.openai.aad_token_cache_path', value='cache/token_cache.bin', type='str', sources=[AppConfigSourceValue(source='default', value='cache/token_cache.bin')]),\n",
       " 'llm.openai.stop_token': AppConfigItem(name='llm.openai.stop_token', value=['<EOS>'], type='list', sources=[AppConfigSourceValue(source='default', value=['<EOS>'])]),\n",
       " 'llm.openai.temperature': AppConfigItem(name='llm.openai.temperature', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.max_tokens': AppConfigItem(name='llm.openai.max_tokens', value=1024, type='int', sources=[AppConfigSourceValue(source='default', value=1024)]),\n",
       " 'llm.openai.top_p': AppConfigItem(name='llm.openai.top_p', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.frequency_penalty': AppConfigItem(name='llm.openai.frequency_penalty', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.presence_penalty': AppConfigItem(name='llm.openai.presence_penalty', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.seed': AppConfigItem(name='llm.openai.seed', value=123456, type='int', sources=[AppConfigSourceValue(source='default', value=123456)]),\n",
       " 'workspace.mode': AppConfigItem(name='workspace.mode', value='local', type='str', sources=[AppConfigSourceValue(source='default', value='local')]),\n",
       " 'workspace.workspace_path': AppConfigItem(name='workspace.workspace_path', value='${AppBaseDir}/workspace', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/workspace')]),\n",
       " 'logging.remote': AppConfigItem(name='logging.remote', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'logging.appinsights_connection_string': AppConfigItem(name='logging.appinsights_connection_string', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'logging.injector': AppConfigItem(name='logging.injector', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'logging.log_folder': AppConfigItem(name='logging.log_folder', value='logs', type='str', sources=[AppConfigSourceValue(source='default', value='logs')]),\n",
       " 'logging.log_file': AppConfigItem(name='logging.log_file', value='task_weaver.log', type='str', sources=[AppConfigSourceValue(source='default', value='task_weaver.log')]),\n",
       " 'session.use_planner': AppConfigItem(name='session.use_planner', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'session.max_internal_chat_round_num': AppConfigItem(name='session.max_internal_chat_round_num', value=10, type='int', sources=[AppConfigSourceValue(source='default', value=10)]),\n",
       " 'planner.use_example': AppConfigItem(name='planner.use_example', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'planner.prompt_file_path': AppConfigItem(name='planner.prompt_file_path', value='${ModuleBaseDir}/planner/planner_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/planner/planner_prompt.yaml')]),\n",
       " 'planner.example_base_path': AppConfigItem(name='planner.example_base_path', value='${AppBaseDir}/planner_examples', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/planner_examples')]),\n",
       " 'planner.prompt_compression': AppConfigItem(name='planner.prompt_compression', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'planner.compression_prompt_path': AppConfigItem(name='planner.compression_prompt_path', value='${ModuleBaseDir}/planner/compression_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/planner/compression_prompt.yaml')]),\n",
       " 'plugin.base_path': AppConfigItem(name='plugin.base_path', value='${AppBaseDir}/plugins', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/plugins')]),\n",
       " 'round_compressor.rounds_to_compress': AppConfigItem(name='round_compressor.rounds_to_compress', value=2, type='int', sources=[AppConfigSourceValue(source='default', value=2)]),\n",
       " 'round_compressor.rounds_to_retain': AppConfigItem(name='round_compressor.rounds_to_retain', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)]),\n",
       " 'execution_service.env_dir': AppConfigItem(name='execution_service.env_dir', value='${AppBaseDir}/env', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/env')]),\n",
       " 'code_generator.role_name': AppConfigItem(name='code_generator.role_name', value='ProgramApe', type='str', sources=[AppConfigSourceValue(source='default', value='ProgramApe')]),\n",
       " 'code_generator.executor_name': AppConfigItem(name='code_generator.executor_name', value='CodeExecutor', type='str', sources=[AppConfigSourceValue(source='default', value='CodeExecutor')]),\n",
       " 'code_generator.load_plugin': AppConfigItem(name='code_generator.load_plugin', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'code_generator.load_example': AppConfigItem(name='code_generator.load_example', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'code_generator.prompt_file_path': AppConfigItem(name='code_generator.prompt_file_path', value='${ModuleBaseDir}/code_interpreter/code_generator/code_generator_json_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/code_interpreter/code_generator/code_generator_json_prompt.yaml')]),\n",
       " 'code_generator.example_base_path': AppConfigItem(name='code_generator.example_base_path', value='${AppBaseDir}/codeinterpreter_examples', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/codeinterpreter_examples')]),\n",
       " 'code_generator.prompt_compression': AppConfigItem(name='code_generator.prompt_compression', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_generator.compression_prompt_path': AppConfigItem(name='code_generator.compression_prompt_path', value='${ModuleBaseDir}/code_interpreter/code_generator/compression_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/code_interpreter/code_generator/compression_prompt.yaml')]),\n",
       " 'code_generator.enable_auto_plugin_selection': AppConfigItem(name='code_generator.enable_auto_plugin_selection', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_generator.auto_plugin_selection_topk': AppConfigItem(name='code_generator.auto_plugin_selection_topk', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)]),\n",
       " 'code_verification.code_verification_on': AppConfigItem(name='code_verification.code_verification_on', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_verification.plugin_only': AppConfigItem(name='code_verification.plugin_only', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_verification.allowed_modules': AppConfigItem(name='code_verification.allowed_modules', value=['pandas', 'matplotlib', 'numpy', 'sklearn', 'scipy', 'seaborn', 'datetime', 'typing'], type='list', sources=[AppConfigSourceValue(source='default', value=['pandas', 'matplotlib', 'numpy', 'sklearn', 'scipy', 'seaborn', 'datetime', 'typing'])]),\n",
       " 'code_interpreter.use_local_uri': AppConfigItem(name='code_interpreter.use_local_uri', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_interpreter.max_retry_count': AppConfigItem(name='code_interpreter.max_retry_count', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the configurations from taskweaver_config.json are stored\n",
    "# https://github.com/microsoft/TaskWeaver/blob/main/docs/configuration.md \n",
    "# Categories of keys within the config\n",
    "# - session_manager\n",
    "# - llm\n",
    "# - workspace\n",
    "# - logging\n",
    "# - session\n",
    "# - planner\n",
    "# - plugin\n",
    "# - round_compressor\n",
    "# - code_generator\n",
    "# - embedding_model\n",
    "# - code_verification\n",
    "# - code_interpreter\n",
    "session.config.src.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_plan:\n",
      "1. Provide information about the capabilities and services offered.\n",
      "plan:\n",
      "1. Provide information about the capabilities and services offered.\n",
      "current_plan_step:\n",
      "Providing information about the capabilities and services offered.\n",
      "send_to:\n",
      "User\n",
      "message:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute them. This includes tasks like data analysis, file operations, and generating Python code to accomplish specific objectives. How can I assist you today?\n",
      "Planner->User:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute them. This includes tasks like data analysis, file operations, and generating Python code to accomplish specific objectives. How can I assist you today?\n",
      "final_reply_message:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute them. This includes tasks like data analysis, file operations, and generating Python code to accomplish specific objectives. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Send a message to the session\n",
    "user_query = \"hello, what can you do?\"\n",
    "response_round = session.send_message(\n",
    "                            user_query,\n",
    "                            event_handler=lambda _type, _msg: print(f\"{_type}:\\n{_msg}\")\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute them. This includes tasks like data analysis, file operations, and generating Python code to accomplish specific objectives. How can I assist you today?'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_round.to_dict()['post_list'][-1]['message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'round-20231214-223036-7696666b',\n",
      " 'post_list': [{'attachment_list': [],\n",
      "                'id': 'post-20231214-223036-a4519d69',\n",
      "                'message': 'hello, what can you do?',\n",
      "                'send_from': 'User',\n",
      "                'send_to': 'Planner'},\n",
      "               {'attachment_list': [{'content': '1. Provide information about '\n",
      "                                                'the capabilities and services '\n",
      "                                                'offered.',\n",
      "                                     'id': 'atta-20231214-223046-7154b8b1',\n",
      "                                     'type': 'init_plan'},\n",
      "                                    {'content': '1. Provide information about '\n",
      "                                                'the capabilities and services '\n",
      "                                                'offered.',\n",
      "                                     'id': 'atta-20231214-223046-a1f1944c',\n",
      "                                     'type': 'plan'},\n",
      "                                    {'content': 'Providing information about '\n",
      "                                                'the capabilities and services '\n",
      "                                                'offered.',\n",
      "                                     'id': 'atta-20231214-223046-dd3f0eb9',\n",
      "                                     'type': 'current_plan_step'}],\n",
      "                'id': 'post-20231214-223046-20c89f11',\n",
      "                'message': 'Hello! I can assist you by breaking down tasks '\n",
      "                           'into subtasks and coordinating with '\n",
      "                           'CodeInterpreter to execute them. This includes '\n",
      "                           'tasks like data analysis, file operations, and '\n",
      "                           'generating Python code to accomplish specific '\n",
      "                           'objectives. How can I assist you today?',\n",
      "                'send_from': 'Planner',\n",
      "                'send_to': 'User'}],\n",
      " 'state': 'finished',\n",
      " 'user_query': 'hello, what can you do?'}\n"
     ]
    }
   ],
   "source": [
    "# Heres what that looks like in JSON\n",
    "pp.pprint(response_round.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classic stock market agent benchmark\n",
    "\n",
    "protip: make sure `llm.response_format` is `text` because when working with `json` i found it crashed when using the code interpreter.\n",
    "\n",
    "> 💡 Up to 11/30/2023, the json_object and text options of `llm.response_format` is only supported by the OpenAI models later than `1106`. If you are using an older version of OpenAI model, you need to set the llm.response_format to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "current_plan_step:\n",
      "Fetching Tesla stock price data YTD\n",
      "send_to:\n",
      "CodeInterpreter\n",
      "message:\n",
      "Please fetch the year-to-date (YTD) stock price data for Tesla (TSLA) and store it in a DataFrame.\n",
      "Planner->CodeInterpreter:\n",
      "Please fetch the year-to-date (YTD) stock price data for Tesla (TSLA) and store it in a DataFrame.\n",
      "CodeInterpreter->CodeInterpreter:\n",
      "Your previous message is not following the output format. You must generate the output as a JSON object with the following format:\n",
      "{\"response\": [{\"type\":\"this is the type\", \"content\": \"this is the content\"}, ...]}\n",
      "You need at least have an element with type 'python' and content being the code to be executed.\n",
      "Don't surround the JSON with ```json and ```, just send the JSON object directly.\n",
      "Please try again.\n",
      "thought:\n",
      "To fetch the YTD stock price data for Tesla, I can use the `yfinance` library which allows for easy retrieval of stock information. I will use the `Ticker` object to get the Tesla stock data and then use the `history` method to fetch the YTD data. I will ensure to calculate the start date of the YTD based on the current date.\n",
      "python:\n",
      "# Importing necessary libraries\n",
      "import yfinance as yf\n",
      "from datetime import datetime\n",
      "\n",
      "# Calculate the start date of the current year (YTD)\n",
      "start_date = datetime(datetime.now().year, 1, 1)\n",
      "\n",
      "# Fetch the stock price data for Tesla (TSLA) YTD\n",
      "# Create a Ticker object for Tesla\n",
      "tesla_ticker = yf.Ticker('TSLA')\n",
      "\n",
      "# Fetch the historical data for YTD\n",
      "# The 'period' parameter is set to 'YTD' to fetch the data from the start of the current year to the current date\n",
      "tesla_data = tesla_ticker.history(start=start_date)\n",
      "\n",
      "# Display the first few rows of the DataFrame\n",
      "tesla_data.head()\n",
      "tesla_data\n",
      "verification:\n",
      "CORRECT\n",
      "Plugin data_analysis failed to load: [Errno 2] No such file or directory: '/Users/award40/Desktop/personal/github/generative-ai-workbook/learning/TaskWeaver/_starter_projects/plugins/data_analysis.py'\n",
      "status:\n",
      "SUCCESS\n",
      "result:\n",
      "The execution of the generated python code above has succeeded\n",
      "\n",
      "The result of above Python code after execution is:\n",
      "                                 Open        High         Low       Close     Volume  Dividends  Stock Splits\n",
      "Date                                                                                                         \n",
      "2023-01-03 00:00:00-05:00  118.470001  118.800003  104.639999  108.099998  231402800        0.0           0.0\n",
      "2023-01-04 00:00:00-05:00  109.110001  114.589996  107.519997  113.639999  180389000        0.0           0.0\n",
      "...                               ...         ...         ...         ...        ...        ...           ...\n",
      "2023-12-13 00:00:00-05:00  234.190002  240.300003  228.199997  239.289993  146286300        0.0           0.0\n",
      "2023-12-14 00:00:00-05:00  241.220001  253.880005  240.789993  251.050003  160015841        0.0           0.0\n",
      "\n",
      "[240 rows x 7 columns]\n",
      "CodeInterpreter->Planner:\n",
      "The following python code has been executed:\n",
      "```python\n",
      "# Importing necessary libraries\n",
      "import yfinance as yf\n",
      "from datetime import datetime\n",
      "\n",
      "# Calculate the start date of the current year (YTD)\n",
      "start_date = datetime(datetime.now().year, 1, 1)\n",
      "\n",
      "# Fetch the stock price data for Tesla (TSLA) YTD\n",
      "# Create a Ticker object for Tesla\n",
      "tesla_ticker = yf.Ticker('TSLA')\n",
      "\n",
      "# Fetch the historical data for YTD\n",
      "# The 'period' parameter is set to 'YTD' to fetch the data from the start of the current year to the current date\n",
      "tesla_data = tesla_ticker.history(start=start_date)\n",
      "\n",
      "# Display the first few rows of the DataFrame\n",
      "tesla_data.head()\n",
      "tesla_data\n",
      "```\n",
      "\n",
      "\n",
      "The execution of the generated python code above has succeeded\n",
      "\n",
      "The result of above Python code after execution is:\n",
      "                                 Open        High         Low       Close     Volume  Dividends  Stock Splits\n",
      "Date                                                                                                         \n",
      "2023-01-03 00:00:00-05:00  118.470001  118.800003  104.639999  108.099998  231402800        0.0           0.0\n",
      "2023-01-04 00:00:00-05:00  109.110001  114.589996  107.519997  113.639999  180389000        0.0           0.0\n",
      "...                               ...         ...         ...         ...        ...        ...           ...\n",
      "2023-12-13 00:00:00-05:00  234.190002  240.300003  228.199997  239.289993  146286300        0.0           0.0\n",
      "2023-12-14 00:00:00-05:00  241.220001  253.880005  240.789993  251.050003  160015841        0.0           0.0\n",
      "\n",
      "[240 rows x 7 columns]\n",
      "init_plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "current_plan_step:\n",
      "Plotting the stock price data\n",
      "send_to:\n",
      "CodeInterpreter\n",
      "message:\n",
      "Please generate a plot for the Tesla stock price data YTD focusing on the 'Close' price and save the plot as an image file in the current working directory.\n",
      "Planner->CodeInterpreter:\n",
      "Please generate a plot for the Tesla stock price data YTD focusing on the 'Close' price and save the plot as an image file in the current working directory.\n",
      "thought:\n",
      "To plot the 'Close' price of the Tesla stock data YTD, I will use the `matplotlib` library, which is a plotting library that allows for creating static, interactive, and animated visualizations in Python. I will create a line plot for the 'Close' price using the DataFrame that contains the Tesla stock data YTD, which was fetched in the previous step. After plotting, I will save the figure as an image file in the current working directory using the `savefig` method from `matplotlib.pyplot`.\n",
      "python:\n",
      "# Importing the matplotlib library for plotting\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plotting the 'Close' price of the Tesla stock data YTD\n",
      "plt.figure(figsize=(14, 7))\n",
      "plt.plot(tesla_data.index, tesla_data['Close'], label='Close Price')\n",
      "plt.title('Tesla Stock Close Price YTD')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Price (USD)')\n",
      "plt.legend()\n",
      "\n",
      "# Save the plot as an image file in the current working directory\n",
      "plt.savefig('tesla_stock_close_price_ytd.png')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "# Variables to be returned\n",
      "tesla_data, 'tesla_stock_close_price_ytd.png'\n",
      "verification:\n",
      "CORRECT\n",
      "status:\n",
      "SUCCESS\n",
      "result:\n",
      "The execution of the generated python code above has succeeded\n",
      "\n",
      "The result of above Python code after execution is:\n",
      "(                                 Open        High         Low       Close     Volume  Dividends  Stock Splits\n",
      " Date                                                                                                         \n",
      " 2023-01-03 00:00:00-05:00  118.470001  118.800003  104.639999  108.099998  231402800        0.0           0.0\n",
      " 2023-01-04 00:00:00-05:00  109.110001  114.589996  107.519997  113.639999  180389000        0.0           0.0\n",
      " ...                               ...         ...         ...         ...        ...        ...           ...\n",
      " 2023-12-13 00:00:00-05:00  234.190002  240.300003  228.199997  239.289993  146286300        0.0           0.0\n",
      " 2023-12-14 00:00:00-05:00  241.220001  253.880005  240.789993  251.050003  160015841        0.0           0.0\n",
      " \n",
      " [240 rows x 7 columns],\n",
      " 'tesla_stock_close_price_ytd.png')\n",
      "The following artifacts were generated:\n",
      "- type: image ; uri: http://artifact-ref/post-20231214-223905-20ec398b-display-1_image.png ; description: <Figure size 2800x1400 with 1 Axes>\n",
      "\n",
      "CodeInterpreter->Planner:\n",
      "The following python code has been executed:\n",
      "```python\n",
      "# Importing the matplotlib library for plotting\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Plotting the 'Close' price of the Tesla stock data YTD\n",
      "plt.figure(figsize=(14, 7))\n",
      "plt.plot(tesla_data.index, tesla_data['Close'], label='Close Price')\n",
      "plt.title('Tesla Stock Close Price YTD')\n",
      "plt.xlabel('Date')\n",
      "plt.ylabel('Price (USD)')\n",
      "plt.legend()\n",
      "\n",
      "# Save the plot as an image file in the current working directory\n",
      "plt.savefig('tesla_stock_close_price_ytd.png')\n",
      "\n",
      "# Display the plot\n",
      "plt.show()\n",
      "\n",
      "# Variables to be returned\n",
      "tesla_data, 'tesla_stock_close_price_ytd.png'\n",
      "```\n",
      "\n",
      "\n",
      "The execution of the generated python code above has succeeded\n",
      "\n",
      "The result of above Python code after execution is:\n",
      "(                                 Open        High         Low       Close     Volume  Dividends  Stock Splits\n",
      " Date                                                                                                         \n",
      " 2023-01-03 00:00:00-05:00  118.470001  118.800003  104.639999  108.099998  231402800        0.0           0.0\n",
      " 2023-01-04 00:00:00-05:00  109.110001  114.589996  107.519997  113.639999  180389000        0.0           0.0\n",
      " ...                               ...         ...         ...         ...        ...        ...           ...\n",
      " 2023-12-13 00:00:00-05:00  234.190002  240.300003  228.199997  239.289993  146286300        0.0           0.0\n",
      " 2023-12-14 00:00:00-05:00  241.220001  253.880005  240.789993  251.050003  160015841        0.0           0.0\n",
      " \n",
      " [240 rows x 7 columns],\n",
      " 'tesla_stock_close_price_ytd.png')\n",
      "The following artifacts were generated:\n",
      "- type: image ; uri: http://artifact-ref/post-20231214-223905-20ec398b-display-1_image.png ; description: <Figure size 2800x1400 with 1 Axes>\n",
      "\n",
      "init_plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "plan:\n",
      "1. Fetch Tesla stock price data YTD\n",
      "2. Plot the stock price data\n",
      "3. Save the plot to the current working directory\n",
      "current_plan_step:\n",
      "Informing the User that the task is completed\n",
      "send_to:\n",
      "User\n",
      "message:\n",
      "The plot for the Tesla stock price YTD has been successfully created and saved to the current working directory as 'tesla_stock_close_price_ytd.png'.\n",
      "Planner->User:\n",
      "The plot for the Tesla stock price YTD has been successfully created and saved to the current working directory as 'tesla_stock_close_price_ytd.png'.\n",
      "final_reply_message:\n",
      "The plot for the Tesla stock price YTD has been successfully created and saved to the current working directory as 'tesla_stock_close_price_ytd.png'.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"Write code to plot the price of tesla stock YTD and save it to the current working directory\"\n",
    "response_round = session.send_message(user_query,\n",
    "                                      event_handler=lambda _type, _msg: print(f\"{_type}:\\n{_msg}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./_starter_projects/tesla_stock_price_ytd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../_starter_projects/workspace/sessions/20231212-200217-2ba37e82/cwd/tesla_stock_price_ytd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugins\n",
    "\n",
    "A plugin is the equivalent to a function call in Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from injector import Injector\n",
    "\n",
    "from taskweaver.config.config_mgt import AppConfigSource\n",
    "from taskweaver.logging import LoggingModule\n",
    "from taskweaver.memory import Memory, Post, Round\n",
    "from taskweaver.memory.plugin import PluginModule, PluginRegistry\n",
    "from taskweaver.planner import Planner\n",
    "from taskweaver.plugin import Plugin, register_plugin, test_plugin\n",
    "\n",
    "# Plugin definition\n",
    "@register_plugin\n",
    "class DataAnalysisPlugin(Plugin):\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Performs a simple analysis on the DataFrame and returns a description.\n",
    "        \"\"\"\n",
    "        row_count = len(df)\n",
    "        column_count = len(df.columns)\n",
    "        description = f\"The DataFrame has {row_count} rows and {column_count} columns.\"\n",
    "        return df, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TaskWeaver environment\n",
    "app_injector = Injector([PluginModule, LoggingModule])\n",
    "app_config = AppConfigSource(config={\n",
    "    \"plugin.base_path\": os.path.join(app_dir, \"plugins\"),\n",
    "    \"llm.api_key\": \"******\",\n",
    "})\n",
    "\n",
    "# app.\n",
    "app_injector.binder.bind(AppConfigSource, to=app_config)\n",
    "\n",
    "plugin_registry = app_injector.get(PluginRegistry)\n",
    "planner = app_injector.create_object(Planner)\n",
    "\n",
    "# Create a memory object for the session\n",
    "memory = Memory(session_id=\"session-1\")\n",
    "\n",
    "def send_message_to_session(query):\n",
    "    # Create a new round with the user query\n",
    "    round1 = Round.create(user_query=query, id=\"round-1\")\n",
    "    post1 = Post.create(message=query, send_from=\"User\", send_to=\"Planner\", attachment_list=[])\n",
    "    round1.add_post(post1)\n",
    "\n",
    "    # Add the round to the memory and compose the prompt\n",
    "    memory.conversation.add_round(round1)\n",
    "    prompt = planner.compose_prompt(rounds=memory.conversation.rounds)\n",
    "\n",
    "    # Here you should process the prompt using the appropriate plugin or logic\n",
    "    # For demonstration, I'm just printing the prompt\n",
    "    print(\"Processing:\", prompt)\n",
    "\n",
    "    # Process the result and print the response (This is where your plugin logic will go)\n",
    "    # Assuming a response is generated\n",
    "    response = \"Processed response for: \" + query\n",
    "    print(\"Response:\\n\", response)\n",
    "\n",
    "    # Update the conversation with the response\n",
    "    response_post = Post.create(message=response, send_from=\"Planner\", send_to=\"User\", attachment_list=[])\n",
    "    round1.add_post(response_post)\n",
    "\n",
    "    return round1\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Tell me about the data\"\n",
    "round  = send_message_to_session(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
