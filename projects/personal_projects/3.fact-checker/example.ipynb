{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain\n",
    "\n",
    "This notebook is an initial experimentation with Langchain.\n",
    "\n",
    "### Components of LangChain\n",
    "\n",
    "1. **Prompts**: You can define prompt templates, so you take the user input and use the templates to create the final prompt for the model \n",
    "2. **Chains**: Chains are sequences of LLM calls.  For example: chain together prompt templates and LLMs. There are many-to-many combinations of how you can chain components together. \n",
    "3. **Memory**: By default, Chains and Agents are stateless, meaning that they treat each incoming query independently (as are the underlying LLMs and chat models). Langchain provides memory interfaces to allow you to store history of conversations. \n",
    "4. **Indexes**: Utility functions to allow you to provide your own text data. For example, it provides data loaders to allow you load in your own PDF files, emails etc. It also provides vector store interfaces to store the data in a vector database - Making it easily retrievable for the model/chain.  \n",
    "5. **Agents/Tools**: You can set up agents powered by large language models that can use tools such as google search, wikipedia or calculators.  \n",
    "\n",
    "<img src=\"./img/lc.png\" style=\"height:500px; display: block; margin-right: auto; margin-left: auto;\">\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "- [Docs](https://langchain.readthedocs.io/en/latest/index.html)\n",
    "- [Github Repo](https://github.com/hwchase17/langchain)\n",
    "- [LangChain explained - The hottest new Python framework](https://www.youtube.com/watch?v=RoR4XJw8wIc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "\n",
    "import os\n",
    "\n",
    "from nbdev.export import nb_export\n",
    "import streamlit as st\n",
    "from streamlit_jupyter import StreamlitPatcher, tqdm\n",
    "\n",
    "StreamlitPatcher().jupyter()  \n",
    "\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain \n",
    "from langchain.llms import OpenAI \n",
    "from langchain.prompts import PromptTemplate "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-21 13:21:03.981 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/award40/opt/anaconda3/envs/dev/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "#|exporti\n",
    "\n",
    "# If an API key has been provided, create an OpenAI language model instance\n",
    "API = os.getenv('OPENAI_KEY')\n",
    "\n",
    "if API:\n",
    "    llm = OpenAI(temperature=0.7, openai_api_key=os.getenv('OPENAI_KEY'))\n",
    "else:\n",
    "    # If an API key hasn't been provided, display a warning message\n",
    "    st.warning(\"OPENAI_KEY Not Found\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain Components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# ✅ What's TRUE  : Using LangChain `SimpleSequentialChain`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Inspired from [fact-checker](https://github.com/jagilley/fact-checker) by Jagiley"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|exporti\n",
    "\n",
    "# Create simple web app using Streamlit\n",
    "\n",
    "# Set the title of the Streamlit app\n",
    "st.title(\"⛓ LangChain Example\")\n",
    "\n",
    "# Add a link to the Github repository that inspired this app\n",
    "st.markdown(\"Enter a question below to ask ChatGPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8e85d35cf14c638805b8bf1b1090ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', description='Enter Your Question : ', placeholder='Type something')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|exporti\n",
    "\n",
    "# Add a text input box for the user's question\n",
    "user_question = st.text_input(\n",
    "    \"Enter Your Question : \",\n",
    "    placeholder = \"How was your day?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain 1: Generating a rephrased version of the user's question\n",
    "template = \"\"\"{question}\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"question\"], template=template)\n",
    "question_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "\n",
    "# Chain 2: Generating assumptions made in the statement\n",
    "template = \"\"\"Here is a statement:\n",
    "    {statement}\n",
    "    Make a bullet point list of the assumptions you made when producing the above statement.\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"statement\"], template=template)\n",
    "assumptions_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "assumptions_chain_seq = SimpleSequentialChain(\n",
    "    chains=[question_chain, assumptions_chain], verbose=True\n",
    ")\n",
    "\n",
    "# Chain 3: Fact checking the assumptions\n",
    "template = \"\"\"Here is a bullet point list of assertions:\n",
    "{assertions}\n",
    "For each assertion, determine whether it is true or false. If it is false, explain why.\\n\\n\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"assertions\"], template=template)\n",
    "fact_checker_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "fact_checker_chain_seq = SimpleSequentialChain(\n",
    "    chains=[question_chain, assumptions_chain, fact_checker_chain], verbose=True\n",
    ")\n",
    "\n",
    "# Final Chain: Generating the final answer to the user's question based on the facts and assumptions\n",
    "template = \"\"\"In light of the above facts, how would you answer the question '{}'\"\"\".format(\n",
    "    user_question\n",
    ")\n",
    "template = \"\"\"{facts}\\n\"\"\" + template\n",
    "prompt_template = PromptTemplate(input_variables=[\"facts\"], template=template)\n",
    "answer_chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "overall_chain = SimpleSequentialChain(\n",
    "    chains=[question_chain, assumptions_chain, fact_checker_chain, answer_chain],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "\n",
    "# Generating the final answer to the user's question using all the chains\n",
    "if st.button(\"Send\", type=\"primary\"):\n",
    "\n",
    "    # Running all the chains on the user's question and displaying the final answer\n",
    "    st.success(overall_chain.run(user_question))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Streamlit components as py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_export('example.ipynb', lib_path='./', name='example')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
