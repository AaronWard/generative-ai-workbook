{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Generation\n",
    "\n",
    "Last time i did this it cost about $10 to create question:answer pairs, this time im gonna use local models to it using ollama.\n",
    "This will iterate over the posts extracted from r/localllama and generate a QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import ollama\n",
    "import pickle\n",
    "import pprint as pp\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "%time\n",
    "response = ollama.chat(model='mistral:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "    'temperature': 0.01,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "%time\n",
    "response = ollama.generate(model='mistral:latest', prompt='Why is the sky blue?')\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama client\n",
    "\n",
    "`OLLAMA_HOST=127.0.0.1:5050 ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 975 questions in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Post ID: 1at0288\\nTitle: Ok, which one of you was this? ðŸ¤£ðŸ¤£ðŸ¤£\\nLink: https://redd.it/1at0288\\nContent: \\nReplies:\\n- No, I don't think OpenAI would ever allow porn to be generated. I rather think that copies of Sora, recreated open source image generators will appear and fullfill this task. Porn is always one of the first use cases in any technologie that appeared and I don't think it'll take long for the industry to hop into this new tech. This is good for us as it further pushes open source AI technology for any use case.\\n\\n\",\n",
       " ' 1aszy6f\\nTitle: What are your favorite resources for evaluating text generation for stuff like readability, engagement (and other \"soft\" metrics)\\nLink: https://redd.it/1aszy6f\\nContent: Hi everyone, i\\'m working on a thesis looking at different prompt engineering methods and trying to evaluate the quality of generated content for stuff like articles, newsletters = human read content. Most research focuses on stuff like factuality, reasoning but I\\'m trying to find more research or these \"soft\" metrics. Any resources welcome, academic papers preferred. Many thanks!\\n\\n',\n",
       " ' 1aszspr\\nTitle: State of Opensource and Closedsource as of right now:\\nLink: https://redd.it/1aszspr\\nContent: What are the best opensource and closedsourced AI models?\\n\\nWhat will be the best opensource and closedsource AI models at the end of this year?\\n\\nList as per the following categories:\\n\\n1. Large Language Models:\\n\\n2. Large Multimodal Models:\\n\\n3. Text to image: \\n\\n4. Text to video: \\n\\n5. Text to 3D:\\n\\n6. Text to audio:\\n\\n7. Audio to text (transcribing):\\n\\n8. General purpose robots powered by Large multimodal models:\\n\\n']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./_output/new/localllama-new-17-02-2024.txt\"\n",
    "with open(DATA_PATH, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_chunks = data.split(\"---\\nPost ID:\")\n",
    "\n",
    "print(f\"There are {len(data_chunks)} questions in total\")\n",
    "data_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ollama.Client(host='http://127.0.0.1:5050')\n",
    "response_chunks = []\n",
    "\n",
    "instructions = f\"\"\"\n",
    "\\n\n",
    "# INSTRUCTIONS: \n",
    "Your job is to look at this single reddit post and to produce several technical question/answer pairs based on the content provided. \n",
    "Your response will be inserted into question and answer dataset made up of hundreds of reddit post QA pairs.\n",
    "For longer posts (such as ones with a lot of information in the content or with many comments) produce a lot of QA pairs. \n",
    "For posts with less content, produce fewer. Only include QA pairs with general useful information. \n",
    "Also look at the replies for additional informative technical information. \n",
    "Write everything in the present tense. Provide code extracts or configurations where appropriate.\n",
    "Write the QA's as general questions and not specific to the reddit post itself, as there would be no context to the post in the dataset.  \n",
    "\n",
    "# RULES: \n",
    "Do NOT produce QA pairs for anything that is not in the provided text. \n",
    "Do NOT include phrases like \"the user\", \"the poster\", \"this post\", \"reddit post\", \"the person\" or \"the author\".\n",
    "Only provide the QA pairs. \n",
    "Do NOT provide introductions or conclusions.\n",
    "Do NOT write anything that is personal information, personal opinion, or conversational text.\n",
    "Failure to comply with these rules will result in you being penalized.\n",
    "Adhering to the rules will get you a $200 tip.\n",
    "\n",
    "# FORMAT: \n",
    "Write your response in this format:\n",
    "```\n",
    "Q: What is the colour of the sky?\n",
    "A: The colour of the sky is blue.\n",
    "\n",
    "Q: How old is OpenAI? \n",
    "A: OpenAI was founded in 2015, therefore it is 8 years old.\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = f\"./_output/new/responses-mistral-new-{datetime.now().strftime('%d-%m-%Y')}.txt\"\n",
    "\n",
    "print(\"Generating QA Pairs...\")\n",
    "with open(output_file, \"w\") as file:\n",
    "    for chunk in tqdm.tqdm(data_chunks):\n",
    "        prompt = f\"\"\"\n",
    "        ```\n",
    "        {chunk}\n",
    "        ```\n",
    "        \"\"\" + instructions\n",
    "\n",
    "        file.writelines(f\"\"\"{\n",
    "            client.chat(model='mistral:latest', messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'temperature': 0.1\n",
    "            }])['message']['content']} \\n\\n\"\"\"\n",
    "        )\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
