{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentchat Module\n",
    "\n",
    "AutoGen offers several classes allowing developers to work with LLMs and solve tasks, particularly in the domain of mathematics and retrieval-based tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "load_dotenv(Path('../../.env'))\n",
    "\n",
    "import openai\n",
    "import autogen\n",
    "\n",
    "env_var = [\n",
    "    {\n",
    "        'model': 'gpt-4',\n",
    "        'api_key': os.environ['OPENAI_API_KEY']\n",
    "    },\n",
    "    {\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'api_key': os.environ['OPENAI_API_KEY']\n",
    "    }\n",
    "]\n",
    "\n",
    "# needed to convert to str\n",
    "env_var = json.dumps(env_var)\n",
    "\n",
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=env_var,\n",
    "    filter_dict={\n",
    "        \"model\": {\n",
    "            \"gpt-4\",\n",
    "            \"gpt4\",\n",
    "            \"gpt-4-32k\",\n",
    "            \"gpt-4-32k-0314\",\n",
    "            \"gpt-4-32k-v0314\",\n",
    "            \"gpt-3.5-turbo\",\n",
    "            \"gpt-3.5-turbo-16k\",\n",
    "            \"gpt-3.5-turbo-0301\",\n",
    "            \"chatgpt-35-turbo-0301\",\n",
    "            \"gpt-35-turbo-v0301\",\n",
    "            \"gpt\",\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### math_user_proxy_agent\n",
    "\n",
    "The `MathUserProxyAgent` is a part of the `agentchat.contrib.math_user_proxy_agent` module, and it is designed to handle math problems. It seems to be experimental at this stage. This class can generate initial messages, execute Python code, and run queries through Wolfram Alpha to solve mathematical problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve_assistant_agent\n",
    "\n",
    "The `RetrieveAssistantAgent` belongs to the `agentchat.contrib.retrieve_assistant_agent` module and is designed to solve tasks with LLMs, including suggesting Python code blocks and debugging. This agent does not execute code by default and expects the user to execute the code. This class is a subclass of AssistantAgent, configured with a default system message to solve tasks with LLMs.\n",
    "\n",
    "**Purpose:**\n",
    "The RetrieveAssistantAgent is a specialized agent designed to solve tasks with Language Learning Models (LLMs) like GPT. It is configured with a default system message and is specialized in suggesting Python code blocks and assisting with debugging. By default, human_input_mode is set to \"NEVER\", and `code_execution_config` is set to False, meaning this agent does not execute code by default and expects the user to execute the code. It is particularly useful when the user needs suggestions or guidance in writing or debugging Python code, but the execution of the code is left to the user.\n",
    "\n",
    "\n",
    "#### retrieve_user_proxy_agent\n",
    "The `RetrieveUserProxyAgent` class is part of the `agentchat.contrib.retrieve_user_proxy_agent` module and seems to be designed to interact with users, asking for human inputs every time a message is received based on its configuration. It also allows the generation of initial messages with given problems and prompts and can be configured with various parameters like the task type, client, docs_path, collection_name, and model to use for the retrieve chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m assistant \u001b[39m=\u001b[39m RetrieveAssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     system_message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are a helpful assistant.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Create an instance of RetrieveUserProxyAgent\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m ragproxyagent \u001b[39m=\u001b[39m RetrieveUserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mragproxyagent\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNEVER\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     max_consecutive_auto_reply\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     retrieve_config\u001b[39m=\u001b[39m{\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtask\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcode\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdocs_path\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m./docs\u001b[39m\u001b[39m\"\u001b[39m,  \u001b[39m# Assuming documentation is stored in a 'docs' directory\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mchunk_token_size\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2000\u001b[39m,\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m: config_list[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mclient\u001b[39m\u001b[39m\"\u001b[39m: chromadb\u001b[39m.\u001b[39mPersistentClient(path\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/tmp/chromadb\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39membedding_model\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mall-mpnet-base-v2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Simulate a user asking a question and initiate chat\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/llm-masterclass/projects/autogen/concept_examples/1.agentchat.contrib.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m user_question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan you provide a sample Python code for printing \u001b[39m\u001b[39m'\u001b[39m\u001b[39mHello, World!\u001b[39m\u001b[39m'\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "import autogen\n",
    "import chromadb\n",
    "\n",
    "# Start logging\n",
    "autogen.ChatCompletion.start_logging()\n",
    "\n",
    "# Create an instance of RetrieveAssistantAgent\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Create an instance of RetrieveUserProxyAgent\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"code\",\n",
    "        \"docs_path\": \"./docs\",  # Assuming documentation is stored in a 'docs' directory\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Simulate a user asking a question and initiate chat\n",
    "user_question = \"Can you provide a sample Python code for printing 'Hello, World!'?\"\n",
    "ragproxyagent.initiate_chat(assistant, problem=user_question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_user_proxy_agent\n",
    "retrieve_assistant_agent\n",
    "retrieve_user_proxy_agent\n",
    "agent\n",
    "assistant_agent\n",
    "conversable_agent\n",
    "groupchat\n",
    "user_proxy_agent\n",
    "oai\n",
    "completion\n",
    "openai_utils\n",
    "code_utils\n",
    "math_utils\n",
    "retrieve_utils\n",
    "\n",
    "\n",
    "\n",
    "With all that you know about the following modules in AutoGen agentchat.contrib module, I want you to provide a comprehensive walk through of how autogen can be used,\n",
    "the examples should follow some logical workflow to achieve a specific problem. Provide full commented code where appropriate. \n",
    "Be detailed in your response. Show how the module can be used with other modules in your example, if it requires it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
