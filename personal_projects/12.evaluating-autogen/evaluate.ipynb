{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import openai\n",
    "import random\n",
    "import autogen\n",
    "import tempfile\n",
    "import chromadb\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from typing import Any\n",
    "from pathlib import Path\n",
    "\n",
    "import wandb\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import QAGenerationChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.output_parsers import OutputFixingParser\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "from utils import is_termination_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"wandb-autogen\"\n",
    "DOC_FILE = \"documents/2308.08155.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def get_split_docs(doc_file):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        # Set a really small chunk size, just to show.\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "    )\n",
    "\n",
    "    loader = PyPDFLoader(doc_file)\n",
    "    pages = loader.load_and_split(text_splitter=text_splitter)\n",
    "    return pages\n",
    "\n",
    "\n",
    "def save_autogen_logs(logs_filename):\n",
    "    logs = autogen.ChatCompletion.logged_history\n",
    "    json.dump(logs, open(logs_filename, \"w\"),\n",
    "              indent=4)\n",
    "    return logs\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    config_list = autogen.config_list_from_dotenv(\n",
    "        dotenv_file_path=\"../../.env\",\n",
    "        model_api_key_map={\n",
    "            \"gpt-4\": \"OPENAI_API_KEY\",\n",
    "            \"gpt-3.5-turbo\": \"OPENAI_API_KEY\",\n",
    "        },\n",
    "        filter_dict={\n",
    "            \"model\": {\n",
    "                \"gpt-3.5-turbo\",\n",
    "                \"gpt-4\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return config_list\n",
    "\n",
    "\n",
    "def get_random_chunks(num_qa_pairs):\n",
    "    random_chunks = []\n",
    "    for i in range(num_qa_pairs):\n",
    "        random_chunks.append(random.randint(5, 172))  # (5, 172)\n",
    "\n",
    "    return random_chunks\n",
    "\n",
    "\n",
    "# LLM Functions\n",
    "def instiate_agents(config_list):\n",
    "    coding_assistant_config = {\n",
    "        \"name\": \"coding_assistant\",\n",
    "        \"llm_config\": {\n",
    "                \"request_timeout\": 1000,\n",
    "                \"seed\": 42,\n",
    "                \"config_list\": config_list,\n",
    "                \"temperature\": 0.1,\n",
    "        }\n",
    "    }\n",
    "    coding_runner_config = {\n",
    "        \"name\": \"coding_runner\",\n",
    "        \"human_input_mode\": \"NEVER\",\n",
    "        \"max_consecutive_auto_reply\": 5,\n",
    "        \"is_termination_msg\": is_termination_message,\n",
    "        \"code_execution_config\": {\n",
    "                \"work_dir\": \"./output\",\n",
    "                \"use_docker\": False\n",
    "        },\n",
    "        \"llm_config\": {\n",
    "            \"request_timeout\": 1000,\n",
    "            \"seed\": 42,\n",
    "            \"config_list\": config_list,\n",
    "            \"temperature\": 0.1,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    coding_assistant = AssistantAgent(**coding_assistant_config)\n",
    "    user_proxy = UserProxyAgent(**coding_runner_config)\n",
    "\n",
    "    return coding_assistant, user_proxy\n",
    "\n",
    "\n",
    "def get_qa_response(user_proxy,\n",
    "                    assistant,\n",
    "                    user_message,\n",
    "                    config_list,\n",
    "                    logs_filename):\n",
    "    \"\"\" Function for getting answer from agents\"\"\"\n",
    "\n",
    "    user_proxy.initiate_chat(\n",
    "        assistant, message=user_message, config_list=config_list)\n",
    "    save_autogen_logs(logs_filename)\n",
    "\n",
    "    # Filter and modify messages with \"TERMINATE\" and take last message\n",
    "    original_message_history = assistant.chat_messages[user_proxy]\n",
    "\n",
    "    message_history = []\n",
    "    for message in original_message_history:\n",
    "        stripped_content = message[\"content\"].strip()\n",
    "        if stripped_content != \"TERMINATE\":\n",
    "            if stripped_content.endswith(\"TERMINATE\"):\n",
    "                message[\"content\"] = stripped_content.replace(\n",
    "                    \"TERMINATE\", \"\").strip()\n",
    "            message_history.append(message)\n",
    "\n",
    "    if message_history:\n",
    "        # Identify the final response\n",
    "        return message_history[-1][\"content\"]\n",
    "    else:\n",
    "        return \"No valid messages found.\"\n",
    "\n",
    "\n",
    "def get_eval_score(graded_outputs):\n",
    "    \"\"\"\n",
    "    https://rajpurkar.github.io/SQuAD-explorer/\n",
    "\n",
    "    Exact Match: For each question-answer pair, if the tokens of the model's prediction \n",
    "        exactly match the tokens of the true answer, exact_match is 100; otherwise, exact_match is 0. \n",
    "        One can imagine that each token matching is a rare occurrence for a stochastic system. \n",
    "        This metric should be taken with a grain of salt for our use case. \n",
    "    F1 Score: This is a well-known metric that cares equally about the precision and \n",
    "        recall of the system. Precision is the ratio of shared tokens to the total number of tokens \n",
    "        in the prediction. Recall is the ratio of shared tokens to the \n",
    "        total number of tokens in the ground truth.\n",
    "    \"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    for graded_output in graded_outputs:\n",
    "        assert isinstance(graded_output, dict)\n",
    "        if graded_output[\"text\"].strip() == \"CORRECT\":\n",
    "            correct += 1\n",
    "\n",
    "    return correct/len(graded_outputs)\n",
    "\n",
    "\n",
    "def generate_eval_dataset(text):\n",
    "    template = \"\"\"You are a smart assistant designed to come up with meaninful question and answer pair. The question should be to the point and the answer should be as detailed as possible.\n",
    "    Given a piece of text, you must come up with a question and answer pair that can be used to evaluate a QA bot. Do not make up stuff. Stick to the text to come up with the question and answer pair.\n",
    "    When coming up with this question/answer pair, you must respond in the following format:\n",
    "    ```\n",
    "    {{\n",
    "        \"question\": \"$YOUR_QUESTION_HERE\",\n",
    "        \"answer\": \"$THE_ANSWER_HERE\"\n",
    "    }}\n",
    "    ```\n",
    "\n",
    "    Everything between the ``` must be valid json.\n",
    "\n",
    "    Please come up with a question/answer pair, in the specified JSON format, for the following text:\n",
    "    ----------------\n",
    "    {text}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "    # Generate QA\n",
    "    llm = ChatOpenAI(temperature=0.9)\n",
    "    return QAGenerationChain.from_llm(llm=llm, prompt=prompt).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(config) -> wandb.config:\n",
    "    \"\"\"\n",
    "    Define the objective function to optimize.\n",
    "    Computes a metric based on the provided configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    - config (Any): Configuration parameters for the model\n",
    "    \n",
    "    Returns:\n",
    "    - \n",
    "    \"\"\"\n",
    "\n",
    "    config\n",
    "    \n",
    "    # Filenames\n",
    "    logs_filename=f\"logs_filename_{sweep}\" # Neeeds to be generated for each configuration\n",
    "\n",
    "    qa_pairs = []\n",
    "    num_qa_pairs = 5\n",
    "    random_chunks = get_random_chunks(num_qa_pairs)\n",
    "    \n",
    "    # Generate evaluation dataset\n",
    "    pages = get_split_docs(DOC_FILE)\n",
    "\n",
    "    for idx in random_chunks:\n",
    "        qa = generate_eval_dataset(pages[idx].page_content)\n",
    "        qa_pairs.extend(qa)\n",
    "\n",
    "    # log questions\n",
    "    qa_df = pd.DataFrame(qa_pairs)\n",
    "    wandb.log({\"QA Eval Pair\": qa_df})\n",
    "\n",
    "\n",
    "    # Set up Agent\n",
    "    config_list = get_config()\n",
    "    assistant, user_proxy = instiate_agents()\n",
    "    predictions = []\n",
    "\n",
    "    for qa_pair in qa_pairs:\n",
    "        question = qa_pair[\"question\"]\n",
    "        print(question)\n",
    "        predictions.append({\"response\": get_qa_response(user_proxy, \n",
    "                                                        assistant, \n",
    "                                                        question, \n",
    "                                                        config_list,\n",
    "                                                        logs_filename)})\n",
    "\n",
    "   \n",
    "    # Evaluation \n",
    "    eval_chain = QAEvalChain.from_llm(llm = OpenAI(temperature=0))\n",
    "\n",
    "    graded_outputs = eval_chain.evaluate(\n",
    "        qa_pairs, predictions, question_key=\"question\", prediction_key=\"response\"\n",
    "    )\n",
    "\n",
    "    score = get_eval_score(graded_outputs)\n",
    "\n",
    "    # Some data munging to get the examples in the right format\n",
    "    for i, eg in enumerate(qa_pairs):\n",
    "        eg[\"id\"] = str(i)\n",
    "        eg[\"answers\"] = {\"text\": [eg[\"answer\"]], \"answer_start\": [0]}\n",
    "        predictions[i][\"id\"] = str(i)\n",
    "        predictions[i][\"prediction_text\"] = predictions[i][\"response\"]\n",
    "\n",
    "    for p in predictions:\n",
    "        del p[\"response\"]\n",
    "\n",
    "    new_qa_pairs = qa_pairs.copy()\n",
    "    for eg in new_qa_pairs:\n",
    "        del eg[\"question\"]\n",
    "        del eg[\"answer\"]\n",
    "\n",
    "    \n",
    "    return score, new_qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Sweep results\n",
    "def visualize_results():\n",
    "    # wandb.log({\n",
    "    #     \"Calculator HF Spaces\": wandb.Html(\n",
    "    #         \"\"\"<iframe\n",
    "    #             src=\"https://<>.hf.space\"\n",
    "    #             frameborder=\"0\"\n",
    "    #             width=\"850\"\n",
    "    #             height=\"450\"\n",
    "    #         ></iframe>\"\"\"\n",
    "    #     )\n",
    "    # })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to initialize wandb and log the results.\n",
    "    \"\"\"\n",
    "    # Initialize W&B run\n",
    "    wandb.init(project=PROJECT_NAME)\n",
    "    \n",
    "    # Get the score from the objective function\n",
    "    score, new_qa_pairs = objective(wandb.config)\n",
    "    visualize_results(wandb.config)\n",
    "\n",
    "    # Log the score to W&B\n",
    "    wandb.log({\"score\": score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2: Define the search space\n",
    "sweep_configuration = None # import from contig.yaml\n",
    "\n",
    "# 3: Start the sweep\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=PROJECT_NAME)\n",
    "\n",
    "wandb.agent(sweep_id, function=main, count=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
