{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mistral 8x7B x Autogen\n",
    "\n",
    "This notebook aims to show how you can run mixtral 8x7B with Autogen.\n",
    "\n",
    "**Step to run locally:**\n",
    "- Download a llamafile of the [HuggingFace Repo](https://huggingface.co/award40/mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile/tree/main) - no installations needed, read the model card.  \n",
    "- `chmod +x mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile`\n",
    "- `./mixtral-8x7b-instruct-v0.1.Q3_K_M.llamafile --port 8081`\n",
    "<!-- ./llamafile -m mixtral-8x7b-instruct-v0.1.Q3_K_M.gguf -->\n",
    "\n",
    "Like many of you, i am GPU poor. The goal behind this approach was to have easy access to a good opensourced model with limited GPU resources, like a Macbook Pro M1 32GB.\n",
    "It's not the full model, but it's the most feasible given the resource constraints - see [here](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF#provided-files) for notes on performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting cache...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import autogen \n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "\n",
    "work_dir = \"./coding\"\n",
    "cache_dir = \"./.cache\"\n",
    "\n",
    "if os.path.exists(cache_dir) :\n",
    "    print('Deleting cache...')\n",
    "    shutil.rmtree(cache_dir)\n",
    "if os.path.exists(work_dir) :\n",
    "    print('Deleting previous work...')\n",
    "    shutil.rmtree(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'config_list': [{'base_url': 'http://127.0.0.1:8081/v1', 'api_key': 'NULL'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API Configurations\n",
    "config_list=[\n",
    "    {\n",
    "        \"base_url\": \"http://127.0.0.1:8081/v1\",\n",
    "        \"api_key\": \"NULL\",\n",
    "    }\n",
    "]\n",
    "\n",
    "llm_config_mistral={\n",
    "    \"config_list\": config_list,\n",
    "}\n",
    "\n",
    "llm_config_mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructions for the two agents\n",
    "system_messages = {\n",
    "    \"USER_PROXY_SYSTEM_MESSAGE\": (\n",
    "\"\"\"\n",
    "<s>[INST]\n",
    "Instruction:\n",
    "Assist in executing code and, upon successful completion, indicate the end of the task.\n",
    "\n",
    "Guidelines:\n",
    "- Confirm the successful execution of code before concluding any task.\n",
    "\n",
    "Communication Etiquette:\n",
    "- Avoid using phrases like \"Thank you\" or \"You're welcome\".\n",
    "- Do not extend assistance beyond the completed task.\n",
    "- If appreciation phrases occur, conclude the interaction with \"TERMINATE\".\n",
    "- \"TERMINATE\" must only be used after confirming the successfully executed the code.\n",
    "</s>[/INST]\n",
    "\"\"\"\n",
    "    ),\n",
    "    \"CODING_ASSISTANT_SYSTEM_MESSAGE\" : (\n",
    "\"\"\"\n",
    "<s>[INST]\n",
    "Instruction:\n",
    "As an AI specialized in programming, generate executable code based on user intent. \n",
    "\n",
    "Guidelines:\n",
    "- Provide complete, executable code without requiring user modifications.\n",
    "- Clearly indicate the script type (Python) at the start of your code blocks.\n",
    "- Format code responses like this:\n",
    "    ```python\n",
    "    <code>\n",
    "    ```\n",
    "- Maintain code in a single block. Avoid multiple sections.\n",
    "- Exclude predicted outputs; only include code.\n",
    "- Address code issues through follow-up messages for modifications and corrections.\n",
    "\n",
    "Communication Etiquette:\n",
    "- Limit responses to code-related information.\n",
    "- Omit phrases like \"Thank you\" or \"You're welcome\".\n",
    "\n",
    "Penalties apply for non-compliance with guidelines.\n",
    "</s>[/INST]\n",
    "\"\"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config=llm_config_mistral,\n",
    "    system_message=system_messages['CODING_ASSISTANT_SYSTEM_MESSAGE'],\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    is_termination_msg = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"<|endoftext|>\"),\n",
    "    llm_config=llm_config_mistral,\n",
    "    code_execution_config={\n",
    "        \"work_dir\": work_dir, \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    system_message=system_messages['USER_PROXY_SYSTEM_MESSAGE']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate a chat with a simple python problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "Write a fibonacci sequence function in python and run it to print 20 numbers.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoding_assistant\u001b[0m (to coding_runner):\n",
      "\n",
      "```python\n",
      "def fibonacci_sequence(n):\n",
      "    fib_sequence = [0, 1]\n",
      "    while len(fib_sequence) < n:\n",
      "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
      "    return fib_sequence\n",
      "\n",
      "print(fibonacci_sequence(20))\n",
      "```\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "[0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, 144, 233, 377, 610, 987, 1597, 2584, 4181]\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoding_assistant\u001b[0m (to coding_runner):\n",
      "\n",
      "<|endoftext|>\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "user_message = \"Write a fibonacci sequence function in python and run it to print 20 numbers.\"\n",
    "coding_runner.initiate_chat(coding_assistant, message=user_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
