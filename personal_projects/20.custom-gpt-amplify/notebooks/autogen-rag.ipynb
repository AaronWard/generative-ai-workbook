{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autogen Discord Embedding\n",
    "\n",
    "I'm to lazy to make an chroma db doc chunking and embedding function so just using an autogen agent that does that already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "sys.path.append('./')\n",
    "\n",
    "import openai\n",
    "import autogen\n",
    "import chromadb\n",
    "\n",
    "config_list = autogen.config_list_from_dotenv(dotenv_file_path='../../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "\n",
    "# Create an instance of RetrieveAssistantAgent\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config={\n",
    "        \"timeout\": 600,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "    },\n",
    ")\n",
    "\n",
    "client = chromadb.PersistentClient(path=f\"{os.getcwd()}/chromadb\")\n",
    "path = Path('../data/docs/')\n",
    "\n",
    "# Create an instance of RetrieveUserProxyAgent\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=10,\n",
    "    retrieve_config={\n",
    "        \"task\": \"default\",\n",
    "        \"docs_path\": str(path), \n",
    "        \"chunk_token_size\": 500,\n",
    "        \"model\": config_list[0][\"model\"],\n",
    "        \"client\": client, \n",
    "        \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "        \"collection_name\": \"autogen-docs-qa\"\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Delete dir each instantiation\n",
    "client.delete_collection('autogen-docs-qa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autogen.retrieve_utils:Found 151 chunks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_90', 'doc_89', 'doc_92', 'doc_47', 'doc_70', 'doc_2', 'doc_84', 'doc_16', 'doc_133', 'doc_135', 'doc_40', 'doc_57', 'doc_96', 'doc_134', 'doc_53', 'doc_107', 'doc_78', 'doc_68', 'doc_129', 'doc_136']]\n",
      "\u001b[32mAdding doc_id doc_90 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_89 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_92 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_47 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_70 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_2 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_84 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_16 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_133 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_135 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_40 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_57 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_96 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_134 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_53 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_107 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user. You should follow the following steps to answer a question:\n",
      "Step 1, you estimate the user's intent based on the question and context. The intent can be a code generation task or\n",
      "a question answering task.\n",
      "Step 2, you reply based on the intent.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "If user's intent is code generation, you must obey the following rules:\n",
      "Rule 1. You MUST NOT install any packages because all the packages needed are already installed.\n",
      "Rule 2. You must follow the formats below to write your code:\n",
      "```language\n",
      "# your code\n",
      "```\n",
      "\n",
      "If user's intent is question answering, you must give as short an answer as possible.\n",
      "\n",
      "User's question is: What is langchain?\n",
      "\n",
      "Context is: \n",
      "Question: What should I do if I receive an error while trying to continue a conversation in Autogen?\n",
      "Answer: To avoid restarting a conversation with `initiate_chat` that leads to losing previous context, you can use `send` or `initiate_chat(clear_history=False)` to carry on the conversation with the existing chat history.\n",
      "\n",
      "Question: How do you use SQLDatabaseToolkit from LangChain with Autogen?\n",
      "Answer: To leverage SQLDatabaseToolkit with Autogen, you can follow the example notebook provided:\n",
      "```url\n",
      "https://colab.research.google.com/gist/ElliotWood/af12566db5d6948e8ed6dd6324aa9697/autogen-langchain.ipynb\n",
      "```\n",
      "This should give you the ability to connect Autogen with SQL databases for complex queries.\n",
      "\n",
      "Question: What are multi-agent conversations in the context of Autogen?\n",
      "Answer: Multi-agent conversations in Autogen refer to complex workflows where multiple language model-based bots (agents) talk to each other and collaborate to solve a problem.\n",
      "\n",
      "Question: How can I add a SQL agent to Autogen?\n",
      "Answer: To add a SQL agent to Autogen, which can connect with a SQL database and perform complex queries, follow the example of using the SQLDatabaseToolkit from LangChain, integrating it within the Autogen framework.\n",
      "\n",
      "Question: How do you address the issue of token constraints in models like GPT-3.5 Turbo when using Autogen?\n",
      "Answer: One approach to handle verbose outputs and prevent exceeding the token limits in models like 3.5 Turbo is to filter the user_proxy's execution results to make them less verbose, thereby keeping the token length within limits. Specific advice or example code for this process was not provided in the given text.\n",
      "\n",
      "Question: What should I do if I find `pyautogen` in the pip list but `autogen` isn't being imported?\n",
      "Answer: If `pyautogen` is present in the pip list, you should be able to import `autogen` in the Python version where it‚Äôs listed. If errors persist, ensure compatibility and confirm that the correct module and version are installed.\n",
      "\n",
      "Question: How can I decide which LLM to use for each agent in AutoGen?\n",
      "Answer: For instance, if an agent is intended to perform coding tasks, you should choose an LLM that is proficient in coding, such as Code Llama for an assistant agent. The user proxy agent may not require an LLM if it's used to simulate user behavior. It's important to match the LLM to the agent's role and the tasks at hand.\n",
      "```\n",
      "Question: How can I improve my code for generating function schemas in LangChain?\n",
      "Answer: You can replace a less efficient code block with a better approach that utilizes the `args` property more effectively. Here‚Äôs a suggested improvement using the `generate_llm_config` function:\n",
      "```python\n",
      "# Define a function to generate llm_config from a LangChain tool\n",
      "def generate_llm_config(tool):\n",
      "    # Define the function schema based on the tool's args_schema\n",
      "    function_schema = {\n",
      "        \"name\": tool.name.lower().replace(' ', '_'),\n",
      "        \"description\": tool.description,\n",
      "        \"parameters\": {\n",
      "            \"type\": \"object\",\n",
      "            \"properties\": {},\n",
      "            \"required\": [],\n",
      "        },\n",
      "    }\n",
      "    if tool.args is not None:\n",
      "        function_schema[\"parameters\"][\"properties\"] = tool.args\n",
      "    return function_schema\n",
      "```\n",
      "\n",
      "Question: How can I fix the TypeError for non-string values in custom_tool?\n",
      "Answer: There's a note mentioning that if you use the custom_tool, a type error might occur for non-strings due to internal AutoGen Schema to function call issues. However, the text suggests it should be easy to fix, though specific details aren't provided.\n",
      "\n",
      "5. \n",
      "Question: Is there an example of using Autogen with multiple agents?\n",
      "Answer: A user requested a simple example of using Autogen with 3+ agents at a time, indicating a need for practical examples.\n",
      "\n",
      "6. \n",
      "Question: How do I get feature or tool agents connected with function calling?\n",
      "Answer: A user mentioned working on this with progress, using text files and function calls as a basic method.\n",
      "\n",
      "7. \n",
      "Question: What does `messages=[]` do for group chat in Autogen?\n",
      "Answer: The question was asked, but the text snippet does not provide an explicit answer to the functionality of `messages=[]`.\n",
      "\n",
      "8. \n",
      "Question: How do I get an agent to use the internet in Autogen?\n",
      "Answer: A user suggested that agents in Autogen could have internet access and mentioned testing it, but did not confirm a definitive method.\n",
      "\n",
      "9.\n",
      "Question: How can I integrate Langchain tools with Autogen?\n",
      "Answer: There was a discussion about writing a toolchain bridge to inherit Langchain tools for Autogen, though no explicit solution or code example was found in the snippet.\n",
      "\n",
      "10.\n",
      "Question: Has anyone tried out `llama2 long`?\n",
      "Answer: A user asked if anyone has tried `llama2 long`, but the snippet does not contain follow-up details or responses to the question.\n",
      "\n",
      "Question: How can one integrate AutoGen with GitHub?\n",
      "Answer: The text snippet does not provide a direct instruction, but it suggests using specific integration tools to connect AutoGen with GitHub services. One such toolkit might be found here: `https://python.langchain.com/docs/integrations/toolkits/github`. Using this toolkit, you can pass `toolkit.get_tools()` into the bridge for interaction with AutoGen agents.\n",
      "\n",
      "Question: Is it essential to use Anaconda for running AutoGen?\n",
      "Answer: No, it is not necessary to use Anaconda to run AutoGen. According to the discussion, you can run AutoGen using Google Colab, which implies that there are other viable environments where AutoGen can be executed.\n",
      "\n",
      "Question: What are some ways to deal with a longer context window when using language models?\n",
      "Answer: An approach to handle longer context windows is using techniques discussed in certain papers, such as FIRE (Functional Interpolation for Relative Position Encoding), which allows Transformers to handle longer inputs. However, these techniques might not be readily available in all language models, and their implementation specifics are not provided in the given text.\n",
      "Question: What is a good way to supplement or replace the need for fine-tuning in main program structures?\n",
      "Answer: Hierarchical graph interpretation of main program structure can be a good way to supplement or replace fine-tuning needs. An agent can represent the repository in a taxonomical graph, and use tests as semantically looked up examples in that graph structure to answer queries about working with a codebase.\n",
      "\n",
      "Question: How do people generally discover Autogen?\n",
      "Answer: Autogen is often discovered through various channels such as YouTube videos, tutorials, GitHub trends, and even social media platforms like Facebook groups dedicated to AI and programming.\n",
      "\n",
      "Question: What is the importance of fine-tuning in the context of programming?\n",
      "Answer: Fine-tuning in the context of programming is supposed to generate more consistently well-structured results from the intermediate steps in logic, aiding each individual piece to work better if fine-tuned for its respective prompt format.\n",
      "\n",
      "Question: How can you use a chatbot to interact with a GitHub repository?\n",
      "Answer: A chatbot like cody.sourcegraph.com can be used specifically for repo questioning, allowing you to query about the repository without recreating the wheel.\n",
      "\n",
      "Question: Is there a GitHub repository that could be a good starting point for someone looking into building a multi-agent system?\n",
      "Answer: Yes, a recommended starting point on GitHub for building a multi-agent system is https://github.com/amadad/agentcy.\n",
      "\n",
      "Question: Can AutoGen work with GPT-4, and how to configure the AI?\n",
      "Answer: While AutoGen has been working with GPT-3, there is interest in using it with GPT-4 as well. Configuration details may vary depending on AutoGen's compatibility with GPT-4.\n",
      "\n",
      "Question: How is local LLM (Large Language Model) support coming along with AutoGen?\n",
      "Answer: There isn't a built-in support for local LLMs in AutoGen yet, but some users have had success using the liteLLM proxy. Also, any local service that serves the OpenAI chat completions endpoint should be workable.\n",
      "\n",
      "Question: Can AutoGen be used for writing complex documents like contracts?\n",
      "Answer: While there is speculation, users are considering whether AutoGen can address complex documentation needs by using agents for different areas of law (commercial, common, state) and a UserProxyAgent to ensure contract parts meet the necessary standards.\n",
      "\n",
      "Question: How does one reduce token consumption when using AutoGen?\n",
      "Answer: You can reduce token consumption by instructing system prompts to \"answer in as few words as possible\", optimizing the verbosity of the processes.\n",
      "\n",
      "Question: Is there a router available that switches between different LLMs like GPT-3.5 and GPT-4?\n",
      "Answer: Yes, there is an LLM router in alpha phase that dynamically switches between models like GPT-3.5 and GPT-4. It can be found at https://github.com/opencopilotdev/llm-router.\n",
      "\n",
      "Question: How can I resolve issues with function configuration in user proxy?\n",
      "Answer: Avoid providing a function configuration directly to the user proxy. Instead, register functions with the user proxy and call them using `user_proxy.register_function`. Refer to documentation and working examples to clarify the setup:\n",
      "```\n",
      "user_proxy.register_function(...)\n",
      "```\n",
      "Refer to the provided example notebook for a working setup: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb\n",
      "```\n",
      "Question: How can I emulate a chat in LM Studio using AutoGen?\n",
      "Answer: You can interact with the model directly or use AutoGen for simulation. Here's an example provided by a user for obtaining a chat history screenshot and a text file by interacting with the same model using AutoGen:\n",
      "```\n",
      "1. Emulate a chat in LM Studio and take a screenshot of the interaction.\n",
      "2. Use AutoGen for the emulation and attach the corresponding text file alongside the code used to obtain it.\n",
      "```\n",
      "Question: What happens when `request_reply` is not set in AutoGen, and why do I still get replies as if it's set to `True`?\n",
      "Answer: If you don't set the `request_reply` parameter (default is `None`), you may still receive replies due to default behaviors or internal configurations within the AssistantAgent. To understand the exact difference between having it unset and explicitly setting it to True, you may need to refer to the documentation or raise an issue for clarification from the developers.\n",
      "\n",
      "Question: Where can I find resources and code examples for AutoGen?\n",
      "Answer: For AutoGen resources and code examples, you could explore GitHub repositories, YouTube tutorials, or community forums where developers share their experiences and code snippets. Look for tutorials that include step-by-step guides and code examples that are freely available for download and use as a starting point for your projects.\n",
      "Question: How does the `groupchat.py` choose the next speaker?\n",
      "Answer: From the provided text, it appears that `groupchat.py` selects the next speaker based on the name.\n",
      "\n",
      "Question: Is the RetrieveAssistantAgent necessary in the mentioned setup?\n",
      "Answer: The text suggests that the RetrieveAssistantAgent checks for the code execution result for termination condition, implying its use is specific to the requirements of code execution monitoring.\n",
      "\n",
      "Question: How should user feedback be integrated into an agent-assisted setup?\n",
      "Answer: Integrating user feedback could be implemented in the Admin agent with 'human_input_mode: ALWAYS' and the system message prompt guiding to ask user's feedback when hearing from agent A, as per the user's suggestion.\n",
      "\n",
      "Question: How can I use GPT-4v from ChatGPT and find the image upload button?\n",
      "Answer: There was a question posted about finding the image upload button on ChatGPT for using GPT-4v. However, the provided text does not contain a direct answer to this query.\n",
      "\n",
      "Question: How can I set up a web scraper combined with a vector database to enable agents to \"browse\" large webpages beyond their context length?\n",
      "Answer: The provided text does not contain a specific answer to this question. More context or a direct answer may be required.\n",
      "\n",
      "Question: How can I handle rate limit errors and timeout errors when using Autogen?\n",
      "Answer: For rate limit and timeout errors, refer to the Autogen FAQ at the following link:\n",
      "```\n",
      "https://microsoft.github.io/autogen/docs/FAQ/#handle-rate-limit-error-and-timeout-error\n",
      "```\n",
      "\n",
      "**Question: How do I enable logging in pyautogen to track the chat summary and usage?**\n",
      "Answer: In `pyautogen=v0.1`, you can start logging, print a usage summary, and access the logged history with the following code snippet:\n",
      "```python\n",
      "import autogen\n",
      "autogen.ChatCompletion.start_logging()\n",
      "# initialize the chat here\n",
      "autogen.ChatCompletion.print_usage_summary()\n",
      "# get summary of the cost and token usage from the chat\n",
      "autogen.ChatCompletion.logged_history\n",
      "# get all creation from the chat with token count and code\n",
      "autogen.ChatCompletion.stop_logging()\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "**Question: Can someone provide an example of using previous chat history as context in Autogen?**\n",
      "Answer: A user discussed putting the chat history to the group chat message property and also appending each message of the chat history to each agent on load as potential solutions.\n",
      "\n",
      "---\n",
      "\n",
      "**Question: If a list of models is passed to an agent, how does Autogen decide which model to assign to which agent?**\n",
      "Answer: When a list of models is passed, Autogen starts with the first model and if it encounters an error, it will go to the next one. You can also pass a specific model to each agent using a different `llm_config` with a single model for better specificity.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Is there a possibility of deploying Autogen with multiple programming languages like C# and C++?\n",
      "Answer: Someone mentioned the desire for support for more languages such as C# and C++, implying there's interest in using Autogen in a wider range of software development settings outside its current language capabilities.\n",
      "\n",
      "Question: Can Autogen be extended to function with budgeting and financial applications?\n",
      "Answer: There was a request for an agent that handles budgeting tasks, suggesting that Autogen could be applied in financial management systems to aid users in tracking and optimizing their expenses, though no specific implementation details were provided in the text.\n",
      "Question: Can I use your app in Portainer?\n",
      "Answer: Yes, you can run each app independently. However, you need to copy the env variables from the docker-compose into independent.env files and use other setup steps based on what you are deploying.\n",
      "\n",
      "Question: Do I need Docker to run AgentCloud?\n",
      "Answer: No, Docker is not the only way to run AgentCloud. You can set it up in a venv type setup if you prefer to stay away from Docker.\n",
      "\n",
      "Question: How do I set up the agent-backend component?\n",
      "Answer: For the agent-backend component you‚Äôll first need to do `poetry install` then run the application with `poetry run python3 main.py`.\n",
      "\n",
      "Question: How do I set up the webapp component?\n",
      "Answer: For setting up the webapp component, you need to run the commands `npm install` followed by `npm run dev`.\n",
      "\n",
      "Question: How do I set up the AgentCloud or associated apps if there are no install instructions?\n",
      "Answer: Install instructions vary for each component of the system. For the webapp and the agent-backend, specific setup steps have been mentioned such as using `npm` commands and `poetry`. For more detailed instructions, it would be best to check the repository documentation or reach out for direct support.\n",
      "\n",
      "Question: What URL can I find more information about validating a b2b SaaS idea?\n",
      "Answer: You can visit `https://loveb2bsaas.discoze.com/` for an app that helps validate your b2b SaaS idea.\n",
      "\n",
      "Question: Is there any graphical user interface (GUI) available for autogen configurations?\n",
      "Answer: As of the knowledge cutoff date, there is no specific mention of an available GUI for autogen configs in the provided text. \n",
      "\n",
      "Question: How can I log prompts used in language models for debugging purposes?\n",
      "Answer: AutoGen's built-in logging mechanism can be used for this purpose, which includes logging system messages and conversation histories. For detailed instructions, visit https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#logging-experimental. \n",
      "\n",
      "Question: Will there be a TypeScript implementation for AutoGen?\n",
      "Answer: The text mentions an interest in a TypeScript implementation, but no definitive plans or availability are described within the given dialogue.\n",
      "\n",
      "Question: Is there a way to access a global database with vectorized knowledge, skills, and tools for knowledge iteration and curation?\n",
      "Answer: The text mentions an interest in such a global database for accelerating knowledge and agency compounding, but no specific solution or example is provided.\n",
      "\n",
      "Question: How can I build an assistant agent that interacts with APIs like Google Trends or SEMrush?\n",
      "Answer: To build an assistant agent that interacts with APIs, one would need to implement the capability to fetch and process data from these APIs. The dialogue suggests a user seeking advice, but no clear instructions are given in the provided text.\n",
      "\n",
      "Question: Can AutoGen update and refer to a local database created for tracking financial transactions?\n",
      "Answer: Trader_pt expressed a need for an agent that could track financial transfers and store information in a local database. While autogen's potential use for the task is being discussed, no definitive answer is presented in the text provided.\n",
      "\n",
      "Question: Are there any existing marketplaces or repositories to discover agents built with the AutoGen framework?\n",
      "Answer: There is an expressed interest in a marketplace for agents built with autogen, suggesting that one might be created, but no specific marketplace or repository is confirmed in the provided dialogue.\n",
      "Question: How can latency and integration issues with AI models in video games be addressed?\n",
      "Answer: At an Audio Engineering Society conference, challenges such as latency, database integration, model training, and customization were discussed. Latency is especially problematic for hardware-based models and cloud-based generative NLPs, with inferencing hoped to eventually happen client-side in future game consoles. Model memory of game events and player interactions was also a focus, along with dynamic narratives using AutoGen, despite it exacerbating latency and computational challenges.\n",
      "\n",
      "Question: How can I run pre-trained GPT models with Botpress?\n",
      "Answer: Although the text snippet does not provide a direct answer, generally, running pre-trained GPT models with Botpress involves configuring the Botpress environment to use AI models, potentially by using APIs that allow the GPT model to communicate with the Botpress platform.\n",
      "\n",
      "Question: What does registering replies mean in Autogen's context, and how is it utilized?\n",
      "Answer: Registering replies in Autogen allows customization of agent behavior. Each reply function represents a way an agent can respond, allowing agents implemented with new reply functions to behave in tailored ways. For example:\n",
      "```\n",
      "@register_reply('my_custom_reply')\n",
      "def custom_reply_function(agent_interface, message):\n",
      "    # Implementation of custom behavior.\n",
      "```\n",
      "This enables the agent to reply according to the custom behavior defined in the function.\n",
      "Question: What are some ways to perform video/audio to text conversion locally?\n",
      "Answer: You can use OpenAI's Whisper, which can be installed locally and used for free, or employ services like Assembly.ai or Deepgram for a fee.\n",
      "\n",
      "Question: What should I do if I'm running out of GPU RAM while trying to run a medium model?\n",
      "Answer: If you encounter GPU RAM limitations, you may need to switch to using smaller models or expand your system's memory resources if possible.\n",
      "\n",
      "Question: Can I run Autogen through Jupyter?\n",
      "Answer: Yes, you can run Autogen through Jupyter, but if you're having trouble importing pyautogen, make sure you have properly installed the required package and that your environment is configured correctly.\n",
      "\n",
      "Question: What security features should be considered when using Autogen in an enterprise setting?\n",
      "Answer: When implementing Autogen in an enterprise, it's recommended to include robust security features such as permission boundaries similar to AWS IAM roles, groups, or Microsoft AD. It should also include cybersec auditing and conform to security frameworks like NIST and ISO 27K.\n",
      "\n",
      "Question: What are the recommended methods for local video or audio to text transcription?\n",
      "Answer: For local transcription, it is recommended to use solutions like OpenAI's Whisper or other similar tools that can be installed locally for batch conversions.\n",
      "\n",
      "Question: What would the new version of the openai-python release affect on tools like AutoGen and Langchain?\n",
      "Answer: The new version of openai-python could impact tools like AutoGen and Langchain primarily in terms of interface compatibility and the handling of new or deprecated features. It's expected that only the openai interface module in AutoGen will need updating to maintain compatibility, and the changes should not be convoluted. However, the exact effects would depend on the update details, which can be found in the release notes or documentation provided by OpenAI: https://github.com/openai/openai-python/discussions/631\n",
      "\n",
      "Question: How can a team of agents provide a summary of an article from a provided URL?\n",
      "Answer: To have a team of agents provide a summary of an article from a URL, one proposed setup involves using a combination of different agents like an Assistant agent, Coding agent, Summary agent, and User proxy. However, the effectiveness of results can vary, and it might take experimentation with different setups to achieve desired outcomes. When contemplating caching versus learning new skills, it's essential to strike a balance between quick retrieval of information and adapting to new tasks by learning new abilities.\n",
      "\n",
      "Question: Is AutoGPT compatible with locally run models, such as zephyr or mistral 7b?\n",
      "Answer: The compatibility of AutoGPT with locally run models such as zephyr or mistral 7b depends on the specific version and configuration of AutoGPT you are using. To get it to work, you would typically need to adapt AutoGPT to interact with the local API served by the local model instead of making HTTP requests to a remote API.\n",
      "\n",
      "Question: What kind of support is available for running AutoGen code in VS Code?\n",
      "Answer: If you encounter an error such as \"Function python not found\" while running AutoGen code in VS Code, ensure that your development environment is correctly set up with the necessary dependencies and that the Python interpreter is correctly configured in VS Code. Additionally, double-check the code and the repository for issues: https://github.com/meetrais/AutoGen/blob/main/tools_as_functions.py\n",
      "\n",
      "assistant = AssistantAgent(\n",
      "    name=\"assistant\",\n",
      "    llm_config=llm_config,\n",
      ")\n",
      "\n",
      "# Similarly, configure the UserProxyAgent if needed\n",
      "```\n",
      "Ensure to update llm_config with the details of your chosen LLM.\n",
      "Question: How can I configure AutoGen to use local large language models (LLMs) like llama-7B or Mistral-7B instead of the OpenAI API?\n",
      "Answer: To configure AutoGen to use local LLMs such as llama-7B or Mistral-7B, you'll need to set up a local server and adjust your configuration parameters. An example configuration in Python would resemble the following:\n",
      "\n",
      "Question: Where can I find the workflow for creating teams within Autogen?\n",
      "Answer: The information about the team creation workflow can be found within the GitHub repository for AgentCloud. Specifically, you can refer to `https://github.com/rnadigital/agentcloud/blob/master/agent-backend/src/config/base.json` for the initial starting point of agents.\n",
      "\n",
      "Question: How do I run AgentCloud with a custom or local language model?\n",
      "Answer: The platform is currently working on enabling the choice of using a local/custom language model, but for specific instructions and current capabilities, it would be best to consult the latest version or direct documentation from the development team.\n",
      "\n",
      "Question: Are there any examples of integrating Dal√≠ or other image generation tools?\n",
      "Answer: There are inquiries about samples with Dall-E and other image generation tools integrated, and while they aren't provided in the extracted conversation, users are recommended to monitor relevant channels and documentation for updates on these integrations.\n",
      "\n",
      "Question: How do I resolve the issue where the manager component doesn't select the correct agent in Autogen?\n",
      "Answer: A recent update that addresses this issue has been pushed. Users should pull the latest version of the application or library in question. If the issue persists, one can contact the support or development team directly for assistance.\n",
      "Question: How does the removal of stopwords affect the model's output in language processing tasks?\n",
      "Answer: The removal of stopwords can result in up to a 30% reduction in tokens and has almost no effect on the model's output if the model is pre-prompted to acknowledge that stopwords have been removed. This suggests that stopwords can be effectively removed without significantly impacting the quality of the model's output.\n",
      "\n",
      "Question: Are there examples of projects with frontends integrating with autogen?\n",
      "Answer: Yes, for examples of projects with frontends that integrate with autogen, see Tutorials and Examples section.\n",
      "\n",
      "Question: What could be the challenges of building a web app for interfacing with autogen?\n",
      "Answer: Building a web app for interfacing with autogen might include challenges such as handling authentication and API keys, ensuring robust interaction with the autogen backend, and managing tasks in a user-friendly manner. A simple demo is available but may have issues, as encountered at https://huggingface.co/spaces/thinkall/autogen-demos.\n",
      "\n",
      "Question: How can I save my conversations in Autogen for later use?\n",
      "Answer: The Python `pickle` package can be used to serialize and deserialize objects, allowing you to save conversations to disk and reload them. An example from the text would be:\n",
      "```\n",
      "import pickle\n",
      "\n",
      "# Save an object to disk\n",
      "with open('conversation.pkl', 'wb') as outp:\n",
      "    pickle.dump(conversation_object, outp, pickle.HIGHEST_PROTOCOL)\n",
      "\n",
      "# Load the object back from disk\n",
      "with open('conversation.pkl', 'rb') as inp:\n",
      "    loaded_conversation = pickle.load(inp)\n",
      "```\n",
      "\n",
      "Question: What setup was mentioned for running Autogen locally?\n",
      "Answer: One user mentioned a working configuration of Autogen on their workstation, which included a local LLM, mistral-7b-instruct_v0.1. Q6_K.gguf model, and a llama-cpp-python server on a Jetson 16 GB Xavier AGX, with a context length of 12K tokens.\n",
      "\n",
      "Question: Where can I find information to get started with Autogen?\n",
      "Answer: Getting started information, tutorials, and examples for Autogen can be found on the official documentation site, GitHub repository, and specific Discord channels.\n",
      "\n",
      "Question: How can I deal with a `InternalServerError` when using Autogen?\n",
      "Answer: Check your requests to make sure they are correctly formed. Error 500 usually indicates a server-side error, but malformed requests might also trigger such errors. Turn on logging to get more information about the error source.\n",
      "\n",
      "Question: How can I save the conversation in Autogen and pick up where I left off?\n",
      "Answer: To save the conversation in Autogen, you can use the Python `pickle` package to serialize the conversation object to disk, and then deserialize it when you want to continue.\n",
      "\n",
      "Question: Is there a notebook available that demonstrates the use of OpenAI's Assistant API with AutoGen?\n",
      "Answer: Yes, there is an example notebook available here: https://github.com/AaronWard/generative-ai-workbook/blob/main/personal_projects/14.openai-assistant-api/OpenAi-assistant-with-autogen.ipynb.\n",
      "\n",
      "Question: What is a good starting point for creating a custom Autogen project?\n",
      "Answer: A recommended starting point for an Autogen project could be exploring existing notebooks, like the one for Observian AutoGen project: https://github.com/denonrailz/obsidian-autogen. It aims to create an accessible framework for tweaking, monitoring, and evaluating AutoGen agents.\n",
      "\n",
      "Question: Where can I find Autogen's documentation on implementing function calling within scripts?\n",
      "Answer: Autogen's documentation and examples of function calling can be found in their GitHub repository, which includes a notebook illustrating the usage: https://github.com/microsoft/autogen/blob/main/notebook/agentchat_function_call.ipynb.\n",
      "\n",
      "Question: Is there an example of AutoGen handling more complex queries, such as reducing answer times in a classroom setting?\n",
      "Answer: Yes, there are examples and discussions around optimizing AutoGen for different scenarios, including reducing answer times in an online education multi-person classroom. You can follow this pull request for updates and strategies: https://github.com/microsoft/autogen/pull/491.\n",
      "\n",
      "Question: How can I get started with AgentCloud, an open source UI for running Autogen?\n",
      "Answer: To get started with AgentCloud for running Autogen, you can refer to its implementation here: https://agentcloud.dev, which provides an open source UI interface.\n",
      "\n",
      "Question: What are some considerations and available tools for dealing with token limit issues in Autogen?\n",
      "Answer: For dealing with token limit issues in Autogen, you can try the CompressibleAgent or MemGPT agent and share experiences or feedback in the Autogen GitHub discussions: https://github.com/microsoft/autogen/discussions/561.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "Langchain appears to be a toolkit mentioned alongside Autogen in the context provided. However, the exact identity, function, or features of \"langchain\" are not described in the given context.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# The collections don't get created until the chat is initated üòê\n",
    "ragproxyagent.initiate_chat(assistant, problem=\"What is langchain?\", clear_history=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragproxyagent._retrieve_config['client'].list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Collection(name=autogen-docs-qa)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
