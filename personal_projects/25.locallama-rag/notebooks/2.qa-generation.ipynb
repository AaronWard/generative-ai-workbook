{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Generation\n",
    "\n",
    "Last time i did this it cost about $10 to create question:answer pairs, this time im gonna use local models to it using ollama.\n",
    "This will iterate over the posts extracted from r/localllama and generate a QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The color of the sky appears blue due to a phenomenon called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with molecules and particles in the air, such as nitrogen and oxygen. Blue light has a shorter wavelength and gets scattered more easily than other colors in the visible spectrum. As a result, when we look up at the sky, we predominantly see the blue light that has been scattered, giving the sky its characteristic blue hue during a clear day. However, at sunrise or sunset, the sky can take on various shades of red, orange, and purple as the sunlight interacts with more particles in the atmosphere and the angle of the sun's rays changes.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "response = ollama.chat(model='mistral:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "    'temperature': 0.01,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 975 questions in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Post ID: 1at0288\\nTitle: Ok, which one of you was this? ðŸ¤£ðŸ¤£ðŸ¤£\\nLink: https://redd.it/1at0288\\nContent: \\nReplies:\\n- No, I don't think OpenAI would ever allow porn to be generated. I rather think that copies of Sora, recreated open source image generators will appear and fullfill this task. Porn is always one of the first use cases in any technologie that appeared and I don't think it'll take long for the industry to hop into this new tech. This is good for us as it further pushes open source AI technology for any use case.\\n\\n\",\n",
       " ' 1aszy6f\\nTitle: What are your favorite resources for evaluating text generation for stuff like readability, engagement (and other \"soft\" metrics)\\nLink: https://redd.it/1aszy6f\\nContent: Hi everyone, i\\'m working on a thesis looking at different prompt engineering methods and trying to evaluate the quality of generated content for stuff like articles, newsletters = human read content. Most research focuses on stuff like factuality, reasoning but I\\'m trying to find more research or these \"soft\" metrics. Any resources welcome, academic papers preferred. Many thanks!\\n\\n',\n",
       " ' 1aszspr\\nTitle: State of Opensource and Closedsource as of right now:\\nLink: https://redd.it/1aszspr\\nContent: What are the best opensource and closedsourced AI models?\\n\\nWhat will be the best opensource and closedsource AI models at the end of this year?\\n\\nList as per the following categories:\\n\\n1. Large Language Models:\\n\\n2. Large Multimodal Models:\\n\\n3. Text to image: \\n\\n4. Text to video: \\n\\n5. Text to 3D:\\n\\n6. Text to audio:\\n\\n7. Audio to text (transcribing):\\n\\n8. General purpose robots powered by Large multimodal models:\\n\\n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./_output/new/localllama-new-17-02-2024.txt\"\n",
    "with open(DATA_PATH, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_chunks = data.split(\"---\\nPost ID:\")\n",
    "\n",
    "print(f\"There are {len(data_chunks)} questions in total\")\n",
    "data_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating QA Pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 975/975 [4:45:06<00:00, 17.55s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response_chunks = []\n",
    "\n",
    "print(\"Generating QA Pairs...\")\n",
    "for chunk in tqdm.tqdm(data_chunks):\n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {chunk}\n",
    "    ```\n",
    "    \\n\n",
    "    Your job is to look at this reddit post and to produce several question/answer pairs based on the content provided. \n",
    "    Look at the replies also and try to extract informative technical information. \n",
    "    Do not produce QA pairs for anything that is not in the provided text. \n",
    "    For longer posts (such as ones with a lot of information in the content or with many comments) produce a lot of QA pairs. \n",
    "    For posts with less content, produce fewer. Only include QA pairs with general useful information. \n",
    "    Do not include anything that could be considered personal information, opinion, or conversational text. \n",
    "    Only provide the QA pairs. Do NOT provide introductions or conclusions. Write your answer in this format:\n",
    "\n",
    "    ```\n",
    "    Q: What is the colour of the sky?\n",
    "    A: The colour of the sky is blue.\n",
    "    ---\n",
    "    Q: How old is OpenAI? \n",
    "    A: OpenAI was founded in 2015, therefore it is 8 years old.\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    response_chunks.append(\n",
    "        ollama.chat(model='mistral:latest', messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "            'temperature': 0.2},\n",
    "    ]))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Q: What model is the user having trouble with in LM Studio?\\n'\n",
      " 'A: The Mixtral_11Bx2_MoE_19B-GGUF model.\\n'\n",
      " '---\\n'\n",
      " 'Q: Where can the user find the Model card for Mixtral_11Bx2_MoE_19B-GGUF?\\n'\n",
      " 'A: The user can find the Model card on Hugging Face at this link: '\n",
      " '<https://huggingface.co/TheBloke/Mixtral_11Bx2_MoE_19B-GGUF>.\\n'\n",
      " '---\\n'\n",
      " 'Q: What does the user see in Models card for the Mixtral_11Bx2_MoE_19B-GGUF '\n",
      " 'model?\\n'\n",
      " 'A: The user only sees that there is no prompt template specified and a blank '\n",
      " '{prompt} field.\\n'\n",
      " '---\\n'\n",
      " 'Q: What is the default preset for LM Studio not working well for this '\n",
      " 'model?\\n'\n",
      " 'A: It is not clear what the default preset for LM Studio is, as it is not '\n",
      " 'mentioned in the post.\\n'\n",
      " '---\\n'\n",
      " 'Q: What does Gemini suggest as a prompt template for the user in the JSON '\n",
      " 'format?\\n'\n",
      " 'A: Gemini suggests using \"{prompt}\" as the prompt template.\\n'\n",
      " '---\\n'\n",
      " 'Q: What should the user replace \"/path/to/model/directory\" with in the '\n",
      " 'JSON?\\n'\n",
      " 'A: The user should replace \"/path/to/model/directory\" with their actual '\n",
      " 'model directory path.')\n"
     ]
    }
   ],
   "source": [
    "import pprint as pp\n",
    "pp.pprint(response_chunks[3]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1aszfil\\nTitle: LM Studio prompt settings for Mixtral 11Bx2 MoE 19B GGUF?\\nLink: https://redd.it/1aszfil\\nContent: Hello friends.\\n\\nI downloaded this model, but I have a problem with Its prompt format.\\n\\n[TheBloke/Mixtral\\\\_11Bx2\\\\_MoE\\\\_19B-GGUF Â· Hugging Face](https://huggingface.co/TheBloke/Mixtral_11Bx2_MoE_19B-GGUF)\\n\\nIn Models card I can only see this:\\n\\nPrompt template: None\\n\\n{prompt}\\n\\nBut I think in LM Studio I\\'m forced to select a preset. Default preset for LM Studio is not working well.\\n\\nCan you help me please?\\n\\nThank you\\n\\nEdit:\\n\\nGemini gave me this. Do you think it\\'s OK?\\n\\n  \\n\\nJSON\\n\\n{\\n\\n\"model\\\\_name\": \"Mixtral\\\\_11Bx2\\\\_MoE\\\\_19B-GGUF\",\\n\\n\"model\\\\_path\": \"/path/to/model/directory\", # Replace with your actual model path\\n\\n\"prompt\\\\_template\": \"{prompt}\", # No specific prompt template needed based on model card\\n\\n\"batch\\\\_size\": 1,\\n\\n\"sequence\\\\_length\": 2048,\\n\\n\"temperature\": 0.7,\\n\\n\"top\\\\_p\": 0.9,\\n\\n\"sampling\\\\_method\": \"nucleus\",\\n\\n\"nucleus\\\\_p\": 0.9,\\n\\n\"no\\\\_repeat\\\\_ngram\\\\_size\": 2,\\n\\n\"num\\\\_beams\": 1,\\n\\n\"early\\\\_stopping\": True,\\n\\n\"max\\\\_length\": 512\\n\\n}\\nReplies:\\n- Are you running the latest lm studio\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pickle' has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./_output/response1.pickle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(response_chunks, file) \n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pickle' has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('./_output/response1.pkl', \"wb\") as file:\n",
    "    pickle.dump(response_chunks, file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
