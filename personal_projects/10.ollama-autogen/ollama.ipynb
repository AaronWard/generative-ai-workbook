{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM, Ollama and Autogen\n",
    "\n",
    "**LiteLLM** provides a simplified interface to interact with language models, offering synchronous and asynchronous text generation capabilities, and supporting streaming outputs for efficient generation of longer texts.\n",
    "\n",
    "**Ollama** lets you run, manage, and interact with a variety of LLMs locally, allowing for enhanced control, customization, and application of language models without the necessity for constant internet connectivity once models are downloaded and stored locally.\n",
    "\n",
    "Being able to leverage all these together, will help reduce cost as you work with autogen, as you don't need to make API calls to a provider. \n",
    "Also a nice bonus, no API keys.\n",
    "\n",
    "---\n",
    "Links:\n",
    "- [Ollama](https://ollama.ai/)\n",
    "- [LiteLLM](https://litellm.ai/)\n",
    "- [Autogen opensource Capability - PR](https://github.com/microsoft/autogen/pull/95)\n",
    "\n",
    "Here all the different models Ollama supports, so you can use this and play around will all different model types:\n",
    "- [Link](https://ollama.ai/library)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in the terminal:\n",
    ">  litellm --model ollama/llama2 --api_base http://localhost:11434/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion\n",
    "\n",
    "response = completion(\n",
    "    model=\"ollama/mistral\", \n",
    "    messages=[{ \"content\": \"Tell me a story about llamas.\", \"role\": \"user\"}], \n",
    "    api_base=\"http://localhost:11434\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "clear_cache()\n",
    "\n",
    "config_list = [\n",
    "    {\n",
    "        # \"model\": \"ollama/mistral\",\n",
    "        \"model\": \"ollama/llama2\",\n",
    "        \"api_base\": \"http://0.0.0.0:8000\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "import pprint as pp\n",
    "pp.pprint(config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.8,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": config_list,\n",
    "        \"temperature\": 0.4,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key.\")\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Multiple Models\n",
    "\n",
    "> litellm --model ollama/llama2 --api_base http://localhost:11434/ \\\n",
    "> litellm --model codellama:7b-code --api_base http://localhost:11434/ --port 8001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting cache...\n",
      "[{'api_base': 'http://0.0.0.0:8000',\n",
      "  'api_type': 'litellm',\n",
      "  'model': 'ollama/mistral'}]\n",
      "[{'api_base': 'http://0.0.0.0:8001',\n",
      "  'api_type': 'litellm',\n",
      "  'model': 'ollama/codellama'}]\n",
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoding_assistant\u001b[0m (to coding_runner):\n",
      "\n",
      " To calculate the percentage gain YTD for Berkshire Hathaway stock, you can follow these steps:\n",
      "1. Go to the website of a financial news or information service (e.g., Google Finance, Yahoo Finance) and enter \"Berkshire Hathaway\" in the search bar.\n",
      "2. Click on the \"Price\" tab and find the current price of Berkshire Hathaway stock. This is the price at which you will later compare with the previous closing price to calculate the percentage gain YTD.\n",
      "3. Now, go back to the financial news or information service website and click on the \"Historical Data\" tab. Select \"Daily\" as the time period and enter \"Berkshire Hathaway\" in the search bar.\n",
      "4. Find the previous closing price of Berkshire Hathaway stock by scrolling through the historical data table. Record this value for later reference.\n",
      "5. Now, go back to the financial news or information service website and find the current price of Berkshire Hathaway stock again. This is the price at which you will calculate the percentage gain YTD.\n",
      "6. Calculate the percentage gain YTD by dividing the difference between the current price and the previous closing price by the previous closing price, and multiplying the result by 100. For example:\n",
      "Suppose the current price of Berkshire Hathaway stock is $500, and its previous closing price was $450. Then, the percentage gain YTD would be calculated as follows:\n",
      "$$\\text{Percentage Gain YTD} = \\frac{\\$500 - \\$450}{\\$450} \\times 100 = 6.67\\%$$\n",
      "7. Once you have calculated the percentage gain YTD for Berkshire Hathaway stock, you can use a graphing tool or create a plot using your preferred programming language to visualize the result and present it as a graphical representation of the price of Berkshire Hathaway stock over time.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mcoding_runner\u001b[0m (to coding_assistant):\n",
      "\n",
      "### System:\n",
      "The percentage gain YTD for Berkshire Hathaway stock can be calculated by following these steps:\n",
      "1. Go to the website of a financial news or information service (e.g., Google Finance, Yahoo Finance) and enter \"Berkshire Hathaway\" in the search bar.\n",
      "2. Click on the \"Price\" tab and find the current price of Berkshire Hathaway stock. This is the price at which you will later compare with the previous closing price to calculate the percentage gain YTD.\n",
      "3. Now, go back to the financial news or information service website and click on the \"Historical Data\" tab. Select \"Daily\" as the time period and enter \"Berkshire Hathaway\" in the search bar.\n",
      "4. Find the previous closing price of Berkshire Hathaway stock by scrolling through the historical data table. Record this value for later reference.\n",
      "5. Now, go back to the financial news or information service website and find the current price of Berkshire Hathaway stock again. This is the price at which you will calculate the percentage gain YTD.\n",
      "6. Calculate the percentage gain YTD by dividing the difference between the current price and the previous closing price by the previous closing price, and multiplying the result by 100. For example:\n",
      "Suppose the current price of Berkshire Hathaway stock is $500, and its previous closing price was $450. Then, the percentage gain YTD would be calculated as follows:\n",
      "$$\\text{Percentage Gain YTD} = \\frac{\\$500 - \\$450}{\\$450} \\times 100 = 6.67\\%$$\n",
      "7. Once you have calculated the percentage gain YTD for Berkshire Hathaway stock, you can use a graphing tool or create a plot using your preferred programming language to visualize the result and present it as a graphical representation of the price of Berkshire Hathaway stock over time.\n",
      "### Response:\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m coding_assistant \u001b[39m=\u001b[39m AssistantAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_assistant\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     llm_config\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m coding_runner \u001b[39m=\u001b[39m UserProxyAgent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcoding_runner\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     human_input_mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNEVER\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     },\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m coding_runner\u001b[39m.\u001b[39;49minitiate_chat(coding_assistant, message\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mCalculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mt use any approach that requires an API key.\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/award40/Desktop/personal/github/generative-ai-workbook/personal_projects/10.ollama-autogen/ollama.ipynb#X11sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# clear_cache(clear_previous_work=False)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:531\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, **context)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Initiate a chat with the recipient agent.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \n\u001b[1;32m    519\u001b[0m \u001b[39mReset the consecutive auto reply counter.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39m        \"message\" needs to be provided if the `generate_init_message` method is not overridden.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_chat(recipient, clear_history)\n\u001b[0;32m--> 531\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_init_message(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcontext), recipient, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:464\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_reply(messages\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_messages[sender], sender\u001b[39m=\u001b[39msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(reply, sender, silent\u001b[39m=\u001b[39;49msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:334\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    332\u001b[0m valid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append_oai_message(message, \u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, recipient)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m valid:\n\u001b[0;32m--> 334\u001b[0m     recipient\u001b[39m.\u001b[39;49mreceive(message, \u001b[39mself\u001b[39;49m, request_reply, silent)\n\u001b[1;32m    335\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    336\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    337\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mMessage can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    338\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:462\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m request_reply \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreply_at_receive[sender] \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> 462\u001b[0m reply \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_reply(messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchat_messages[sender], sender\u001b[39m=\u001b[39;49msender)\n\u001b[1;32m    463\u001b[0m \u001b[39mif\u001b[39;00m reply \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(reply, sender, silent\u001b[39m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:779\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, exclude)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[39m\"\u001b[39m\u001b[39mtrigger\u001b[39m\u001b[39m\"\u001b[39m], sender):\n\u001b[0;32m--> 779\u001b[0m     final, reply \u001b[39m=\u001b[39m reply_func(\u001b[39mself\u001b[39;49m, messages\u001b[39m=\u001b[39;49mmessages, sender\u001b[39m=\u001b[39;49msender, config\u001b[39m=\u001b[39;49mreply_func_tuple[\u001b[39m\"\u001b[39;49m\u001b[39mconfig\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m final:\n\u001b[1;32m    781\u001b[0m         \u001b[39mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:606\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m    603\u001b[0m     messages \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_oai_messages[sender]\n\u001b[1;32m    605\u001b[0m \u001b[39m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m--> 606\u001b[0m response \u001b[39m=\u001b[39m oai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    607\u001b[0m     context\u001b[39m=\u001b[39;49mmessages[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcontext\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m), messages\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_oai_system_message \u001b[39m+\u001b[39;49m messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mllm_config\n\u001b[1;32m    608\u001b[0m )\n\u001b[1;32m    609\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m, oai\u001b[39m.\u001b[39mChatCompletion\u001b[39m.\u001b[39mextract_text_or_function_call(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:790\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    788\u001b[0m     base_config[\u001b[39m\"\u001b[39m\u001b[39mmax_retry_period\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 790\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m    791\u001b[0m         context,\n\u001b[1;32m    792\u001b[0m         use_cache,\n\u001b[1;32m    793\u001b[0m         raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mi \u001b[39m<\u001b[39;49m last \u001b[39mor\u001b[39;49;00m raise_on_ratelimit_or_timeout,\n\u001b[1;32m    794\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbase_config,\n\u001b[1;32m    795\u001b[0m     )\n\u001b[1;32m    796\u001b[0m     \u001b[39mif\u001b[39;00m response \u001b[39m==\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    797\u001b[0m         \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:821\u001b[0m, in \u001b[0;36mCompletion.create\u001b[0;34m(cls, context, use_cache, config_list, filter_func, raise_on_ratelimit_or_timeout, allow_format_str_template, **config)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[39mwith\u001b[39;00m diskcache\u001b[39m.\u001b[39mCache(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcache_path) \u001b[39mas\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_cache:\n\u001b[1;32m    820\u001b[0m     \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mset_cache(seed)\n\u001b[0;32m--> 821\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_response(params, raise_on_ratelimit_or_timeout\u001b[39m=\u001b[39;49mraise_on_ratelimit_or_timeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/autogen/oai/completion.py:216\u001b[0m, in \u001b[0;36mCompletion._get_response\u001b[0;34m(cls, config, raise_on_ratelimit_or_timeout, use_cache)\u001b[0m\n\u001b[1;32m    214\u001b[0m api_type \u001b[39m=\u001b[39m config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mapi_type\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m \u001b[39mif\u001b[39;00m api_type \u001b[39mand\u001b[39;00m re\u001b[39m.\u001b[39msub(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m[^a-zA-Z0-9]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m, api_type)\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlitellm\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 216\u001b[0m     response \u001b[39m=\u001b[39m litellm\u001b[39m.\u001b[39;49mcompletion(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mconfig)\n\u001b[1;32m    217\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     response \u001b[39m=\u001b[39m openai_completion\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig)\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/utils.py:700\u001b[0m, in \u001b[0;36mclient.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[39mreturn\u001b[39;00m cached_result\n\u001b[1;32m    699\u001b[0m \u001b[39m# MODEL CALL\u001b[39;00m\n\u001b[0;32m--> 700\u001b[0m result \u001b[39m=\u001b[39m original_function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    701\u001b[0m end_time \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\n\u001b[1;32m    702\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39m# TODO: Add to cache for streaming\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/site-packages/litellm/timeout.py:53\u001b[0m, in \u001b[0;36mtimeout.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m     local_timeout_duration \u001b[39m=\u001b[39m kwargs[\u001b[39m\"\u001b[39m\u001b[39mrequest_timeout\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     result \u001b[39m=\u001b[39m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mlocal_timeout_duration)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m futures\u001b[39m.\u001b[39mTimeoutError:\n\u001b[1;32m     55\u001b[0m     thread\u001b[39m.\u001b[39mstop_loop()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/concurrent/futures/_base.py:453\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m    451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[0;32m--> 453\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    456\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/envs/masterclass/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import autogen\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "def clear_cache(clear_previous_work=True):\n",
    "    # Function for cleaning up cash to\n",
    "    # avoid potential spill of conversation\n",
    "    # between models\n",
    "\n",
    "    # Should be run before and after each chat initialization\n",
    "    if os.path.exists('.cache') and os.path.isdir('.cache'):\n",
    "        print('deleting cache...')\n",
    "        shutil.rmtree('.cache')\n",
    "clear_cache()\n",
    "\n",
    "proxy_config_list = [\n",
    "    {\n",
    "        \"model\": \"ollama/mistral\",\n",
    "        # \"model\": \"ollama/llama2\",\n",
    "        \"api_base\": \"http://0.0.0.0:8000\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "coding_config_list = [\n",
    "    {\n",
    "        # \"model\": \"ollama/mistral\",\n",
    "        \"model\": \"ollama/codellama\",\n",
    "        \"api_base\": \"http://0.0.0.0:8001\",\n",
    "        \"api_type\": \"litellm\"\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "import pprint as pp\n",
    "pp.pprint(proxy_config_list)\n",
    "pp.pprint(coding_config_list)\n",
    "\n",
    "coding_assistant = AssistantAgent(\n",
    "    name=\"coding_assistant\",\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": coding_config_list,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "\n",
    "coding_runner = UserProxyAgent(\n",
    "    name=\"coding_runner\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=20,\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    code_execution_config={\n",
    "        \"work_dir\": \"coding\",\n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    llm_config={\n",
    "        \"request_timeout\": 1000,\n",
    "        \"seed\": 42,\n",
    "        \"config_list\": proxy_config_list,\n",
    "        \"temperature\": 0.1,\n",
    "    },\n",
    ")\n",
    "coding_runner.initiate_chat(coding_assistant, message=\"Calculate the percentage gain YTD for Berkshire Hathaway stock and plot it as price.png. Don't use any approach that requires an API key. Write python of SH code to achieve the problem\")\n",
    "# clear_cache(clear_previous_work=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
