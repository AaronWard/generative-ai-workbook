{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TaskWeaver\n",
    "\n",
    "This notebook aims to condence all requirements needed for coding up a taskweaver project.\n",
    "\n",
    "Firstly, you need to make a project folder. this is a prerequisite to running TaskWeaver. \n",
    "\n",
    "Then we need to make a session:\n",
    "> TaskWeaver supports a basic session management to keep different users' <br>\n",
    "> data separate. The code execution is separated into different processes <br>\n",
    "> in order not to interfere with each other.\n",
    "\n",
    "See [starter_project](\"../_starter_projects/\")\n",
    "\n",
    "We also need to enable modules that the code interpreter can use and code verification.\n",
    "> Code verification - TaskWeaver is designed to verify the generated code before execution. <br>\n",
    "> It can detect potential issues in the generated code and provide suggestions to fix them.\n",
    "\n",
    "Add this to the `taskweaver_config.json`\n",
    "\n",
    "```\n",
    "\"code_verification.allowed_modules\": [\"pandas\", \"matplotlib\", \"numpy\", \"sklearn\", \"scipy\", \"seaborn\", \"datetime\", \"typing\"],\n",
    "\"code_verification.code_verification_on\": true,\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Any, Optional\n",
    "\n",
    "import pprint as pp\n",
    "from taskweaver.app.app import TaskWeaverApp\n",
    "\n",
    "# Create a session\n",
    "# would prefer if the expected folder structure was autogenerated\n",
    "app_dir = \"../_starter_projects/\"\n",
    "app = TaskWeaverApp(app_dir=app_dir)\n",
    "session = app.get_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can add customer configurations programatically by adding\n",
    "# something like the following to taskweaver_config.json\n",
    "# \"session.max_internal_chat_round_num\": 5\n",
    "# or  session.config.max_internal_chat_round_num = 5\n",
    "session.config.max_internal_chat_round_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.config.max_internal_chat_round_num = 30\n",
    "session.config.max_internal_chat_round_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session_manager.store_type': AppConfigItem(name='session_manager.store_type', value='in_memory', type='enum', sources=[AppConfigSourceValue(source='default', value='in_memory')]),\n",
       " 'llm.api_type': AppConfigItem(name='llm.api_type', value='openai', type='str', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.embedding_api_type': AppConfigItem(name='llm.embedding_api_type', value='openai', type='str', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.api_base': AppConfigItem(name='llm.api_base', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.api_key': AppConfigItem(name='llm.api_key', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.model': AppConfigItem(name='llm.model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.backup_model': AppConfigItem(name='llm.backup_model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.embedding_model': AppConfigItem(name='llm.embedding_model', value=None, type='str', sources=[AppConfigSourceValue(source='default', value=None)]),\n",
       " 'llm.response_format': AppConfigItem(name='llm.response_format', value='json_object', type='enum', sources=[AppConfigSourceValue(source='default', value='json_object')]),\n",
       " 'llm.openai.api_base': AppConfigItem(name='llm.openai.api_base', value='https://api.openai.com/v1', type='str', sources=[AppConfigSourceValue(source='default', value='https://api.openai.com/v1')]),\n",
       " 'llm.openai.api_key': AppConfigItem(name='llm.openai.api_key', value='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', type='str', sources=[AppConfigSourceValue(source='default', value='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')]),\n",
       " 'llm.openai.model': AppConfigItem(name='llm.openai.model', value='gpt-4-1106-preview', type='str', sources=[AppConfigSourceValue(source='default', value='gpt-4-1106-preview')]),\n",
       " 'llm.openai.backup_model': AppConfigItem(name='llm.openai.backup_model', value='gpt-4-1106-preview', type='str', sources=[AppConfigSourceValue(source='default', value='gpt-4-1106-preview')]),\n",
       " 'llm.openai.embedding_model': AppConfigItem(name='llm.openai.embedding_model', value='text-embedding-ada-002', type='str', sources=[AppConfigSourceValue(source='default', value='text-embedding-ada-002')]),\n",
       " 'llm.openai.api_version': AppConfigItem(name='llm.openai.api_version', value='2023-12-01-preview', type='str', sources=[AppConfigSourceValue(source='default', value='2023-12-01-preview')]),\n",
       " 'llm.openai.api_auth_type': AppConfigItem(name='llm.openai.api_auth_type', value='openai', type='enum', sources=[AppConfigSourceValue(source='default', value='openai')]),\n",
       " 'llm.openai.aad_auth_mode': AppConfigItem(name='llm.openai.aad_auth_mode', value='device_login', type='enum', sources=[AppConfigSourceValue(source='default', value='device_login')]),\n",
       " 'llm.openai.aad_tenant_id': AppConfigItem(name='llm.openai.aad_tenant_id', value='common', type='str', sources=[AppConfigSourceValue(source='default', value='common')]),\n",
       " 'llm.openai.aad_api_resource': AppConfigItem(name='llm.openai.aad_api_resource', value='https://cognitiveservices.azure.com/', type='str', sources=[AppConfigSourceValue(source='default', value='https://cognitiveservices.azure.com/')]),\n",
       " 'llm.openai.aad_api_scope': AppConfigItem(name='llm.openai.aad_api_scope', value='.default', type='str', sources=[AppConfigSourceValue(source='default', value='.default')]),\n",
       " 'llm.openai.aad_client_id': AppConfigItem(name='llm.openai.aad_client_id', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'llm.openai.aad_client_secret': AppConfigItem(name='llm.openai.aad_client_secret', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'llm.openai.aad_use_token_cache': AppConfigItem(name='llm.openai.aad_use_token_cache', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'llm.openai.aad_token_cache_path': AppConfigItem(name='llm.openai.aad_token_cache_path', value='cache/token_cache.bin', type='str', sources=[AppConfigSourceValue(source='default', value='cache/token_cache.bin')]),\n",
       " 'llm.openai.stop_token': AppConfigItem(name='llm.openai.stop_token', value=['<EOS>'], type='list', sources=[AppConfigSourceValue(source='default', value=['<EOS>'])]),\n",
       " 'llm.openai.temperature': AppConfigItem(name='llm.openai.temperature', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.max_tokens': AppConfigItem(name='llm.openai.max_tokens', value=1024, type='int', sources=[AppConfigSourceValue(source='default', value=1024)]),\n",
       " 'llm.openai.top_p': AppConfigItem(name='llm.openai.top_p', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.frequency_penalty': AppConfigItem(name='llm.openai.frequency_penalty', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.presence_penalty': AppConfigItem(name='llm.openai.presence_penalty', value=0, type='int', sources=[AppConfigSourceValue(source='default', value=0)]),\n",
       " 'llm.openai.seed': AppConfigItem(name='llm.openai.seed', value=123456, type='int', sources=[AppConfigSourceValue(source='default', value=123456)]),\n",
       " 'workspace.mode': AppConfigItem(name='workspace.mode', value='local', type='str', sources=[AppConfigSourceValue(source='default', value='local')]),\n",
       " 'workspace.workspace_path': AppConfigItem(name='workspace.workspace_path', value='${AppBaseDir}/workspace', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/workspace')]),\n",
       " 'logging.remote': AppConfigItem(name='logging.remote', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'logging.appinsights_connection_string': AppConfigItem(name='logging.appinsights_connection_string', value='', type='str', sources=[AppConfigSourceValue(source='default', value='')]),\n",
       " 'logging.injector': AppConfigItem(name='logging.injector', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'logging.log_folder': AppConfigItem(name='logging.log_folder', value='logs', type='str', sources=[AppConfigSourceValue(source='default', value='logs')]),\n",
       " 'logging.log_file': AppConfigItem(name='logging.log_file', value='task_weaver.log', type='str', sources=[AppConfigSourceValue(source='default', value='task_weaver.log')]),\n",
       " 'session.use_planner': AppConfigItem(name='session.use_planner', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'session.max_internal_chat_round_num': AppConfigItem(name='session.max_internal_chat_round_num', value=10, type='int', sources=[AppConfigSourceValue(source='default', value=10)]),\n",
       " 'planner.use_example': AppConfigItem(name='planner.use_example', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'planner.prompt_file_path': AppConfigItem(name='planner.prompt_file_path', value='${ModuleBaseDir}/planner/planner_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/planner/planner_prompt.yaml')]),\n",
       " 'planner.example_base_path': AppConfigItem(name='planner.example_base_path', value='${AppBaseDir}/planner_examples', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/planner_examples')]),\n",
       " 'planner.prompt_compression': AppConfigItem(name='planner.prompt_compression', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'planner.compression_prompt_path': AppConfigItem(name='planner.compression_prompt_path', value='${ModuleBaseDir}/planner/compression_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/planner/compression_prompt.yaml')]),\n",
       " 'plugin.base_path': AppConfigItem(name='plugin.base_path', value='${AppBaseDir}/plugins', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/plugins')]),\n",
       " 'round_compressor.rounds_to_compress': AppConfigItem(name='round_compressor.rounds_to_compress', value=2, type='int', sources=[AppConfigSourceValue(source='default', value=2)]),\n",
       " 'round_compressor.rounds_to_retain': AppConfigItem(name='round_compressor.rounds_to_retain', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)]),\n",
       " 'execution_service.env_dir': AppConfigItem(name='execution_service.env_dir', value='${AppBaseDir}/env', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/env')]),\n",
       " 'code_generator.role_name': AppConfigItem(name='code_generator.role_name', value='ProgramApe', type='str', sources=[AppConfigSourceValue(source='default', value='ProgramApe')]),\n",
       " 'code_generator.executor_name': AppConfigItem(name='code_generator.executor_name', value='CodeExecutor', type='str', sources=[AppConfigSourceValue(source='default', value='CodeExecutor')]),\n",
       " 'code_generator.load_plugin': AppConfigItem(name='code_generator.load_plugin', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'code_generator.load_example': AppConfigItem(name='code_generator.load_example', value=True, type='bool', sources=[AppConfigSourceValue(source='default', value=True)]),\n",
       " 'code_generator.prompt_file_path': AppConfigItem(name='code_generator.prompt_file_path', value='${ModuleBaseDir}/code_interpreter/code_generator/code_generator_json_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/code_interpreter/code_generator/code_generator_json_prompt.yaml')]),\n",
       " 'code_generator.example_base_path': AppConfigItem(name='code_generator.example_base_path', value='${AppBaseDir}/codeinterpreter_examples', type='path', sources=[AppConfigSourceValue(source='default', value='${AppBaseDir}/codeinterpreter_examples')]),\n",
       " 'code_generator.prompt_compression': AppConfigItem(name='code_generator.prompt_compression', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_generator.compression_prompt_path': AppConfigItem(name='code_generator.compression_prompt_path', value='${ModuleBaseDir}/code_interpreter/code_generator/compression_prompt.yaml', type='path', sources=[AppConfigSourceValue(source='default', value='${ModuleBaseDir}/code_interpreter/code_generator/compression_prompt.yaml')]),\n",
       " 'code_generator.enable_auto_plugin_selection': AppConfigItem(name='code_generator.enable_auto_plugin_selection', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_generator.auto_plugin_selection_topk': AppConfigItem(name='code_generator.auto_plugin_selection_topk', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)]),\n",
       " 'code_verification.code_verification_on': AppConfigItem(name='code_verification.code_verification_on', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_verification.plugin_only': AppConfigItem(name='code_verification.plugin_only', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_verification.allowed_modules': AppConfigItem(name='code_verification.allowed_modules', value=['pandas', 'matplotlib', 'numpy', 'sklearn', 'scipy', 'seaborn', 'datetime', 'typing'], type='list', sources=[AppConfigSourceValue(source='default', value=['pandas', 'matplotlib', 'numpy', 'sklearn', 'scipy', 'seaborn', 'datetime', 'typing'])]),\n",
       " 'code_interpreter.use_local_uri': AppConfigItem(name='code_interpreter.use_local_uri', value=False, type='bool', sources=[AppConfigSourceValue(source='default', value=False)]),\n",
       " 'code_interpreter.max_retry_count': AppConfigItem(name='code_interpreter.max_retry_count', value=3, type='int', sources=[AppConfigSourceValue(source='default', value=3)])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is where the configurations from taskweaver_config.json are stored\n",
    "# https://github.com/microsoft/TaskWeaver/blob/main/docs/configuration.md \n",
    "# Categories of keys within the config\n",
    "# - session_manager\n",
    "# - llm\n",
    "# - workspace\n",
    "# - logging\n",
    "# - session\n",
    "# - planner\n",
    "# - plugin\n",
    "# - round_compressor\n",
    "# - code_generator\n",
    "# - embedding_model\n",
    "# - code_verification\n",
    "# - code_interpreter\n",
    "session.config.src.config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple test message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_plan:\n",
      "1. Provide a brief introduction of the capabilities.\n",
      "plan:\n",
      "1. Provide a brief introduction of the capabilities.\n",
      "current_plan_step:\n",
      "Providing a brief introduction of the capabilities to the User.\n",
      "send_to:\n",
      "User\n",
      "message:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute Python code to complete those tasks. This can include data analysis, file operations, and other Python-related tasks. How can I assist you today?\n",
      "Planner->User:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute Python code to complete those tasks. This can include data analysis, file operations, and other Python-related tasks. How can I assist you today?\n",
      "final_reply_message:\n",
      "Hello! I can assist you by breaking down tasks into subtasks and coordinating with CodeInterpreter to execute Python code to complete those tasks. This can include data analysis, file operations, and other Python-related tasks. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Send a message to the session\n",
    "user_query = \"hello, what can you do?\"\n",
    "response_round = session.send_message(\n",
    "                            user_query,\n",
    "                            event_handler=lambda _type, _msg: print(f\"{_type}:\\n{_msg}\")\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'round-20231214-000218-a978c5b5',\n",
      " 'post_list': [{'attachment_list': [],\n",
      "                'id': 'post-20231214-000218-d987f3c1',\n",
      "                'message': 'hello, what can you do?',\n",
      "                'send_from': 'User',\n",
      "                'send_to': 'Planner'},\n",
      "               {'attachment_list': [{'content': '1. Provide a brief '\n",
      "                                                'introduction of the '\n",
      "                                                'capabilities.',\n",
      "                                     'id': 'atta-20231214-000230-92345dcc',\n",
      "                                     'type': 'init_plan'},\n",
      "                                    {'content': '1. Provide a brief '\n",
      "                                                'introduction of the '\n",
      "                                                'capabilities.',\n",
      "                                     'id': 'atta-20231214-000230-6e218e27',\n",
      "                                     'type': 'plan'},\n",
      "                                    {'content': 'Providing a brief '\n",
      "                                                'introduction of the '\n",
      "                                                'capabilities to the User.',\n",
      "                                     'id': 'atta-20231214-000230-2d8fe4fd',\n",
      "                                     'type': 'current_plan_step'}],\n",
      "                'id': 'post-20231214-000230-7a6b1e0e',\n",
      "                'message': 'Hello! I can assist you by breaking down tasks '\n",
      "                           'into subtasks and coordinating with '\n",
      "                           'CodeInterpreter to execute Python code to complete '\n",
      "                           'those tasks. This can include data analysis, file '\n",
      "                           'operations, and other Python-related tasks. How '\n",
      "                           'can I assist you today?',\n",
      "                'send_from': 'Planner',\n",
      "                'send_to': 'User'}],\n",
      " 'state': 'finished',\n",
      " 'user_query': 'hello, what can you do?'}\n"
     ]
    }
   ],
   "source": [
    "# Heres what that looks like in JSON\n",
    "pp.pprint(response_round.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classic stock market agent benchmark\n",
    "\n",
    "> protip: make sure `llm.response_format` is `text` because when working with `json` i found it crashed when using the code interpreter.\n",
    "> üí° Up to 11/30/2023, the json_object and text options of `llm.response_format` is only supported by the OpenAI models later than `1106`. If you are using an older version of OpenAI model, you need to set the llm.response_format to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Write code to plot the price of tesla stock YTD and save it to the current working directory\"\n",
    "response_round = session.send_message(user_query,\n",
    "                                      event_handler=lambda _type, _msg: print(f\"{_type}:\\n{_msg}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../_starter_projects/workspace/sessions/20231212-200217-2ba37e82/cwd/tesla_stock_price_ytd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plugins\n",
    "\n",
    "A plugin is the equivalent to a function call in Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no registry for loading plugin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from injector import Injector\n",
    "\n",
    "from taskweaver.config.config_mgt import AppConfigSource\n",
    "from taskweaver.logging import LoggingModule\n",
    "from taskweaver.memory import Memory, Post, Round\n",
    "from taskweaver.memory.plugin import PluginModule, PluginRegistry\n",
    "from taskweaver.planner import Planner\n",
    "from taskweaver.plugin import Plugin, register_plugin, test_plugin\n",
    "\n",
    "# Plugin definition\n",
    "@register_plugin\n",
    "class DataAnalysisPlugin(Plugin):\n",
    "    def __call__(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Performs a simple analysis on the DataFrame and returns a description.\n",
    "        \"\"\"\n",
    "        row_count = len(df)\n",
    "        column_count = len(df.columns)\n",
    "        description = f\"The DataFrame has {row_count} rows and {column_count} columns.\"\n",
    "        return df, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: [{'role': 'system', 'content': 'You are the Planner who can coordinate CodeInterpreter to finish the user task.\\n\\n# The characters involved in the conversation\\n\\n## User Character\\n- The User\\'s input should be the request or additional information required to complete the user\\'s task.\\n- The User can only talk to the Planner.\\n- The input of the User will prefix with \"User:\" in the chat history.\\n\\n## CodeInterpreter Character\\n- CodeInterpreter is responsible for generating and running Python code to complete the subtasks assigned by the Planner.\\n- CodeInterpreter can access the files, data base, web and other resources in the environment via generated Python code.\\n- CodeInterpreter has the following plugin functions:\\n\\t- data_analysis: This plugin performs a basic analysis on the provided DataFrame and returns a summary.\\n- CodeInterpreter can only talk to the Planner.\\n- CodeInterpreter can only follow one instruction at a time.\\n- CodeInterpreter returns the execution results, generated Python code, or error messages to the Planner.\\n- CodeInterpreter is stateful and it remembers the execution results of the previous rounds.\\n- The input of CodeInterpreter will be prefixed with \"CodeInterpreter:\" in the chat history.\\n\\n## Planner Character\\n- Planner\\'s role is to plan the subtasks and to instruct CodeInterpreter to resolve the request from the User.\\n- Planner can only talk to 2 characters: the User and the CodeInterpreter.\\n- Planner MUST NOT talk to the Planner itself.\\n\\n# Interactions between different characters\\n\\n## Conversation between Planner and User\\n- Planner receives the request from the User and decompose the request into subtasks.\\n- Planner should respond to the User when the task is finished.\\n- If the Planner needs additional information from the User, Planner should ask the User to provide.\\n\\n## Conversation between Planner and CodeInterpreter\\n- Planner instructs CodeInterpreter to execute the subtasks.\\n- Planner should execute the plan step by step and observe the output of the CodeInterpreter.\\n- Planner should refine or change the plan according to the output of the CodeInterpreter or the new requests of User.\\n- If User has made any changes to the environment, Planner should inform CodeInterpreter accordingly.\\n- Planner can ignore the permission or data access issues because CodeInterpreter can handle this kind of problem.\\n- Planner must include 2 parts: description of the User\\'s request and the current step that the Planner is executing.\\n\\n## Planner\\'s response format\\n- Planner must strictly format the response into the following JSON object:\\n  { \\n  \"response\": [\\n    {\\n      \"type\": \"init_plan\",\\n      \"content\": \"1. the first step in the plan\\\\n2. the second step in the plan <interactive or sequential depend on 1>\\\\n 3. the third step in the plan <interactive or sequential depend on 2>\"\\n    },\\n    {\\n      \"type\": \"plan\",\\n      \"content\": \"1. the first step in the refined plan\\\\n2. the second step in the refined plan\\\\n3. the third step in the refined plan\"\\n    },\\n    {\\n      \"type\": \"current_plan_step\",\\n      \"content\": \"the current step that the Planner is executing\"\\n    },\\n    {\\n      \"type\": \"send_to\",\\n      \"content\": \"User or CodeInterpreter\"\\n    },\\n    {\\n      \"type\": \"message\",\\n      \"content\": \"The text message to the User or the request to the CodeInterpreter from the Planner\"\\n    }\\n  ]\\n}\\n- Planner\\'s response must always include the 5 types of elements \"init_plan\", \"plan\", \"current_plan_step\", \"send_to\", and \"message\".\\n  - \"init_plan\" is the initial plan that Planner provides to the User.\\n  - \"plan\" is the refined plan that Planner provides to the User.\\n  - \"current_plan_step\" is the current step that Planner is executing.\\n  - \"send_to\" is the character that Planner wants to send the message to, that should be one of \"User\", \"CodeInterpreter\", or \"Planner\".\\n  - \"message\" is the message that Planner wants to send to the character.\\n- Planner must not include any other types of elements in the response that can cause parsing errors.\\n\\n# About multiple conversations\\n- There could be multiple Conversations in the chat history\\n- Each Conversation starts with the user query \"Let\\'s start a new conversation!\".\\n- You should not refer to any information from previous Conversations that are independent of the current Conversation.\\n\\n# About planning\\nYou need to make a step-by-step plan to complete the User\\'s task. The planning process includes 2 phases:\\n\\n## Initial planning\\n  - Decompose User\\'s task into subtasks and list them as the detailed plan steps.\\n  - Annotate the dependencies between these steps. There are 2 dependency types:\\n    1. Sequential Dependency: the current step depends on the previous step, but both steps can be executed by CodeInterpreter in an sequential manner.\\n      No additional information is required from User or Planner.\\n      For example:\\n      Task: count rows for ./data.csv\\n      Initial plan:\\n        1. Read ./data.csv file \\n        2. Count the rows of the loaded data <sequential depend on 1>\\n    2. Interactive Dependency: the current step depends on the previous step but requires additional information from User because the current step is ambiguous or complicated.\\n      Without the additional information (e.g., hyperparameters, data path, model name, file content, data schema, etc.), the CodeInterpreter cannot generate the complete and correct Python code to execute the current step.\\n      For example:\\n      Task: Read a manual file and follow the instructions in it.\\n      Initial plan:\\n        1. Read the file content.  \\n        2. Follow the instructions based on the file content.  <interactively depends on 1>\\n      Task: detect anomaly on ./data.csv\\n      Initial plan:\\n        1. Read the ./data.csv.  \\n        2. Confirm the columns to be detected anomalies  <interactively depends on 1>\\n        3. Detect anomalies on the loaded data <sequentially depends on 2>\\n        4. Report the detected anomalies to the user <interactively depends on 3>\\n  - If some steps can be executed in parallel, no dependency is needed to be annotated.\\n    For example:\\n      Task: read a.csv and b.csv and join them together\\n      Initial plan:\\n        1. Load a.csv as dataframe\\n        2. Load b.csv as dataframe\\n        3. Ask which column to join <interactively depends on 1, 2>\\n        4. Join the two dataframes <sequentially depends on 3>\\n        5. report the result to the user <interactively depends on 4>\\n\\n## Planning Refinement\\n  - Planner should try to merge adjacent sequential dependency steps, unless the merged step becomes too complicated.\\n  - Planner should not merge steps with interactive dependency or no dependency.\\n  - The final plan must not contain dependency annotations.'}, {'role': 'user', 'content': \"User: Let's start the new conversation!\\nTell me about the data\"}]\n",
      "Response:\n",
      " Processed response for: Tell me about the data\n"
     ]
    }
   ],
   "source": [
    "# Setup TaskWeaver environment\n",
    "app_injector = Injector([PluginModule, LoggingModule])\n",
    "app_config = AppConfigSource(config={\n",
    "    \"plugin.base_path\": os.path.join(app_dir, \"plugins\"),\n",
    "    \"llm.api_key\": \"******\",\n",
    "})\n",
    "\n",
    "# app.\n",
    "app_injector.binder.bind(AppConfigSource, to=app_config)\n",
    "\n",
    "plugin_registry = app_injector.get(PluginRegistry)\n",
    "planner = app_injector.create_object(Planner)\n",
    "\n",
    "# Create a memory object for the session\n",
    "memory = Memory(session_id=\"session-1\")\n",
    "\n",
    "def send_message_to_session(query):\n",
    "    # Create a new round with the user query\n",
    "    round1 = Round.create(user_query=query, id=\"round-1\")\n",
    "    post1 = Post.create(message=query, send_from=\"User\", send_to=\"Planner\", attachment_list=[])\n",
    "    round1.add_post(post1)\n",
    "\n",
    "    # Add the round to the memory and compose the prompt\n",
    "    memory.conversation.add_round(round1)\n",
    "    prompt = planner.compose_prompt(rounds=memory.conversation.rounds)\n",
    "\n",
    "    # Here you should process the prompt using the appropriate plugin or logic\n",
    "    # For demonstration, I'm just printing the prompt\n",
    "    print(\"Processing:\", prompt)\n",
    "\n",
    "    # Process the result and print the response (This is where your plugin logic will go)\n",
    "    # Assuming a response is generated\n",
    "    response = \"Processed response for: \" + query\n",
    "    print(\"Response:\\n\", response)\n",
    "\n",
    "    # Update the conversation with the response\n",
    "    response_post = Post.create(message=response, send_from=\"Planner\", send_to=\"User\", attachment_list=[])\n",
    "    round1.add_post(response_post)\n",
    "\n",
    "    return round1\n",
    "\n",
    "# Example usage\n",
    "user_query = \"Tell me about the data\"\n",
    "round  = send_message_to_session(user_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Autogen\n",
    "\n",
    "User(Human) input mode:\n",
    "- Autogen ‚úÖ (`human_input_mode` with `UserProxyAgent`)\n",
    "- TaskWeaver ‚úÖ (Planner can ask User)\n",
    "\n",
    "Compression:\n",
    "- Autogen ‚úÖ (Compressible Agent)\n",
    "- TaskWeaver ‚úÖ (Built in compression mechanism)\n",
    "\n",
    "Custom Agent Types:\n",
    "- Autogen  ‚úÖ (Autogen is a multi-agent framework)\n",
    "- TaskWeaver ‚ùå (Taskweaver is single-agent framework)\n",
    "\n",
    "Predefined Code Tools:\n",
    "- Autogen  ‚úÖ (with function calling)\n",
    "- TaskWeaver  ‚úÖ (with Plugins)\n",
    "\n",
    "Adaptive Code Generative (for domain specific code):\n",
    "- Autogen  ‚úÖ (Technically, with context stuffing the prompt)\n",
    "- TaskWeaver  ‚úÖ (with Examples)\n",
    "\n",
    "Session Management:\n",
    "- Autogen ‚úÖ  (need to use different seed for cache)\n",
    "- TaskWeaver ‚úÖ (Sessions built in natively)\n",
    "\n",
    "Stateful Code Execution:\n",
    "- Autogen ‚ùå (runs .py files)\n",
    "- TaskWeaver ‚úÖ (runs stateful execution, like a jupyter notebook)\n",
    "\n",
    "Parallel Code Execution:\n",
    "- Autogen ‚ùå \n",
    "- TaskWeaver ‚úÖ (If tasks are not dependent on eachother, they can run in different processes)\n",
    "\n",
    "Code Execution Security Measures:\n",
    "- Autogen ‚úÖ (through containers)\n",
    "- TaskWeaver ‚úÖ (through predefined rules)\n",
    "\n",
    "\n",
    "\n",
    "TaskWeaver seems excellent for when you have code dependent tasks, and appears to be a more structured way to manage those types of scenarios. <br>\n",
    "Where it fails in comparison to Autogen is constraints. It's only meant to be used as a LLM coder. For my dynamic agent scenarios that would involve<br>\n",
    "tasks from various roles Autogen would be a better choice. <br>\n",
    "\n",
    "Using taskweaver as a add-on agent in a Autogen GroupChat would be very beneficial, not only in terms of code generation performance but in terms of security and maintainability also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extendibility?\n",
    "\n",
    "In [taskweaver.memory.post](https://github.com/microsoft/TaskWeaver/blob/4e9b71518c98bde20dbc87e6441ff1d84197dc48/taskweaver/memory/post.py#L13) it states that a post can only be one of these roles.\n",
    "> A post is the message used to communicate between two roles. <br>\n",
    "> It should always have a text_message to denote the string message,  <br>\n",
    ">while other data formats should be put in the attachment. <br>\n",
    "> The role can be either a User, a Planner, or a CodeInterpreter. <br>\n",
    "\n",
    "And in `type_vars.py`:\n",
    "```python\n",
    "RoleName = Literal[\"User\", \"Planner\", \"CodeInterpreter\"]\n",
    "```\n",
    "\n",
    "So it differs from autogen in that you don't create a number of agents that have different roles, with TaskWeaver it seems you use these three roles to do one specific type of task very well: **natural language to executable code**. In the paper they even state that multi-agent systems is \n",
    "\n",
    ">*The first approach involves one agent (powered by TaskWeaver) calling other agents via its plugins.\n",
    ">Fig. 4 (a) depicts a simple example, although this can be extended to a more complex network <br> where multiple agents form a mesh network.\n",
    "> he second approach involves embedding TaskWeaverpowered agents into an existing multi-agent framework, such as AutoGen [1], as demonstrated in Fig.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes in the paper:\n",
    "\n",
    "https://export.arxiv.org/pdf/2311.17541\n",
    "\n",
    "> TaskWeaver gets the LLM to condense tasks into larger tasks to that it doesn't go back and forward executing the code and reduces costs.\n",
    "\n",
    "> *Our TaskWeaver is a single-agent framework that focuses on converting user requests into code, evenfor plugin calls.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE TO SELF:\n",
    "\n",
    "- Autogen as a plugin/example generator for TaskWeaver:\n",
    "    - Plugins:\n",
    "        - \"I need a plugin to do ____\" --> save it in Plugin store\n",
    "    - Examples:\n",
    "        - Domain specific code base -> llm to make examples -> store as examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
