{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Generation\n",
    "\n",
    "Last time i did this it cost about $10 to create question:answer pairs, this time im gonna use local models to it using ollama.\n",
    "This will iterate over the posts extracted from r/localllama and generate a QA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import ollama\n",
    "import pprint as pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The color of the sky appears blue due to a phenomenon called Rayleigh scattering. As sunlight reaches Earth's atmosphere, it interacts with molecules and particles in the air, such as nitrogen and oxygen. Blue light has a shorter wavelength and gets scattered more easily than other colors in the visible spectrum. As a result, when we look up at the sky, we predominantly see the blue light that has been scattered, giving the sky its characteristic blue hue during a clear day. However, at sunrise or sunset, the sky can take on various shades of red, orange, and purple as the sunlight interacts with more particles in the atmosphere and the angle of the sun's rays changes.\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "response = ollama.chat(model='mistral:latest', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "    'temperature': 0.01,\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 975 questions in total\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Post ID: 1at0288\\nTitle: Ok, which one of you was this? ðŸ¤£ðŸ¤£ðŸ¤£\\nLink: https://redd.it/1at0288\\nContent: \\nReplies:\\n- No, I don't think OpenAI would ever allow porn to be generated. I rather think that copies of Sora, recreated open source image generators will appear and fullfill this task. Porn is always one of the first use cases in any technologie that appeared and I don't think it'll take long for the industry to hop into this new tech. This is good for us as it further pushes open source AI technology for any use case.\\n\\n\",\n",
       " ' 1aszy6f\\nTitle: What are your favorite resources for evaluating text generation for stuff like readability, engagement (and other \"soft\" metrics)\\nLink: https://redd.it/1aszy6f\\nContent: Hi everyone, i\\'m working on a thesis looking at different prompt engineering methods and trying to evaluate the quality of generated content for stuff like articles, newsletters = human read content. Most research focuses on stuff like factuality, reasoning but I\\'m trying to find more research or these \"soft\" metrics. Any resources welcome, academic papers preferred. Many thanks!\\n\\n',\n",
       " ' 1aszspr\\nTitle: State of Opensource and Closedsource as of right now:\\nLink: https://redd.it/1aszspr\\nContent: What are the best opensource and closedsourced AI models?\\n\\nWhat will be the best opensource and closedsource AI models at the end of this year?\\n\\nList as per the following categories:\\n\\n1. Large Language Models:\\n\\n2. Large Multimodal Models:\\n\\n3. Text to image: \\n\\n4. Text to video: \\n\\n5. Text to 3D:\\n\\n6. Text to audio:\\n\\n7. Audio to text (transcribing):\\n\\n8. General purpose robots powered by Large multimodal models:\\n\\n']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH = \"./_output/new/localllama-new-17-02-2024.txt\"\n",
    "with open(DATA_PATH, \"r\") as file:\n",
    "    data = file.read()\n",
    "\n",
    "data_chunks = data.split(\"---\\nPost ID:\")\n",
    "\n",
    "print(f\"There are {len(data_chunks)} questions in total\")\n",
    "data_chunks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating QA Pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 975/975 [4:45:06<00:00, 17.55s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response_chunks = []\n",
    "\n",
    "print(\"Generating QA Pairs...\")\n",
    "for chunk in tqdm.tqdm(data_chunks):\n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {chunk}\n",
    "    ```\n",
    "    \\n\n",
    "    Your job is to look at this reddit post and to produce several question/answer pairs based on the content provided. \n",
    "    Look at the replies also and try to extract informative technical information. \n",
    "    Do not produce QA pairs for anything that is not in the provided text. \n",
    "    For longer posts (such as ones with a lot of information in the content or with many comments) produce a lot of QA pairs. \n",
    "    For posts with less content, produce fewer. Only include QA pairs with general useful information. \n",
    "    Do not include anything that could be considered personal information, opinion, or conversational text. \n",
    "    Only provide the QA pairs. Do NOT provide introductions or conclusions. Write your answer in this format:\n",
    "\n",
    "    ```\n",
    "    Q: What is the colour of the sky?\n",
    "    A: The colour of the sky is blue.\n",
    "    ---\n",
    "    Q: How old is OpenAI? \n",
    "    A: OpenAI was founded in 2015, therefore it is 8 years old.\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    response_chunks.append(\n",
    "        ollama.chat(model='mistral:latest', messages=[{\n",
    "            'role': 'user',\n",
    "            'content': prompt,\n",
    "            'temperature': 0.2},\n",
    "    ]))\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Q: What is the current capacity and bandwidth of consumer desktop RAM '\n",
      " 'compared to that of high-end GPUs?\\n'\n",
      " 'A: Current high-end consumer desktops have up to 64GB of RAM with a maximum '\n",
      " 'bandwidth of around 32 GBy/s, while high-end GPUs can have VRAM sizes '\n",
      " 'ranging from 12GB to over 256GB and VRAM bandwidths exceeding 500 GBy/s.\\n'\n",
      " '---\\n'\n",
      " 'Q: What is the significance of GPU architecture evolution for HPC and AIML?\\n'\n",
      " 'A: The lack of improvement in consumer desktop hardware, such as ECC RAM '\n",
      " 'capacity, number of memory channels, and available PCIe lanes, hinders the '\n",
      " 'development of high-performance computing (HPC) and artificial intelligence '\n",
      " 'machine learning (AIML) applications. GPUs have vastly more VRAM bandwidth '\n",
      " 'than current consumer desktops, which is crucial for HPC and AIML '\n",
      " 'workloads.\\n'\n",
      " '---\\n'\n",
      " 'Q: What is the Tesla M10 GPU and what are its advantages?\\n'\n",
      " 'A: The Tesla M10 is a GPU designed for server use, featuring 32GB of VRAM '\n",
      " \"and relatively high compute capabilities. It's an older model with slightly \"\n",
      " 'slower performance but offers a cost-effective solution for experimenting '\n",
      " 'with AIML applications. Its low price point makes it an attractive '\n",
      " 'alternative to more expensive options like the RTX 4090.\\n'\n",
      " '---\\n'\n",
      " 'Q: What are some challenges in scaling consumer desktop systems for '\n",
      " 'high-performance computing and AIML?\\n'\n",
      " 'A: Consumer desktops face limitations in terms of memory capacity, VRAM '\n",
      " 'bandwidth, number of available PCIe lanes, and cooling capabilities which '\n",
      " 'hinder the scalability of HPC and AIML applications. This limits their '\n",
      " 'potential performance compared to dedicated server hardware or specialized '\n",
      " 'GPUs.')\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(response_chunks[99]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 1aszfil\\nTitle: LM Studio prompt settings for Mixtral 11Bx2 MoE 19B GGUF?\\nLink: https://redd.it/1aszfil\\nContent: Hello friends.\\n\\nI downloaded this model, but I have a problem with Its prompt format.\\n\\n[TheBloke/Mixtral\\\\_11Bx2\\\\_MoE\\\\_19B-GGUF Â· Hugging Face](https://huggingface.co/TheBloke/Mixtral_11Bx2_MoE_19B-GGUF)\\n\\nIn Models card I can only see this:\\n\\nPrompt template: None\\n\\n{prompt}\\n\\nBut I think in LM Studio I\\'m forced to select a preset. Default preset for LM Studio is not working well.\\n\\nCan you help me please?\\n\\nThank you\\n\\nEdit:\\n\\nGemini gave me this. Do you think it\\'s OK?\\n\\n  \\n\\nJSON\\n\\n{\\n\\n\"model\\\\_name\": \"Mixtral\\\\_11Bx2\\\\_MoE\\\\_19B-GGUF\",\\n\\n\"model\\\\_path\": \"/path/to/model/directory\", # Replace with your actual model path\\n\\n\"prompt\\\\_template\": \"{prompt}\", # No specific prompt template needed based on model card\\n\\n\"batch\\\\_size\": 1,\\n\\n\"sequence\\\\_length\": 2048,\\n\\n\"temperature\": 0.7,\\n\\n\"top\\\\_p\": 0.9,\\n\\n\"sampling\\\\_method\": \"nucleus\",\\n\\n\"nucleus\\\\_p\": 0.9,\\n\\n\"no\\\\_repeat\\\\_ngram\\\\_size\": 2,\\n\\n\"num\\\\_beams\": 1,\\n\\n\"early\\\\_stopping\": True,\\n\\n\"max\\\\_length\": 512\\n\\n}\\nReplies:\\n- Are you running the latest lm studio\\n\\n'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_chunks[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./_output/response1.pkl', \"wb\") as file:\n",
    "    pickle.dump(response_chunks, file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ollama client\n",
    "\n",
    "`OLLAMA_HOST=127.0.0.1:5050 ollama serve`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating QA Pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/975 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/975 [00:38<5:13:47, 19.35s/it]"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(host='http://127.0.0.1:5050')\n",
    "\n",
    "response_chunks = []\n",
    "\n",
    "print(\"Generating QA Pairs...\")\n",
    "for chunk in tqdm.tqdm(data_chunks):\n",
    "    prompt = f\"\"\"\n",
    "    ```\n",
    "    {chunk}\n",
    "    ```\n",
    "    \\n\n",
    "    Your job is to look at this reddit post and to produce several question/answer pairs based on the content provided. \n",
    "    Look at the replies also and try to extract informative technical information. \n",
    "    Do not produce QA pairs for anything that is not in the provided text. \n",
    "    For longer posts (such as ones with a lot of information in the content or with many comments) produce a lot of QA pairs. \n",
    "    For posts with less content, produce fewer. Only include QA pairs with general useful information. \n",
    "    Do not include anything that could be considered personal information, opinion, or conversational text. \n",
    "    Only provide the QA pairs. Do NOT provide introductions or conclusions. Write your answer in this format:\n",
    "\n",
    "    ```\n",
    "    Q: What is the colour of the sky?\n",
    "    A: The colour of the sky is blue.\n",
    "    ---\n",
    "    Q: How old is OpenAI? \n",
    "    A: OpenAI was founded in 2015, therefore it is 8 years old.\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    response_chunks.append(\n",
    "        client.chat(model='llama2:latest', messages=[{\n",
    "                'role': 'user',\n",
    "                'content': prompt,\n",
    "                'temperature': 0.2\n",
    "        }])\n",
    "    )\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./_output/response2_llama.pkl', \"wb\") as file:\n",
    "    pickle.dump(response_chunks, file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masterclass",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
